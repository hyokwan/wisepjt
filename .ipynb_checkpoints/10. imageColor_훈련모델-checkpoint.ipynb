{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 구성 라이브러리\n",
    "# from google.colab import drive\n",
    "\n",
    "# 디렉토리 구성 라이브러리\n",
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "\n",
    "# 시각화 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 연산관련 라이브러리\n",
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "from numpy import savez_compressed\n",
    "from numpy.random import randint\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "\n",
    "# 딥러닝 모델 라이브러리\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모델 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>NAME</th>\n",
       "      <th>SEQ</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>YN</th>\n",
       "      <th>DESC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>OBJECT</td>\n",
       "      <td>1</td>\n",
       "      <td>512RESOL_431IMGS</td>\n",
       "      <td>Y</td>\n",
       "      <td>프로젝트 목적: 압축파일 및 H5 파일명에 영향</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>SEG_NAME</td>\n",
       "      <td>1</td>\n",
       "      <td>GOGUNG</td>\n",
       "      <td>Y</td>\n",
       "      <td>H5 결과 파일명에 영향</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>SEG_TIME</td>\n",
       "      <td>1</td>\n",
       "      <td>DAY</td>\n",
       "      <td>Y</td>\n",
       "      <td>H5 결과 파일명에 영향</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>COLAB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>RESOLUTION</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>Y</td>\n",
       "      <td>훈련 및 예측 모델에 영향</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>ROOT_PATH</td>\n",
       "      <td>1</td>\n",
       "      <td>./</td>\n",
       "      <td>Y</td>\n",
       "      <td>프로젝트 구조에 영향</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>TRAIN_FOLDER</td>\n",
       "      <td>1</td>\n",
       "      <td>문화유산 이미지 자료/train431</td>\n",
       "      <td>Y</td>\n",
       "      <td>ROOT_PATH 기준 훈련 이미지 PATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>TRAIN_PASS</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>훈련부분 자동주석</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>TEST_FOLDER</td>\n",
       "      <td>1</td>\n",
       "      <td>문화유산 이미지 자료/test4_gray</td>\n",
       "      <td>Y</td>\n",
       "      <td>ROOT_PATH 기준 테스트 이미지 PATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>RESULT_FOLDER</td>\n",
       "      <td>1</td>\n",
       "      <td>문화유산 이미지 자료/result3842522_02</td>\n",
       "      <td>Y</td>\n",
       "      <td>ROOT_PATH 기준 예측결과 PATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>COMMON</td>\n",
       "      <td>H5_FULLPATH</td>\n",
       "      <td>1</td>\n",
       "      <td>model_img384_25_22gogung_day.h5</td>\n",
       "      <td>Y</td>\n",
       "      <td>ROOT_PATH 기준 테스트 이미지 PATH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CATEGORY           NAME  SEQ                            VALUE YN  \\\n",
       "0    COMMON         OBJECT    1                 512RESOL_431IMGS  Y   \n",
       "1    COMMON       SEG_NAME    1                           GOGUNG  Y   \n",
       "2    COMMON       SEG_TIME    1                              DAY  Y   \n",
       "3    COMMON          COLAB    1                                0  Y   \n",
       "4    COMMON     RESOLUTION    1                              512  Y   \n",
       "5    COMMON      ROOT_PATH    1                               ./  Y   \n",
       "6    COMMON   TRAIN_FOLDER    1             문화유산 이미지 자료/train431  Y   \n",
       "7    COMMON     TRAIN_PASS    1                                Y  Y   \n",
       "8    COMMON    TEST_FOLDER    1           문화유산 이미지 자료/test4_gray  Y   \n",
       "9    COMMON  RESULT_FOLDER    1     문화유산 이미지 자료/result3842522_02  Y   \n",
       "10   COMMON    H5_FULLPATH    1  model_img384_25_22gogung_day.h5  Y   \n",
       "\n",
       "                          DESC  \n",
       "0   프로젝트 목적: 압축파일 및 H5 파일명에 영향  \n",
       "1                H5 결과 파일명에 영향  \n",
       "2                H5 결과 파일명에 영향  \n",
       "3                          NaN  \n",
       "4               훈련 및 예측 모델에 영향  \n",
       "5                  프로젝트 구조에 영향  \n",
       "6     ROOT_PATH 기준 훈련 이미지 PATH  \n",
       "7                    훈련부분 자동주석  \n",
       "8    ROOT_PATH 기준 테스트 이미지 PATH  \n",
       "9       ROOT_PATH 기준 예측결과 PATH  \n",
       "10   ROOT_PATH 기준 테스트 이미지 PATH  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramData = pd.read_excel(\"./MUSE_PARAM.xlsx\")\n",
    "paramData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#프로젝트 코랩사용여부 설정\n",
    "paramCategory=\"COMMON\"\n",
    "paramName=\"COLAB\"\n",
    "COLAB_YN = paramData.loc[ ((paramData.CATEGORY==paramCategory) &\n",
    "               (paramData.NAME==paramName))].VALUE.values[0]\n",
    "print(\"colab: {} \".format(COLAB_YN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB_YN == 1:\n",
    "    drive.mount('/content/gdrive') \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pjt_imgcat: GOGUNG \n"
     ]
    }
   ],
   "source": [
    "#프로젝트 세그명 설정\n",
    "paramCategory=\"COMMON\"\n",
    "paramValue=\"SEG_NAME\"\n",
    "IMG_CATEGORY = paramData.loc[ ((paramData.CATEGORY==paramCategory) &\n",
    "               (paramData.NAME==paramValue))].VALUE.values[0]\n",
    "print(\"pjt_imgcat: {} \".format(IMG_CATEGORY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pjt_segtime: DAY \n"
     ]
    }
   ],
   "source": [
    "#프로젝트 세그타임명 설정\n",
    "paramCategory=\"COMMON\"\n",
    "paramValue=\"SEG_TIME\"\n",
    "IMG_DATE = paramData.loc[ ((paramData.CATEGORY==paramCategory) &\n",
    "               (paramData.NAME==paramValue))].VALUE.values[0]\n",
    "print(\"pjt_segtime: {} \".format(IMG_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resoultion: 512, dim: (512, 512) \n"
     ]
    }
   ],
   "source": [
    "#프로젝트 해상도 설정\n",
    "paramCategory=\"COMMON\"\n",
    "paramValue=\"RESOLUTION\"\n",
    "LOAD_IMG_SIZE = paramData.loc[ ((paramData.CATEGORY==paramCategory) &\n",
    "               (paramData.NAME==paramValue))].VALUE.values[0]\n",
    "LOAD_IMG_DIM = (LOAD_IMG_SIZE,LOAD_IMG_SIZE)\n",
    "print(\"resoultion: {}, dim: {} \".format(LOAD_IMG_SIZE,LOAD_IMG_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pjt_object: 512RESOL_431IMGS \n"
     ]
    }
   ],
   "source": [
    "#프로젝트 입력데이터 압축파일명 설정\n",
    "paramCategory=\"COMMON\"\n",
    "paramValue=\"OBJECT\"\n",
    "PJT_OBJECT = paramData.loc[ ((paramData.CATEGORY==paramCategory) &\n",
    "               (paramData.NAME==paramValue))].VALUE.values[0]\n",
    "print(\"pjt_object: {} \".format(PJT_OBJECT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file compresed name : muse_512_512RESOL_431IMGS.npz \n"
     ]
    }
   ],
   "source": [
    "ONFILENAME= \"muse_{}_{}.npz\".format(LOAD_IMG_SIZE, PJT_OBJECT)\n",
    "print(\"file compresed name : {} \".format(ONFILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pjt_root_path: ./ \n"
     ]
    }
   ],
   "source": [
    "#프로젝트 입력데이터 압축파일명 설정\n",
    "paramCategory=\"COMMON\"\n",
    "paramValue=\"ROOT_PATH\"\n",
    "# ROOT_PATH=\"./gdrive/MyDrive/Colab Notebooks/wise/\"\n",
    "ROOT_PATH = paramData.loc[ ((paramData.CATEGORY==paramCategory) &\n",
    "               (paramData.NAME==paramValue))].VALUE.values[0]\n",
    "print(\"pjt_root_path: {} \".format(ROOT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pjt_train_pass: Y \n"
     ]
    }
   ],
   "source": [
    "#프로젝트 트레인 PAA여부\n",
    "paramCategory=\"COMMON\"\n",
    "paramValue=\"TRAIN_PASS\"\n",
    "# ROOT_PATH=\"./gdrive/MyDrive/Colab Notebooks/wise/\"\n",
    "TRAIN_PASS = paramData.loc[ ((paramData.CATEGORY==paramCategory) &\n",
    "               (paramData.NAME==paramValue))].VALUE.values[0]\n",
    "print(\"pjt_train_pass: {} \".format(TRAIN_PASS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pjt_train_path: 문화유산 이미지 자료/train431 \n"
     ]
    }
   ],
   "source": [
    "#프로젝트 입력데이터 압축파일명 설정\n",
    "paramCategory=\"COMMON\"\n",
    "paramValue=\"TRAIN_FOLDER\"\n",
    "# ROOT_PATH=\"./gdrive/MyDrive/Colab Notebooks/wise/\"\n",
    "TRAIN_PATH = paramData.loc[ ((paramData.CATEGORY==paramCategory) &\n",
    "               (paramData.NAME==paramValue))].VALUE.values[0]\n",
    "print(\"pjt_train_path: {} \".format(TRAIN_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: ./문화유산 이미지 자료/train431 \n"
     ]
    }
   ],
   "source": [
    "TRAIN_ROOT_FOLDER = os.path.join(ROOT_PATH,TRAIN_PATH)\n",
    "SRC_FOLDER = \"gray\"\n",
    "TAR_FOLDER = \"color\"\n",
    "print(\"train_path: {} \".format(TRAIN_ROOT_FOLDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 훈련데이터 불러오기 및 압축저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Udoc2j6q3t6v",
    "outputId": "e5bf4ffa-ea00-4e19-9569-0a0b197df668"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load all images in a directory into memory\n",
    "def load_images(inpath, inorgpath, insize, srcFolder, tarFolder):\n",
    "    src_list, tar_list = list(), list()\n",
    "    for filename in listdir(inorgpath):\n",
    "        try:\n",
    "            srcpath = os.path.join(inpath,srcFolder,filename)\n",
    "            tarpath = os.path.join(inpath,tarFolder,filename.replace(\"_gray\",\"\") )\n",
    "            # load and resize the image\n",
    "            srcPixels = load_img(srcpath, target_size=insize)\n",
    "            tarPixels = load_img(tarpath, target_size=insize)\n",
    "            # convert to numpy array\n",
    "            srcPixels = img_to_array(srcPixels)\n",
    "            tarPixels = img_to_array(tarPixels)\n",
    "            # save into list\n",
    "            src_list.append(srcPixels)\n",
    "            tar_list.append(tarPixels)\n",
    "        except Exception as e:\n",
    "            print(e, srcpath)\n",
    "        \n",
    "    return [asarray(src_list), asarray(tar_list)]\n",
    "\n",
    "if (TRAIN_PASS == \"N\"):\n",
    "    # dataset path\n",
    "    path = TRAIN_ROOT_FOLDER\n",
    "    orgpath = os.path.join(path,SRC_FOLDER)\n",
    "    # load dataset\n",
    "    [src_images, tar_images] = load_images(inpath=path,\n",
    "                                           inorgpath = orgpath,\n",
    "                                           insize=LOAD_IMG_DIM,\n",
    "                                           srcFolder = SRC_FOLDER,\n",
    "                                           tarFolder = TAR_FOLDER)\n",
    "\n",
    "    print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "    # 압축된 numpy array로 저장\n",
    "    filename = os.path.join(ROOT_PATH,ONFILENAME)\n",
    "    savez_compressed(filename, src_images, tar_images)\n",
    "    print('Saved dataset: ', filename)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqyN7epPGop-",
    "outputId": "12db5c57-63a5-40d8-8d0d-8297cb327731"
   },
   "outputs": [],
   "source": [
    "# # load the dataset\n",
    "# data = load('maps_{}.npz'.format(LOAD_IMG_SIZE))\n",
    "\n",
    "# src_images, tar_images = data['arr_0'], data['arr_1']\n",
    "# print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# # plot source images\n",
    "# n_samples = 3\n",
    "# for i in range(n_samples):\n",
    "#     plt.subplot(2, n_samples, 1 + i)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(src_images[i].astype('uint8'))\n",
    "# # plot target image\n",
    "# for i in range(n_samples):\n",
    "#     plt.subplot(2, n_samples, 1 + n_samples + i)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(tar_images[i].astype('uint8'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqyN7epPGop-",
    "outputId": "12db5c57-63a5-40d8-8d0d-8297cb327731"
   },
   "outputs": [],
   "source": [
    "# ###  * PatchGAN은 전체 영역이 아니라 특정 크기의 patch 단위로 Generator가 만든 이미지의 진위 여부를 판단\n",
    "\n",
    "# 판별자 모델 정의\n",
    "def define_discriminator(image_shape):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # source image input\n",
    "    in_src_image = Input(shape=image_shape)\n",
    "    # target image input\n",
    "    in_target_image = Input(shape=image_shape)\n",
    "    # concatenate images channel-wise\n",
    "    merged = Concatenate()([in_src_image, in_target_image])\n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C128\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C256\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer\n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # patch output\n",
    "    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    patch_out = Activation('sigmoid')(d)\n",
    "    # define model\n",
    "    model = Model([in_src_image, in_target_image], patch_out)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model\n",
    "\n",
    "# 정의: 인코더 모델 정의\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    # 가중치 초기화 (평균0, 표준편차 0.02)\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # 다운샘플링 레이어 (합성곱 신경망)\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    # conditionally add batch normalization\n",
    "    \n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "        \n",
    "    # 활성함수정의: leaky relu\n",
    "    # gradient 미분값 변화량이 적어질경우 학습이 제대로 되지 않는다-> rel\n",
    "    # 입력값이 음수일 때 출력값을 이 아닌 0.001과 같은 매우 작은 값을 출력하도록 설정 단, relu보다 계산량 많음\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g\n",
    "\n",
    "# 정의: 디코더 모델 정의\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "    # 가중치 초기화 (평균0, 표준편차 0.02)\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    #  업샘플링 레리어 (합성곱 역)\n",
    "    g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    # 배치정규화 기본\n",
    "    g = BatchNormalization()(g, training=True)\n",
    "    # dropout 레이어는 요청시 정의\n",
    "    if dropout:\n",
    "        g = Dropout(0.5)(g, training=True)\n",
    "    # 스킵인 레이어와 통합 (동일크기의 기존 이미지 정보 skip in)\n",
    "    g = Concatenate()([g, skip_in])\n",
    "    # 활성함수정의: relu\n",
    "    g = Activation('relu')(g)\n",
    "    return g\n",
    "\n",
    "# 정의: 생성자 모델 정의\n",
    "def define_generator(image_shape=(LOAD_IMG_SIZE,LOAD_IMG_SIZE,3)):\n",
    "    # 가중치 초기화 (평균 0, 표준편차 0.02)\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # 1. 인풋레이어 정의 ( 이미지사이즈) 인코더 모델 (conv레이어 + Leakly Relu 조합)\n",
    "    in_image = Input(shape=image_shape)\n",
    "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "    \n",
    "    # 2. 히든레이어 정의 (conv레이어 + Leakly Relu 조합)\n",
    "    e2 = define_encoder_block(e1, 128)\n",
    "    e3 = define_encoder_block(e2, 256)\n",
    "    e4 = define_encoder_block(e3, 512)\n",
    "    e5 = define_encoder_block(e4, 512)\n",
    "    e6 = define_encoder_block(e5, 512)\n",
    "    e7 = define_encoder_block(e6, 512)\n",
    "    # bottleneck, no batch norm and relu\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "    b = Activation('relu')(b)\n",
    "    # 디코더 모델 (convTranspost 레이어 + relu 조합)\n",
    "    d1 = decoder_block(b, e7, 512)\n",
    "    d2 = decoder_block(d1, e6, 512)\n",
    "    d3 = decoder_block(d2, e5, 512)\n",
    "    d4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "    d5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "    d6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "    d7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "    \n",
    "    # 3. 아웃풋레이어 정의 하이퍼볼릭 탄젠트 (-1~1) 정의\n",
    "    g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "    out_image = Activation('tanh')(g)\n",
    "\n",
    "    # 모델 정의 (input, output) 조립 \n",
    "    model = Model(in_image, out_image)\n",
    "    return model\n",
    "\n",
    "# 생성자모델 업데이트를 위한 생성자 및 판별자 조합 모델\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # define the source image\n",
    "    in_src = Input(shape=image_shape)\n",
    "    # 이미지소스를 생성자 모델에 연결\n",
    "    gen_out = g_model(in_src)\n",
    "    # 입력소스 및 생성자 출력결과를 판별자 input에 연결 connect the source input and generator output to the discriminator input\n",
    "    dis_out = d_model([in_src, gen_out])\n",
    "    # 모델정의 (input, ouput, input 이미지, output: 출력결과, 생성자 출력결과)\n",
    "    # src image as input, generated image and classification output\n",
    "    model = Model(in_src, [dis_out, gen_out])\n",
    "    # compile model\n",
    "    # 모델 컴파일\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=[\"binary_crossentropy\",\"mae\"], optimizer=opt, loss_weights=[1,100])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 기타 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqyN7epPGop-",
    "outputId": "12db5c57-63a5-40d8-8d0d-8297cb327731"
   },
   "outputs": [],
   "source": [
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    "    # load compressed arrays\n",
    "    data = load(filename)\n",
    "    # unpack arrays\n",
    "    X1, X2 = data['arr_0'], data['arr_1']\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X1 = (X1 - 127.5) / 127.5\n",
    "    X2 = (X2 - 127.5) / 127.5\n",
    "    return [X1, X2]\n",
    "\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "    # unpack dataset\n",
    "    trainA, trainB = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, trainA.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X1, X2 = trainA[ix], trainB[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [X1, X2], y\n",
    "\n",
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "    # generate fake instance\n",
    "    X = g_model.predict(samples)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y\n",
    "\n",
    "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
    "    # select a sample of input images\n",
    "    [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
    "    # generate a batch of fake samples\n",
    "    X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
    "    # scale all pixels from [-1,1] to [0,1]\n",
    "    X_realA = (X_realA + 1) / 2.0\n",
    "    X_realB = (X_realB + 1) / 2.0\n",
    "    X_fakeB = (X_fakeB + 1) / 2.0\n",
    "    # plot real source images\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_realA[i])\n",
    "    # plot generated target image\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_fakeB[i])\n",
    "    # plot real target image\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_realB[i])\n",
    "    # save plot to file\n",
    "    filename1 = 'plot_%06d.png' % (step+1)\n",
    "    plt.savefig(filename1)\n",
    "    plt.close()\n",
    "    # save the generator model\n",
    "    filename2 = './model_{}_{}_{}.h5'.format(IMG_CATEGORY, IMG_DATE,PJT_OBJECT)\n",
    "    g_model.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))\n",
    "\n",
    "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
    "    # 판별자(Descriminator)의 출력 shape 정의 (예: 32)\n",
    "    n_patch = d_model.output_shape[1]\n",
    "    # trainA, trainB = 훈련데이터의 흑백이미지, 컬러이미지\n",
    "    trainA, trainB = dataset\n",
    "    # 에포크별 배치 수 연산 (예: 이미지 50장 배치 1 -> 50 )\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    # 반복횟수 연산(예: 500 * 에포크(11) = 500)\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # 정의: 훈련이미지 중 흑백/컬러 샘플 1장(n_batch) 선택 및 1장의 1로 채운 patch shape (1, 32, 32, 1) 반환\n",
    "        # 정의 데이터를 불러와서 문제지/정답지 이미지 한케이스 및 판별자 출력값과 같은 차원의 1 값 반환\n",
    "        # 파라미터 : dataset: 훈련 이미지 (-1~1사이)\n",
    "        #           n_batch: 배치 사이즈 (1)\n",
    "        #           n_patch: 판별자 출력결과 크기 (32)\n",
    "        # 출력값: X_realA: 흑백 이미지\n",
    "        #        X_realB: 컬러 이미지\n",
    "        #         y_real: 정답지 (1, 32, 32, 1)\n",
    "        [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "\n",
    "        # 정의: 배치훈련을 위한 가짜 샘플 생성\n",
    "        # 파라미터: g_model: 생성자 (다운/업샘플링)\n",
    "        #          X_realA: 흑백이미지 샘플 1장 \n",
    "        #          n_patch: 판별자 출력 shape 32\n",
    "        # XrealA(훈련데이터의 흑백이미지를 넣고 생성자로 예측) -> 1,512,512,3\n",
    "        # 이후  판별자 shape크기에 값1 채운값 생성 (예: 1,32,32,1 shape)\n",
    "        # 출력값: X_fakeB: 흑백사진 넣고 생성자 모델을 통해 예측된 이미지 결과\n",
    "        #        y_fake: 1,32,32,1 판별자 출력 크기의 0으로 채운 값\n",
    "        X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "\n",
    "        # 정의: 판별자 모델 업데이트 (훈련데이터중 컬러/흑백 사진 및 1 결과 넣음)\n",
    "        # 파라미터 : X_realA: 훈련데이터중 랜덤하게 뽑은 흑백사진 1장 [문제지]\n",
    "        #           X_realB: 훈련데이터중 랜덤하게 뽑은 컬러사진 1장 [답지]\n",
    "        #           y_real: 판별자 1로 훈련\n",
    "        # 출력값: 판별자 Loss1\n",
    "        d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "\n",
    "        # 정의: 판별자 모델 업데이트 ( 생성자를 통해 생성된 이미지 활용 )\n",
    "        # 파라미터 : X_realA: 훈련데이터중 랜덤하게 뽑은 흑백사진 1장 [문제지]\n",
    "        #           X_fakeB: 흑백사진 넣고 생성자 모델을 통해 예측된 이미지 결과\n",
    "        # 출력:     y_fake: 1,32,32,1 판별자 출력 크기의 0으로 채운 값\n",
    "        d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "\n",
    "        # 정의: 생성자 모델 업데이트\n",
    "        # 파라미터: X_realA: 훈련데이터중 랜덤하게 뽑은 흑백사진 1장 [문제지] -> input image shape\n",
    "        #          y_real: 판별자 1로 훈련 -> 판별자 출력결과\n",
    "        #          X_realB: 훈련데이터중 랜덤하게 뽑은 컬러사진 1장 [답지] -> 생성자 출력\n",
    "        # 출력: loss   \n",
    "        g_loss,_,_ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "\n",
    "        # 스탭별 loss 출력\n",
    "        print(\"{} step d_loss1: {}, d_loss2: {}, g_loss: {}\".format(i+1, d_loss1, d_loss2, g_loss) )\n",
    "\n",
    "        # summarize model performance\n",
    "        if (i+1) % (bat_per_epo * 10) == 0:\n",
    "            summarize_performance(i, g_model, dataset)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqyN7epPGop-",
    "outputId": "12db5c57-63a5-40d8-8d0d-8297cb327731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (431, 512, 512, 3) (431, 512, 512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMART-22\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 이미지 데이터 불러오기 (예:512*512) 사이즈로 변환 및 -1~1사리오 변환된 이미지 (+1/2 해야함)\n",
    "dataset = load_real_samples(filename)\n",
    "# dataset = [src_images, tar_images] \n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "\n",
    "# 불러온 데이터를 가지고 모델에 적용하기 위한 input_shape 정의 (예: 512,512,3)\n",
    "image_shape = dataset[0].shape[1:]\n",
    "\n",
    "# 모델정의\n",
    "# d_model: 판별 모델 patch gan\n",
    "# g_model: u-net정의\n",
    "d_model = define_discriminator(image_shape)\n",
    "g_model = define_generator(image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWtvIDYXBdzX",
    "outputId": "ffaebea8-b8d0-403d-d61e-e085b07f3174",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 step d_loss1: 0.33451157808303833, d_loss2: 1.2041720151901245, g_loss: 64.69255828857422\n",
      "2 step d_loss1: 0.372160404920578, d_loss2: 1.6958897113800049, g_loss: 54.09368896484375\n",
      "3 step d_loss1: 0.6076145172119141, d_loss2: 0.8333396911621094, g_loss: 42.41920852661133\n",
      "4 step d_loss1: 0.271719366312027, d_loss2: 0.7286723852157593, g_loss: 64.4298095703125\n",
      "5 step d_loss1: 0.29797011613845825, d_loss2: 0.54667067527771, g_loss: 38.13102722167969\n",
      "6 step d_loss1: 0.34339845180511475, d_loss2: 0.4527478516101837, g_loss: 37.322330474853516\n",
      "7 step d_loss1: 0.35729560256004333, d_loss2: 0.4515876770019531, g_loss: 29.226699829101562\n",
      "8 step d_loss1: 0.38839036226272583, d_loss2: 0.4131052494049072, g_loss: 25.90198516845703\n",
      "9 step d_loss1: 0.3717224597930908, d_loss2: 0.41085025668144226, g_loss: 33.29899597167969\n",
      "10 step d_loss1: 0.3941553831100464, d_loss2: 0.3834121823310852, g_loss: 27.340700149536133\n",
      "11 step d_loss1: 0.38396161794662476, d_loss2: 0.3915318548679352, g_loss: 41.538021087646484\n",
      "12 step d_loss1: 0.37638986110687256, d_loss2: 0.38969358801841736, g_loss: 25.781280517578125\n",
      "13 step d_loss1: 0.3866591155529022, d_loss2: 0.4425433278083801, g_loss: 23.119239807128906\n",
      "14 step d_loss1: 0.3627176284790039, d_loss2: 0.3882662355899811, g_loss: 27.204187393188477\n",
      "15 step d_loss1: 0.38329222798347473, d_loss2: 0.4000096321105957, g_loss: 33.873348236083984\n",
      "16 step d_loss1: 0.39372706413269043, d_loss2: 0.3703817129135132, g_loss: 30.863006591796875\n",
      "17 step d_loss1: 0.3819934129714966, d_loss2: 0.3838745951652527, g_loss: 16.71311378479004\n",
      "18 step d_loss1: 0.37078195810317993, d_loss2: 0.3711536228656769, g_loss: 20.622800827026367\n",
      "19 step d_loss1: 0.3837287425994873, d_loss2: 0.5062451362609863, g_loss: 31.656490325927734\n",
      "20 step d_loss1: 0.4147588014602661, d_loss2: 0.4450224041938782, g_loss: 39.99386215209961\n",
      "21 step d_loss1: 0.39807847142219543, d_loss2: 0.3853190243244171, g_loss: 23.492870330810547\n",
      "22 step d_loss1: 0.3675088882446289, d_loss2: 0.3830040693283081, g_loss: 14.886770248413086\n",
      "23 step d_loss1: 0.366530179977417, d_loss2: 0.37662211060523987, g_loss: 15.972203254699707\n",
      "24 step d_loss1: 0.3701304793357849, d_loss2: 0.377847820520401, g_loss: 17.29086685180664\n",
      "25 step d_loss1: 0.37046346068382263, d_loss2: 0.39661723375320435, g_loss: 22.185150146484375\n",
      "26 step d_loss1: 0.3868764638900757, d_loss2: 0.3730623126029968, g_loss: 13.486222267150879\n",
      "27 step d_loss1: 0.36695486307144165, d_loss2: 0.3930472135543823, g_loss: 16.241256713867188\n",
      "28 step d_loss1: 0.3722766935825348, d_loss2: 0.39048895239830017, g_loss: 22.99908447265625\n",
      "29 step d_loss1: 0.38343924283981323, d_loss2: 0.40154755115509033, g_loss: 18.123003005981445\n",
      "30 step d_loss1: 0.3791221082210541, d_loss2: 0.3707565665245056, g_loss: 26.36540985107422\n",
      "31 step d_loss1: 0.37664973735809326, d_loss2: 0.3730010986328125, g_loss: 33.40679931640625\n",
      "32 step d_loss1: 0.3885754644870758, d_loss2: 0.3591301739215851, g_loss: 23.293609619140625\n",
      "33 step d_loss1: 0.36919599771499634, d_loss2: 0.3810722529888153, g_loss: 14.546649932861328\n",
      "34 step d_loss1: 0.35541144013404846, d_loss2: 0.4168914556503296, g_loss: 24.567028045654297\n",
      "35 step d_loss1: 0.38987037539482117, d_loss2: 0.38316333293914795, g_loss: 20.828166961669922\n",
      "36 step d_loss1: 0.3795551359653473, d_loss2: 0.3715367913246155, g_loss: 17.983182907104492\n",
      "37 step d_loss1: 0.36974355578422546, d_loss2: 0.3748167157173157, g_loss: 15.64319896697998\n",
      "38 step d_loss1: 0.3677085041999817, d_loss2: 0.37571024894714355, g_loss: 15.987760543823242\n",
      "39 step d_loss1: 0.368556946516037, d_loss2: 0.3669133484363556, g_loss: 17.500492095947266\n",
      "40 step d_loss1: 0.37285006046295166, d_loss2: 0.3968344032764435, g_loss: 13.742741584777832\n",
      "41 step d_loss1: 0.37282827496528625, d_loss2: 0.36655193567276, g_loss: 11.135547637939453\n",
      "42 step d_loss1: 0.3678937256336212, d_loss2: 0.3763461112976074, g_loss: 13.153100967407227\n",
      "43 step d_loss1: 0.3757196366786957, d_loss2: 0.37296849489212036, g_loss: 13.516201972961426\n",
      "44 step d_loss1: 0.37068361043930054, d_loss2: 0.37126317620277405, g_loss: 12.559273719787598\n",
      "45 step d_loss1: 0.3726959824562073, d_loss2: 0.382111519575119, g_loss: 22.363588333129883\n",
      "46 step d_loss1: 0.36999720335006714, d_loss2: 0.3776191771030426, g_loss: 11.89968204498291\n",
      "47 step d_loss1: 0.3631405234336853, d_loss2: 0.37115785479545593, g_loss: 45.57180404663086\n",
      "48 step d_loss1: 0.36625146865844727, d_loss2: 0.37862497568130493, g_loss: 12.762596130371094\n",
      "49 step d_loss1: 0.37322181463241577, d_loss2: 0.3711930513381958, g_loss: 24.43954849243164\n",
      "50 step d_loss1: 0.37860655784606934, d_loss2: 0.37920695543289185, g_loss: 14.665498733520508\n",
      "51 step d_loss1: 0.36484456062316895, d_loss2: 0.3727738559246063, g_loss: 24.874095916748047\n",
      "52 step d_loss1: 0.3629162609577179, d_loss2: 0.3649905323982239, g_loss: 11.873664855957031\n",
      "53 step d_loss1: 0.3551086187362671, d_loss2: 0.3649359345436096, g_loss: 11.999411582946777\n",
      "54 step d_loss1: 0.37374147772789, d_loss2: 0.3904825448989868, g_loss: 23.30072593688965\n",
      "55 step d_loss1: 0.3532179594039917, d_loss2: 0.3755875825881958, g_loss: 15.556305885314941\n",
      "56 step d_loss1: 0.35986679792404175, d_loss2: 0.3540739119052887, g_loss: 13.752226829528809\n",
      "57 step d_loss1: 0.36745187640190125, d_loss2: 0.3790762424468994, g_loss: 20.50507926940918\n",
      "58 step d_loss1: 0.3622671067714691, d_loss2: 0.4014757573604584, g_loss: 13.01741886138916\n",
      "59 step d_loss1: 0.34763506054878235, d_loss2: 0.38517308235168457, g_loss: 18.05025291442871\n",
      "60 step d_loss1: 0.3292454779148102, d_loss2: 0.3705627918243408, g_loss: 16.990598678588867\n",
      "61 step d_loss1: 0.3809809982776642, d_loss2: 0.36634019017219543, g_loss: 19.515790939331055\n",
      "62 step d_loss1: 0.3373417854309082, d_loss2: 0.35494112968444824, g_loss: 11.275491714477539\n",
      "63 step d_loss1: 0.38200098276138306, d_loss2: 0.3931923806667328, g_loss: 17.094730377197266\n",
      "64 step d_loss1: 0.3571728467941284, d_loss2: 0.36749890446662903, g_loss: 11.097646713256836\n",
      "65 step d_loss1: 0.3441169857978821, d_loss2: 0.35802775621414185, g_loss: 11.847010612487793\n",
      "66 step d_loss1: 0.35860878229141235, d_loss2: 0.3701353073120117, g_loss: 11.468358993530273\n",
      "67 step d_loss1: 0.36879974603652954, d_loss2: 0.38101091980934143, g_loss: 13.545694351196289\n",
      "68 step d_loss1: 0.36647918820381165, d_loss2: 0.3673667907714844, g_loss: 12.504764556884766\n",
      "69 step d_loss1: 0.37019988894462585, d_loss2: 0.4034019410610199, g_loss: 14.78421688079834\n",
      "70 step d_loss1: 0.3248499929904938, d_loss2: 0.3645230829715729, g_loss: 20.98447036743164\n",
      "71 step d_loss1: 0.37318623065948486, d_loss2: 0.34942981600761414, g_loss: 13.293103218078613\n",
      "72 step d_loss1: 0.3311017155647278, d_loss2: 0.4484800398349762, g_loss: 13.32893180847168\n",
      "73 step d_loss1: 0.4398772418498993, d_loss2: 0.36949726939201355, g_loss: 14.520423889160156\n",
      "74 step d_loss1: 0.34229719638824463, d_loss2: 0.43397077918052673, g_loss: 16.704919815063477\n",
      "75 step d_loss1: 0.40805912017822266, d_loss2: 0.3359888195991516, g_loss: 14.421759605407715\n",
      "76 step d_loss1: 0.37809860706329346, d_loss2: 0.38435229659080505, g_loss: 13.53558349609375\n",
      "77 step d_loss1: 0.4179750084877014, d_loss2: 0.370074987411499, g_loss: 22.091650009155273\n",
      "78 step d_loss1: 0.37652888894081116, d_loss2: 0.35606566071510315, g_loss: 12.765533447265625\n",
      "79 step d_loss1: 0.4066949486732483, d_loss2: 0.37689363956451416, g_loss: 14.443650245666504\n",
      "80 step d_loss1: 0.36212801933288574, d_loss2: 0.3610742390155792, g_loss: 11.432117462158203\n",
      "81 step d_loss1: 0.35290321707725525, d_loss2: 0.3738882541656494, g_loss: 13.065774917602539\n",
      "82 step d_loss1: 0.38944578170776367, d_loss2: 0.35034695267677307, g_loss: 12.513750076293945\n",
      "83 step d_loss1: 0.35748863220214844, d_loss2: 0.3804020881652832, g_loss: 12.29519271850586\n",
      "84 step d_loss1: 0.3465864062309265, d_loss2: 0.38594236969947815, g_loss: 13.202527046203613\n",
      "85 step d_loss1: 0.39139047265052795, d_loss2: 0.38114988803863525, g_loss: 13.952754020690918\n",
      "86 step d_loss1: 0.35349637269973755, d_loss2: 0.3620169460773468, g_loss: 16.850141525268555\n",
      "87 step d_loss1: 0.362277626991272, d_loss2: 0.3720240592956543, g_loss: 12.8331298828125\n",
      "88 step d_loss1: 0.36671334505081177, d_loss2: 0.3766241669654846, g_loss: 9.932389259338379\n",
      "89 step d_loss1: 0.35949504375457764, d_loss2: 0.40625035762786865, g_loss: 32.41242599487305\n",
      "90 step d_loss1: 0.36682483553886414, d_loss2: 0.38541051745414734, g_loss: 13.175469398498535\n",
      "91 step d_loss1: 0.342473566532135, d_loss2: 0.3600406348705292, g_loss: 30.465517044067383\n",
      "92 step d_loss1: 0.37302863597869873, d_loss2: 0.38823938369750977, g_loss: 20.810998916625977\n",
      "93 step d_loss1: 0.38152578473091125, d_loss2: 0.38507941365242004, g_loss: 16.16808319091797\n",
      "94 step d_loss1: 0.3999428451061249, d_loss2: 0.36532318592071533, g_loss: 16.26499366760254\n",
      "95 step d_loss1: 0.37798064947128296, d_loss2: 0.38378235697746277, g_loss: 11.132278442382812\n",
      "96 step d_loss1: 0.38614606857299805, d_loss2: 0.3820154070854187, g_loss: 18.50261878967285\n",
      "97 step d_loss1: 0.37269267439842224, d_loss2: 0.36658114194869995, g_loss: 12.863199234008789\n",
      "98 step d_loss1: 0.3673367500305176, d_loss2: 0.3549146056175232, g_loss: 12.715584754943848\n",
      "99 step d_loss1: 0.36955535411834717, d_loss2: 0.359103262424469, g_loss: 9.989514350891113\n",
      "100 step d_loss1: 0.3593418598175049, d_loss2: 0.3687475919723511, g_loss: 12.977150917053223\n",
      "101 step d_loss1: 0.34193214774131775, d_loss2: 0.37983134388923645, g_loss: 9.700928688049316\n",
      "102 step d_loss1: 0.36427223682403564, d_loss2: 0.36720019578933716, g_loss: 15.542837142944336\n",
      "103 step d_loss1: 0.354361355304718, d_loss2: 0.39538103342056274, g_loss: 16.036598205566406\n",
      "104 step d_loss1: 0.3948521018028259, d_loss2: 0.3885732591152191, g_loss: 10.915057182312012\n",
      "105 step d_loss1: 0.37360861897468567, d_loss2: 0.36905667185783386, g_loss: 9.198094367980957\n",
      "106 step d_loss1: 0.3756808638572693, d_loss2: 0.36374932527542114, g_loss: 14.643472671508789\n",
      "107 step d_loss1: 0.3565250635147095, d_loss2: 0.3815150856971741, g_loss: 12.124725341796875\n",
      "108 step d_loss1: 0.3641248643398285, d_loss2: 0.38434094190597534, g_loss: 11.487757682800293\n",
      "109 step d_loss1: 0.3387850522994995, d_loss2: 0.33679357171058655, g_loss: 15.39322566986084\n",
      "110 step d_loss1: 0.3444124460220337, d_loss2: 0.3490292429924011, g_loss: 12.854413986206055\n",
      "111 step d_loss1: 0.342184841632843, d_loss2: 0.36376845836639404, g_loss: 17.853431701660156\n",
      "112 step d_loss1: 0.3460403382778168, d_loss2: 0.39188334345817566, g_loss: 12.61915397644043\n",
      "113 step d_loss1: 0.3306521475315094, d_loss2: 0.32645902037620544, g_loss: 12.298274040222168\n",
      "114 step d_loss1: 0.33875584602355957, d_loss2: 0.39248934388160706, g_loss: 10.659950256347656\n",
      "115 step d_loss1: 0.38334494829177856, d_loss2: 0.38822096586227417, g_loss: 21.834148406982422\n",
      "116 step d_loss1: 0.39165475964546204, d_loss2: 0.43455153703689575, g_loss: 13.7323637008667\n",
      "117 step d_loss1: 0.34249556064605713, d_loss2: 0.3572819232940674, g_loss: 12.801950454711914\n",
      "118 step d_loss1: 0.3278503119945526, d_loss2: 0.31934913992881775, g_loss: 32.35111999511719\n",
      "119 step d_loss1: 0.3772834837436676, d_loss2: 0.36609983444213867, g_loss: 9.953530311584473\n",
      "120 step d_loss1: 0.3634584844112396, d_loss2: 0.37023985385894775, g_loss: 16.77849769592285\n",
      "121 step d_loss1: 0.34273791313171387, d_loss2: 0.3537452518939972, g_loss: 10.78210735321045\n",
      "122 step d_loss1: 0.3669200539588928, d_loss2: 0.40286922454833984, g_loss: 13.751611709594727\n",
      "123 step d_loss1: 0.3589599132537842, d_loss2: 0.3742842972278595, g_loss: 14.191694259643555\n",
      "124 step d_loss1: 0.3468100428581238, d_loss2: 0.32925140857696533, g_loss: 22.145503997802734\n",
      "125 step d_loss1: 0.3875304162502289, d_loss2: 0.3759447932243347, g_loss: 14.699298858642578\n",
      "126 step d_loss1: 0.32661885023117065, d_loss2: 0.3214935064315796, g_loss: 12.799753189086914\n",
      "127 step d_loss1: 0.3559024930000305, d_loss2: 0.3601047694683075, g_loss: 12.19953441619873\n",
      "128 step d_loss1: 0.32824644446372986, d_loss2: 0.480199933052063, g_loss: 14.645584106445312\n",
      "129 step d_loss1: 0.386491060256958, d_loss2: 0.3563331365585327, g_loss: 14.18111515045166\n",
      "130 step d_loss1: 0.41572076082229614, d_loss2: 0.37067416310310364, g_loss: 10.16025447845459\n",
      "131 step d_loss1: 0.361806720495224, d_loss2: 0.4245584011077881, g_loss: 10.448677062988281\n",
      "132 step d_loss1: 0.39248088002204895, d_loss2: 0.3678606152534485, g_loss: 10.512574195861816\n",
      "133 step d_loss1: 0.3803102970123291, d_loss2: 0.37593504786491394, g_loss: 12.414806365966797\n",
      "134 step d_loss1: 0.36290881037712097, d_loss2: 0.37218940258026123, g_loss: 13.732869148254395\n",
      "135 step d_loss1: 0.30770230293273926, d_loss2: 0.3245796859264374, g_loss: 30.053150177001953\n",
      "136 step d_loss1: 0.3889862596988678, d_loss2: 0.3407342731952667, g_loss: 21.570085525512695\n",
      "137 step d_loss1: 0.3778042197227478, d_loss2: 0.37530964612960815, g_loss: 9.559086799621582\n",
      "138 step d_loss1: 0.3364779055118561, d_loss2: 0.35234156250953674, g_loss: 12.37332820892334\n",
      "139 step d_loss1: 0.32161176204681396, d_loss2: 0.3087526559829712, g_loss: 28.172332763671875\n",
      "140 step d_loss1: 0.40002888441085815, d_loss2: 0.4101679027080536, g_loss: 11.614441871643066\n",
      "141 step d_loss1: 0.3804853558540344, d_loss2: 0.399563729763031, g_loss: 8.271374702453613\n",
      "142 step d_loss1: 0.38285964727401733, d_loss2: 0.34283551573753357, g_loss: 14.35048770904541\n",
      "143 step d_loss1: 0.364591509103775, d_loss2: 0.3822365403175354, g_loss: 11.427745819091797\n",
      "144 step d_loss1: 0.3722801208496094, d_loss2: 0.3801722526550293, g_loss: 10.443482398986816\n",
      "145 step d_loss1: 0.39723819494247437, d_loss2: 0.3771185874938965, g_loss: 15.379505157470703\n",
      "146 step d_loss1: 0.36523139476776123, d_loss2: 0.37639930844306946, g_loss: 10.630247116088867\n",
      "147 step d_loss1: 0.3600354790687561, d_loss2: 0.36610889434814453, g_loss: 9.531700134277344\n",
      "148 step d_loss1: 0.3430206775665283, d_loss2: 0.367099791765213, g_loss: 9.615433692932129\n",
      "149 step d_loss1: 0.3896595537662506, d_loss2: 0.36402055621147156, g_loss: 10.180639266967773\n",
      "150 step d_loss1: 0.39145994186401367, d_loss2: 0.3803534209728241, g_loss: 9.55370044708252\n",
      "151 step d_loss1: 0.3842446506023407, d_loss2: 0.37554994225502014, g_loss: 10.040338516235352\n",
      "152 step d_loss1: 0.359678715467453, d_loss2: 0.37236374616622925, g_loss: 9.347672462463379\n",
      "153 step d_loss1: 0.3549792468547821, d_loss2: 0.3764471411705017, g_loss: 9.596747398376465\n",
      "154 step d_loss1: 0.3056981563568115, d_loss2: 0.36437898874282837, g_loss: 14.100086212158203\n",
      "155 step d_loss1: 0.39665859937667847, d_loss2: 0.4146396219730377, g_loss: 12.893531799316406\n",
      "156 step d_loss1: 0.3338257670402527, d_loss2: 0.32831767201423645, g_loss: 11.320754051208496\n",
      "157 step d_loss1: 0.3425081968307495, d_loss2: 0.4874914288520813, g_loss: 11.728034973144531\n",
      "158 step d_loss1: 0.31487783789634705, d_loss2: 0.31623634696006775, g_loss: 20.39426040649414\n",
      "159 step d_loss1: 0.34961938858032227, d_loss2: 0.3258601129055023, g_loss: 14.764378547668457\n",
      "160 step d_loss1: 0.3722519278526306, d_loss2: 0.4177263379096985, g_loss: 8.973893165588379\n",
      "161 step d_loss1: 0.35136741399765015, d_loss2: 0.35267895460128784, g_loss: 23.773700714111328\n",
      "162 step d_loss1: 0.3616814911365509, d_loss2: 0.41257810592651367, g_loss: 9.535247802734375\n",
      "163 step d_loss1: 0.33699408173561096, d_loss2: 0.3789480924606323, g_loss: 10.584711074829102\n",
      "164 step d_loss1: 0.429140567779541, d_loss2: 0.35690462589263916, g_loss: 8.968574523925781\n",
      "165 step d_loss1: 0.3416393995285034, d_loss2: 0.32948604226112366, g_loss: 12.165811538696289\n",
      "166 step d_loss1: 0.3588409423828125, d_loss2: 0.39208120107650757, g_loss: 20.4693660736084\n",
      "167 step d_loss1: 0.32509124279022217, d_loss2: 0.36963579058647156, g_loss: 12.23515510559082\n",
      "168 step d_loss1: 0.36196014285087585, d_loss2: 0.3317009210586548, g_loss: 13.601752281188965\n",
      "169 step d_loss1: 0.3183680474758148, d_loss2: 0.4188302457332611, g_loss: 9.92246150970459\n",
      "170 step d_loss1: 0.4151846468448639, d_loss2: 0.39726921916007996, g_loss: 9.99201774597168\n",
      "171 step d_loss1: 0.40844234824180603, d_loss2: 0.3483433425426483, g_loss: 10.129440307617188\n",
      "172 step d_loss1: 0.379465788602829, d_loss2: 0.32394927740097046, g_loss: 10.716349601745605\n",
      "173 step d_loss1: 0.36527594923973083, d_loss2: 0.3418032228946686, g_loss: 10.017578125\n",
      "174 step d_loss1: 0.3881487250328064, d_loss2: 0.35689693689346313, g_loss: 10.09797477722168\n",
      "175 step d_loss1: 0.287765771150589, d_loss2: 0.3432312309741974, g_loss: 18.654441833496094\n",
      "176 step d_loss1: 0.2752003073692322, d_loss2: 0.2824186682701111, g_loss: 35.17319107055664\n",
      "177 step d_loss1: 0.33989185094833374, d_loss2: 0.3732539713382721, g_loss: 9.85012435913086\n",
      "178 step d_loss1: 0.35512828826904297, d_loss2: 0.30161991715431213, g_loss: 9.3666353225708\n",
      "179 step d_loss1: 0.3900502622127533, d_loss2: 0.35048314929008484, g_loss: 11.211518287658691\n",
      "180 step d_loss1: 0.25269123911857605, d_loss2: 0.39548343420028687, g_loss: 14.426702499389648\n",
      "181 step d_loss1: 0.3961826264858246, d_loss2: 0.557620108127594, g_loss: 9.811613082885742\n",
      "182 step d_loss1: 0.439829021692276, d_loss2: 0.34915849566459656, g_loss: 10.655389785766602\n",
      "183 step d_loss1: 0.3940197825431824, d_loss2: 0.3558077812194824, g_loss: 10.241252899169922\n",
      "184 step d_loss1: 0.3780169188976288, d_loss2: 0.376770555973053, g_loss: 6.69695520401001\n",
      "185 step d_loss1: 0.38154345750808716, d_loss2: 0.3206046521663666, g_loss: 17.97559356689453\n",
      "186 step d_loss1: 0.3631722927093506, d_loss2: 0.3475854694843292, g_loss: 10.644367218017578\n",
      "187 step d_loss1: 0.37971583008766174, d_loss2: 0.38034284114837646, g_loss: 11.38325023651123\n",
      "188 step d_loss1: 0.33950844407081604, d_loss2: 0.41669315099716187, g_loss: 8.99168872833252\n",
      "189 step d_loss1: 0.38133183121681213, d_loss2: 0.3760603070259094, g_loss: 9.154082298278809\n",
      "190 step d_loss1: 0.3483648896217346, d_loss2: 0.3696964383125305, g_loss: 15.560968399047852\n",
      "191 step d_loss1: 0.3968723714351654, d_loss2: 0.38957974314689636, g_loss: 11.727986335754395\n",
      "192 step d_loss1: 0.34909698367118835, d_loss2: 0.4067203104496002, g_loss: 10.44933032989502\n",
      "193 step d_loss1: 0.37979307770729065, d_loss2: 0.4127150774002075, g_loss: 10.161789894104004\n",
      "194 step d_loss1: 0.30437570810317993, d_loss2: 0.281436562538147, g_loss: 26.465068817138672\n",
      "195 step d_loss1: 0.36793315410614014, d_loss2: 0.3689761161804199, g_loss: 12.387691497802734\n",
      "196 step d_loss1: 0.33682551980018616, d_loss2: 0.4032968580722809, g_loss: 10.532552719116211\n",
      "197 step d_loss1: 0.35054147243499756, d_loss2: 0.39447689056396484, g_loss: 10.3534574508667\n",
      "198 step d_loss1: 0.42164742946624756, d_loss2: 0.34567567706108093, g_loss: 11.143335342407227\n",
      "199 step d_loss1: 0.3704482316970825, d_loss2: 0.3887377381324768, g_loss: 9.03923511505127\n",
      "200 step d_loss1: 0.3582146167755127, d_loss2: 0.38288018107414246, g_loss: 8.368006706237793\n",
      "201 step d_loss1: 0.3603760302066803, d_loss2: 0.3888179063796997, g_loss: 7.473691940307617\n",
      "202 step d_loss1: 0.39016085863113403, d_loss2: 0.3507724702358246, g_loss: 11.653229713439941\n",
      "203 step d_loss1: 0.3257242441177368, d_loss2: 0.34943997859954834, g_loss: 13.868759155273438\n",
      "204 step d_loss1: 0.3354850113391876, d_loss2: 0.38394761085510254, g_loss: 10.201085090637207\n",
      "205 step d_loss1: 0.35433709621429443, d_loss2: 0.3996410667896271, g_loss: 9.220232963562012\n",
      "206 step d_loss1: 0.39370185136795044, d_loss2: 0.37433359026908875, g_loss: 10.150297164916992\n",
      "207 step d_loss1: 0.387237012386322, d_loss2: 0.31983646750450134, g_loss: 10.92607307434082\n",
      "208 step d_loss1: 0.3111352026462555, d_loss2: 0.293338418006897, g_loss: 9.39694595336914\n",
      "209 step d_loss1: 0.3130854070186615, d_loss2: 0.3161173462867737, g_loss: 9.370854377746582\n",
      "210 step d_loss1: 0.40540385246276855, d_loss2: 0.42933839559555054, g_loss: 10.53850269317627\n",
      "211 step d_loss1: 0.3745480179786682, d_loss2: 0.40223824977874756, g_loss: 9.044978141784668\n",
      "212 step d_loss1: 0.38364335894584656, d_loss2: 0.38048550486564636, g_loss: 11.502230644226074\n",
      "213 step d_loss1: 0.38466447591781616, d_loss2: 0.3545854091644287, g_loss: 10.8954439163208\n",
      "214 step d_loss1: 0.3158254623413086, d_loss2: 0.43224066495895386, g_loss: 9.661691665649414\n",
      "215 step d_loss1: 0.3002663850784302, d_loss2: 0.4214198589324951, g_loss: 8.706172943115234\n",
      "216 step d_loss1: 0.37959787249565125, d_loss2: 0.3782675564289093, g_loss: 9.603780746459961\n",
      "217 step d_loss1: 0.29555219411849976, d_loss2: 0.43274250626564026, g_loss: 9.001168251037598\n",
      "218 step d_loss1: 0.3132004141807556, d_loss2: 0.40580785274505615, g_loss: 10.799917221069336\n",
      "219 step d_loss1: 0.3629569411277771, d_loss2: 0.35443437099456787, g_loss: 10.005105018615723\n",
      "220 step d_loss1: 0.36480575799942017, d_loss2: 0.3425862491130829, g_loss: 29.076202392578125\n",
      "221 step d_loss1: 0.3458835184574127, d_loss2: 0.3725249767303467, g_loss: 8.961085319519043\n",
      "222 step d_loss1: 0.28646165132522583, d_loss2: 0.5413684844970703, g_loss: 9.2483491897583\n",
      "223 step d_loss1: 0.41022738814353943, d_loss2: 0.3388039469718933, g_loss: 8.400677680969238\n",
      "224 step d_loss1: 0.40402647852897644, d_loss2: 0.3346590995788574, g_loss: 9.135432243347168\n",
      "225 step d_loss1: 0.38577961921691895, d_loss2: 0.33680325746536255, g_loss: 11.84488582611084\n",
      "226 step d_loss1: 0.34225648641586304, d_loss2: 0.3894822597503662, g_loss: 8.562042236328125\n",
      "227 step d_loss1: 0.37543100118637085, d_loss2: 0.3567412793636322, g_loss: 10.118386268615723\n",
      "228 step d_loss1: 0.3353302776813507, d_loss2: 0.3948736786842346, g_loss: 19.82949447631836\n",
      "229 step d_loss1: 0.3893035352230072, d_loss2: 0.35391467809677124, g_loss: 8.816329956054688\n",
      "230 step d_loss1: 0.36395490169525146, d_loss2: 0.3955954611301422, g_loss: 12.0797700881958\n",
      "231 step d_loss1: 0.299895703792572, d_loss2: 0.3631434142589569, g_loss: 14.901590347290039\n",
      "232 step d_loss1: 0.2871216833591461, d_loss2: 0.37077459692955017, g_loss: 13.507490158081055\n",
      "233 step d_loss1: 0.4426124393939972, d_loss2: 0.36111462116241455, g_loss: 13.147754669189453\n",
      "234 step d_loss1: 0.41879814863204956, d_loss2: 0.330865740776062, g_loss: 10.13370418548584\n",
      "235 step d_loss1: 0.4166097044944763, d_loss2: 0.3634842038154602, g_loss: 11.851181983947754\n",
      "236 step d_loss1: 0.2778994143009186, d_loss2: 0.45985156297683716, g_loss: 8.410995483398438\n",
      "237 step d_loss1: 0.3497641682624817, d_loss2: 0.3800979554653168, g_loss: 11.085458755493164\n",
      "238 step d_loss1: 0.4222191870212555, d_loss2: 0.3543567359447479, g_loss: 9.593661308288574\n",
      "239 step d_loss1: 0.30454105138778687, d_loss2: 0.3424195349216461, g_loss: 13.387487411499023\n",
      "240 step d_loss1: 0.35711759328842163, d_loss2: 0.3730432391166687, g_loss: 7.116275787353516\n",
      "241 step d_loss1: 0.3379039764404297, d_loss2: 0.3847351372241974, g_loss: 13.272710800170898\n",
      "242 step d_loss1: 0.23048827052116394, d_loss2: 0.4062218964099884, g_loss: 23.87520408630371\n",
      "243 step d_loss1: 0.303758442401886, d_loss2: 0.5156495571136475, g_loss: 11.632415771484375\n",
      "244 step d_loss1: 0.4350703954696655, d_loss2: 0.34199079871177673, g_loss: 9.4519624710083\n",
      "245 step d_loss1: 0.4652203321456909, d_loss2: 0.3100510835647583, g_loss: 11.387782096862793\n",
      "246 step d_loss1: 0.37207168340682983, d_loss2: 0.3760678172111511, g_loss: 9.005241394042969\n",
      "247 step d_loss1: 0.3248543441295624, d_loss2: 0.3053964376449585, g_loss: 33.248878479003906\n",
      "248 step d_loss1: 0.41019508242607117, d_loss2: 0.4246472418308258, g_loss: 13.248817443847656\n",
      "249 step d_loss1: 0.3486655354499817, d_loss2: 0.41488853096961975, g_loss: 11.09296989440918\n",
      "250 step d_loss1: 0.37954074144363403, d_loss2: 0.3797663450241089, g_loss: 10.301008224487305\n",
      "251 step d_loss1: 0.3510275185108185, d_loss2: 0.3926050662994385, g_loss: 9.131114959716797\n",
      "252 step d_loss1: 0.36975663900375366, d_loss2: 0.3799575865268707, g_loss: 8.84451675415039\n",
      "253 step d_loss1: 0.33787432312965393, d_loss2: 0.36558032035827637, g_loss: 8.583507537841797\n",
      "254 step d_loss1: 0.35003504157066345, d_loss2: 0.43265819549560547, g_loss: 9.237229347229004\n",
      "255 step d_loss1: 0.41890236735343933, d_loss2: 0.3526970148086548, g_loss: 9.050861358642578\n",
      "256 step d_loss1: 0.3564252257347107, d_loss2: 0.3706398904323578, g_loss: 8.212766647338867\n",
      "257 step d_loss1: 0.3383074700832367, d_loss2: 0.3460031747817993, g_loss: 8.907886505126953\n",
      "258 step d_loss1: 0.4323307275772095, d_loss2: 0.3695376515388489, g_loss: 7.3504815101623535\n",
      "259 step d_loss1: 0.21166327595710754, d_loss2: 0.5747297406196594, g_loss: 9.452988624572754\n",
      "260 step d_loss1: 0.42768558859825134, d_loss2: 0.4134705662727356, g_loss: 21.7476749420166\n",
      "261 step d_loss1: 0.3873603641986847, d_loss2: 0.5239725708961487, g_loss: 7.028632164001465\n",
      "262 step d_loss1: 0.3802386522293091, d_loss2: 0.3844589591026306, g_loss: 10.561482429504395\n",
      "263 step d_loss1: 0.4480675160884857, d_loss2: 0.34333983063697815, g_loss: 10.326318740844727\n",
      "264 step d_loss1: 0.373546302318573, d_loss2: 0.35317128896713257, g_loss: 17.04949378967285\n",
      "265 step d_loss1: 0.4430059790611267, d_loss2: 0.32914671301841736, g_loss: 10.079736709594727\n",
      "266 step d_loss1: 0.4136578142642975, d_loss2: 0.3581167161464691, g_loss: 9.652068138122559\n",
      "267 step d_loss1: 0.3883363604545593, d_loss2: 0.3699115514755249, g_loss: 7.960149765014648\n",
      "268 step d_loss1: 0.35311102867126465, d_loss2: 0.3870316445827484, g_loss: 8.07213306427002\n",
      "269 step d_loss1: 0.38019898533821106, d_loss2: 0.36509376764297485, g_loss: 9.181639671325684\n",
      "270 step d_loss1: 0.3684195578098297, d_loss2: 0.37825924158096313, g_loss: 9.079062461853027\n",
      "271 step d_loss1: 0.334821492433548, d_loss2: 0.3862563371658325, g_loss: 12.973895072937012\n",
      "272 step d_loss1: 0.3719165623188019, d_loss2: 0.37365952134132385, g_loss: 14.615108489990234\n",
      "273 step d_loss1: 0.3578433692455292, d_loss2: 0.3814818561077118, g_loss: 11.233423233032227\n",
      "274 step d_loss1: 0.3257983922958374, d_loss2: 0.4064461886882782, g_loss: 20.273080825805664\n",
      "275 step d_loss1: 0.378741592168808, d_loss2: 0.36549073457717896, g_loss: 11.172672271728516\n",
      "276 step d_loss1: 0.38122740387916565, d_loss2: 0.38372528553009033, g_loss: 12.004566192626953\n",
      "277 step d_loss1: 0.37220829725265503, d_loss2: 0.3682909309864044, g_loss: 9.251969337463379\n",
      "278 step d_loss1: 0.39067694544792175, d_loss2: 0.3685574531555176, g_loss: 10.996245384216309\n",
      "279 step d_loss1: 0.35833540558815, d_loss2: 0.37983837723731995, g_loss: 21.17913246154785\n",
      "280 step d_loss1: 0.4018961191177368, d_loss2: 0.35588541626930237, g_loss: 11.287988662719727\n",
      "281 step d_loss1: 0.3848128914833069, d_loss2: 0.36943894624710083, g_loss: 8.05654525756836\n",
      "282 step d_loss1: 0.35805487632751465, d_loss2: 0.3732786178588867, g_loss: 10.197403907775879\n",
      "283 step d_loss1: 0.37714529037475586, d_loss2: 0.3775821328163147, g_loss: 8.286285400390625\n",
      "284 step d_loss1: 0.36513957381248474, d_loss2: 0.37603601813316345, g_loss: 8.339171409606934\n",
      "285 step d_loss1: 0.3148115873336792, d_loss2: 0.37185418605804443, g_loss: 16.19874382019043\n",
      "286 step d_loss1: 0.3617209792137146, d_loss2: 0.3670938014984131, g_loss: 9.463534355163574\n",
      "287 step d_loss1: 0.36884912848472595, d_loss2: 0.36239704489707947, g_loss: 10.138578414916992\n",
      "288 step d_loss1: 0.364734023809433, d_loss2: 0.36536169052124023, g_loss: 11.859027862548828\n",
      "289 step d_loss1: 0.3732531666755676, d_loss2: 0.35656604170799255, g_loss: 8.773958206176758\n",
      "290 step d_loss1: 0.36183983087539673, d_loss2: 0.37710875272750854, g_loss: 10.656543731689453\n",
      "291 step d_loss1: 0.3717433214187622, d_loss2: 0.36723580956459045, g_loss: 9.528373718261719\n",
      "292 step d_loss1: 0.331026554107666, d_loss2: 0.39935633540153503, g_loss: 10.78044605255127\n",
      "293 step d_loss1: 0.3826526701450348, d_loss2: 0.3880327045917511, g_loss: 9.664103507995605\n",
      "294 step d_loss1: 0.3716171383857727, d_loss2: 0.3604799807071686, g_loss: 11.378584861755371\n",
      "295 step d_loss1: 0.37625443935394287, d_loss2: 0.37227219343185425, g_loss: 10.293708801269531\n",
      "296 step d_loss1: 0.3732728362083435, d_loss2: 0.37170886993408203, g_loss: 10.380879402160645\n",
      "297 step d_loss1: 0.34732240438461304, d_loss2: 0.3908701539039612, g_loss: 10.46214771270752\n",
      "298 step d_loss1: 0.3613537847995758, d_loss2: 0.3749620318412781, g_loss: 10.079936027526855\n",
      "299 step d_loss1: 0.36518457531929016, d_loss2: 0.3705301284790039, g_loss: 8.298981666564941\n",
      "300 step d_loss1: 0.37813109159469604, d_loss2: 0.363709032535553, g_loss: 9.88241958618164\n",
      "301 step d_loss1: 0.4019381105899811, d_loss2: 0.3545275926589966, g_loss: 7.634028911590576\n",
      "302 step d_loss1: 0.35129567980766296, d_loss2: 0.3752884268760681, g_loss: 8.874281883239746\n",
      "303 step d_loss1: 0.38229241967201233, d_loss2: 0.3635908365249634, g_loss: 8.142800331115723\n",
      "304 step d_loss1: 0.342143177986145, d_loss2: 0.38270288705825806, g_loss: 11.33561897277832\n",
      "305 step d_loss1: 0.3907238841056824, d_loss2: 0.3621295690536499, g_loss: 8.181498527526855\n",
      "306 step d_loss1: 0.35641300678253174, d_loss2: 0.386381059885025, g_loss: 7.205495834350586\n",
      "307 step d_loss1: 0.39198872447013855, d_loss2: 0.36443892121315, g_loss: 7.529202938079834\n",
      "308 step d_loss1: 0.34073004126548767, d_loss2: 0.39093053340911865, g_loss: 10.39553451538086\n",
      "309 step d_loss1: 0.3711625933647156, d_loss2: 0.3658203184604645, g_loss: 11.078314781188965\n",
      "310 step d_loss1: 0.32022300362586975, d_loss2: 0.4128778576850891, g_loss: 11.719616889953613\n",
      "311 step d_loss1: 0.3833330273628235, d_loss2: 0.37105804681777954, g_loss: 8.884939193725586\n",
      "312 step d_loss1: 0.3623018264770508, d_loss2: 0.3701343834400177, g_loss: 9.064499855041504\n",
      "313 step d_loss1: 0.3510099947452545, d_loss2: 0.3784253001213074, g_loss: 8.541949272155762\n",
      "314 step d_loss1: 0.3808727264404297, d_loss2: 0.3619607985019684, g_loss: 8.074172019958496\n",
      "315 step d_loss1: 0.3881053030490875, d_loss2: 0.3573947548866272, g_loss: 8.496194839477539\n",
      "316 step d_loss1: 0.33782631158828735, d_loss2: 0.4072958528995514, g_loss: 7.550188064575195\n",
      "317 step d_loss1: 0.35930314660072327, d_loss2: 0.38803911209106445, g_loss: 7.500625133514404\n",
      "318 step d_loss1: 0.36477982997894287, d_loss2: 0.3886825442314148, g_loss: 11.003519058227539\n",
      "319 step d_loss1: 0.40473634004592896, d_loss2: 0.35048815608024597, g_loss: 6.496276378631592\n",
      "320 step d_loss1: 0.37399721145629883, d_loss2: 0.36660343408584595, g_loss: 7.223796367645264\n",
      "321 step d_loss1: 0.3816015124320984, d_loss2: 0.37003666162490845, g_loss: 7.458421230316162\n",
      "322 step d_loss1: 0.34815043210983276, d_loss2: 0.3957398533821106, g_loss: 9.31779956817627\n",
      "323 step d_loss1: 0.34898507595062256, d_loss2: 0.3863769769668579, g_loss: 8.469596862792969\n",
      "324 step d_loss1: 0.3561951518058777, d_loss2: 0.4205639362335205, g_loss: 10.114258766174316\n",
      "325 step d_loss1: 0.36132943630218506, d_loss2: 0.367018461227417, g_loss: 8.232961654663086\n",
      "326 step d_loss1: 0.352524071931839, d_loss2: 0.3726702630519867, g_loss: 8.061493873596191\n",
      "327 step d_loss1: 0.3820408284664154, d_loss2: 0.3619019687175751, g_loss: 8.266902923583984\n",
      "328 step d_loss1: 0.319888174533844, d_loss2: 0.41274064779281616, g_loss: 22.46050453186035\n",
      "329 step d_loss1: 0.37957465648651123, d_loss2: 0.36836281418800354, g_loss: 8.533834457397461\n",
      "330 step d_loss1: 0.4183170795440674, d_loss2: 0.34294864535331726, g_loss: 7.418245315551758\n",
      "331 step d_loss1: 0.3584813177585602, d_loss2: 0.3897542357444763, g_loss: 7.770153522491455\n",
      "332 step d_loss1: 0.3230826258659363, d_loss2: 0.3870910406112671, g_loss: 11.603991508483887\n",
      "333 step d_loss1: 0.3547874093055725, d_loss2: 0.35551583766937256, g_loss: 10.635889053344727\n",
      "334 step d_loss1: 0.37183818221092224, d_loss2: 0.3745863139629364, g_loss: 10.498614311218262\n",
      "335 step d_loss1: 0.36035680770874023, d_loss2: 0.383888840675354, g_loss: 9.706510543823242\n",
      "336 step d_loss1: 0.385093629360199, d_loss2: 0.36361098289489746, g_loss: 7.381937503814697\n",
      "337 step d_loss1: 0.3445751965045929, d_loss2: 0.3927233815193176, g_loss: 9.134062767028809\n",
      "338 step d_loss1: 0.31088945269584656, d_loss2: 0.4018259048461914, g_loss: 7.638150215148926\n",
      "339 step d_loss1: 0.2850553095340729, d_loss2: 0.370407372713089, g_loss: 15.832168579101562\n",
      "340 step d_loss1: 0.41161835193634033, d_loss2: 0.3417671322822571, g_loss: 7.944380760192871\n",
      "341 step d_loss1: 0.3840242624282837, d_loss2: 0.35323697328567505, g_loss: 12.599895477294922\n",
      "342 step d_loss1: 0.2812231183052063, d_loss2: 0.39164143800735474, g_loss: 15.760271072387695\n",
      "343 step d_loss1: 0.38715824484825134, d_loss2: 0.3388538956642151, g_loss: 9.321731567382812\n",
      "344 step d_loss1: 0.3849872350692749, d_loss2: 0.3612806797027588, g_loss: 11.01240062713623\n",
      "345 step d_loss1: 0.4293104410171509, d_loss2: 0.3480469286441803, g_loss: 9.007588386535645\n",
      "346 step d_loss1: 0.39556175470352173, d_loss2: 0.3612504005432129, g_loss: 8.84786605834961\n",
      "347 step d_loss1: 0.32822781801223755, d_loss2: 0.40152671933174133, g_loss: 9.921517372131348\n",
      "348 step d_loss1: 0.30314403772354126, d_loss2: 0.4353901147842407, g_loss: 9.705415725708008\n",
      "349 step d_loss1: 0.30747559666633606, d_loss2: 0.4069693684577942, g_loss: 17.95613670349121\n",
      "350 step d_loss1: 0.4327736496925354, d_loss2: 0.3351781964302063, g_loss: 8.07451343536377\n",
      "351 step d_loss1: 0.4035155475139618, d_loss2: 0.35387083888053894, g_loss: 8.805991172790527\n",
      "352 step d_loss1: 0.3384968042373657, d_loss2: 0.3996824622154236, g_loss: 11.6327486038208\n",
      "353 step d_loss1: 0.3848535120487213, d_loss2: 0.3575100898742676, g_loss: 9.387991905212402\n",
      "354 step d_loss1: 0.399267315864563, d_loss2: 0.3601613938808441, g_loss: 7.368180274963379\n",
      "355 step d_loss1: 0.35396066308021545, d_loss2: 0.3707113564014435, g_loss: 8.083833694458008\n",
      "356 step d_loss1: 0.377898633480072, d_loss2: 0.388217955827713, g_loss: 7.730635166168213\n",
      "357 step d_loss1: 0.3271252512931824, d_loss2: 0.40912145376205444, g_loss: 8.107217788696289\n",
      "358 step d_loss1: 0.3497360646724701, d_loss2: 0.3854844570159912, g_loss: 6.317209720611572\n",
      "359 step d_loss1: 0.3531707525253296, d_loss2: 0.3766312003135681, g_loss: 7.435853958129883\n",
      "360 step d_loss1: 0.43025290966033936, d_loss2: 0.35048919916152954, g_loss: 6.9793596267700195\n",
      "361 step d_loss1: 0.3420167565345764, d_loss2: 0.39986124634742737, g_loss: 11.801037788391113\n",
      "362 step d_loss1: 0.3777037262916565, d_loss2: 0.37484654784202576, g_loss: 8.87283992767334\n",
      "363 step d_loss1: 0.3299315571784973, d_loss2: 0.40914469957351685, g_loss: 8.962058067321777\n",
      "364 step d_loss1: 0.3952295184135437, d_loss2: 0.362334668636322, g_loss: 6.3018341064453125\n",
      "365 step d_loss1: 0.36249521374702454, d_loss2: 0.36899155378341675, g_loss: 8.400062561035156\n",
      "366 step d_loss1: 0.3484756052494049, d_loss2: 0.39086025953292847, g_loss: 7.844173431396484\n",
      "367 step d_loss1: 0.3340964615345001, d_loss2: 0.3867344856262207, g_loss: 9.454120635986328\n",
      "368 step d_loss1: 0.37934574484825134, d_loss2: 0.3741576373577118, g_loss: 7.777561664581299\n",
      "369 step d_loss1: 0.36720582842826843, d_loss2: 0.3711821436882019, g_loss: 9.171177864074707\n",
      "370 step d_loss1: 0.36895084381103516, d_loss2: 0.3716319799423218, g_loss: 7.798686504364014\n",
      "371 step d_loss1: 0.3472483158111572, d_loss2: 0.37545230984687805, g_loss: 10.626907348632812\n",
      "372 step d_loss1: 0.3086916208267212, d_loss2: 0.41338256001472473, g_loss: 8.109363555908203\n",
      "373 step d_loss1: 0.35541677474975586, d_loss2: 0.3722934126853943, g_loss: 8.643498420715332\n",
      "374 step d_loss1: 0.42417144775390625, d_loss2: 0.3457934260368347, g_loss: 7.56394624710083\n",
      "375 step d_loss1: 0.39234477281570435, d_loss2: 0.34290894865989685, g_loss: 9.811819076538086\n",
      "376 step d_loss1: 0.35844671726226807, d_loss2: 0.37429678440093994, g_loss: 11.677337646484375\n",
      "377 step d_loss1: 0.3451172113418579, d_loss2: 0.36131539940834045, g_loss: 41.486549377441406\n",
      "378 step d_loss1: 0.3570757508277893, d_loss2: 0.3973402678966522, g_loss: 8.1652193069458\n",
      "379 step d_loss1: 0.3482052683830261, d_loss2: 0.4247841536998749, g_loss: 9.616046905517578\n",
      "380 step d_loss1: 0.35535305738449097, d_loss2: 0.4129762053489685, g_loss: 11.473665237426758\n",
      "381 step d_loss1: 0.3863146901130676, d_loss2: 0.35415464639663696, g_loss: 10.642964363098145\n",
      "382 step d_loss1: 0.35965341329574585, d_loss2: 0.3790961503982544, g_loss: 12.957907676696777\n",
      "383 step d_loss1: 0.37261250615119934, d_loss2: 0.36796629428863525, g_loss: 9.123007774353027\n",
      "384 step d_loss1: 0.3576805293560028, d_loss2: 0.3729861080646515, g_loss: 10.005030632019043\n",
      "385 step d_loss1: 0.39832279086112976, d_loss2: 0.343014121055603, g_loss: 7.1924519538879395\n",
      "386 step d_loss1: 0.35210496187210083, d_loss2: 0.3694309890270233, g_loss: 10.860136985778809\n",
      "387 step d_loss1: 0.3322542905807495, d_loss2: 0.40728092193603516, g_loss: 8.317367553710938\n",
      "388 step d_loss1: 0.38241153955459595, d_loss2: 0.34706491231918335, g_loss: 8.082846641540527\n",
      "389 step d_loss1: 0.3175617456436157, d_loss2: 0.3711881935596466, g_loss: 11.163941383361816\n",
      "390 step d_loss1: 0.3875657916069031, d_loss2: 0.34315037727355957, g_loss: 8.330977439880371\n",
      "391 step d_loss1: 0.3026648163795471, d_loss2: 0.36159342527389526, g_loss: 10.9747896194458\n",
      "392 step d_loss1: 0.3609115481376648, d_loss2: 0.39667758345603943, g_loss: 7.728433609008789\n",
      "393 step d_loss1: 0.3659164011478424, d_loss2: 0.4351707100868225, g_loss: 6.845253944396973\n",
      "394 step d_loss1: 0.3750235438346863, d_loss2: 0.41196438670158386, g_loss: 11.008548736572266\n",
      "395 step d_loss1: 0.3603760898113251, d_loss2: 0.36257240176200867, g_loss: 10.81011962890625\n",
      "396 step d_loss1: 0.3661291003227234, d_loss2: 0.36705684661865234, g_loss: 9.087502479553223\n",
      "397 step d_loss1: 0.3283648192882538, d_loss2: 0.3725694417953491, g_loss: 10.718924522399902\n",
      "398 step d_loss1: 0.3276098668575287, d_loss2: 0.3775348663330078, g_loss: 12.835653305053711\n",
      "399 step d_loss1: 0.39726489782333374, d_loss2: 0.345653772354126, g_loss: 10.064208984375\n",
      "400 step d_loss1: 0.316070556640625, d_loss2: 0.37567561864852905, g_loss: 11.05855941772461\n",
      "401 step d_loss1: 0.34569525718688965, d_loss2: 0.369815856218338, g_loss: 17.9948673248291\n",
      "402 step d_loss1: 0.37140747904777527, d_loss2: 0.3482270836830139, g_loss: 9.971837997436523\n",
      "403 step d_loss1: 0.4023698568344116, d_loss2: 0.3558836579322815, g_loss: 10.272953033447266\n",
      "404 step d_loss1: 0.3336549997329712, d_loss2: 0.36674046516418457, g_loss: 9.246156692504883\n",
      "405 step d_loss1: 0.3920924663543701, d_loss2: 0.37779873609542847, g_loss: 8.788615226745605\n",
      "406 step d_loss1: 0.3168971538543701, d_loss2: 0.3412048816680908, g_loss: 7.602584362030029\n",
      "407 step d_loss1: 0.3756600618362427, d_loss2: 0.37836307287216187, g_loss: 8.375285148620605\n",
      "408 step d_loss1: 0.3370773494243622, d_loss2: 0.4649553894996643, g_loss: 10.231908798217773\n",
      "409 step d_loss1: 0.3791298270225525, d_loss2: 0.41471120715141296, g_loss: 11.080073356628418\n",
      "410 step d_loss1: 0.35589903593063354, d_loss2: 0.3772614896297455, g_loss: 9.371792793273926\n",
      "411 step d_loss1: 0.3087092339992523, d_loss2: 0.40487873554229736, g_loss: 21.44611930847168\n",
      "412 step d_loss1: 0.30759167671203613, d_loss2: 0.27616652846336365, g_loss: 9.608450889587402\n",
      "413 step d_loss1: 0.4222410321235657, d_loss2: 0.2869097590446472, g_loss: 16.951614379882812\n",
      "414 step d_loss1: 0.3863570988178253, d_loss2: 0.38765594363212585, g_loss: 16.065195083618164\n",
      "415 step d_loss1: 0.4792023003101349, d_loss2: 0.3135671317577362, g_loss: 9.916792869567871\n",
      "416 step d_loss1: 0.37640541791915894, d_loss2: 0.36162543296813965, g_loss: 9.349230766296387\n",
      "417 step d_loss1: 0.3719169497489929, d_loss2: 0.3634129762649536, g_loss: 8.506595611572266\n",
      "418 step d_loss1: 0.31890904903411865, d_loss2: 0.4569433927536011, g_loss: 20.571794509887695\n",
      "419 step d_loss1: 0.365043580532074, d_loss2: 0.35013189911842346, g_loss: 8.343281745910645\n",
      "420 step d_loss1: 0.3507927358150482, d_loss2: 0.398509681224823, g_loss: 11.425020217895508\n",
      "421 step d_loss1: 0.30358076095581055, d_loss2: 0.41772782802581787, g_loss: 9.606532096862793\n",
      "422 step d_loss1: 0.3864832818508148, d_loss2: 0.5080658197402954, g_loss: 8.243460655212402\n",
      "423 step d_loss1: 0.41382676362991333, d_loss2: 0.5605659484863281, g_loss: 10.345734596252441\n",
      "424 step d_loss1: 0.3818807005882263, d_loss2: 0.39767399430274963, g_loss: 7.933494567871094\n",
      "425 step d_loss1: 0.4784663915634155, d_loss2: 0.32971543073654175, g_loss: 8.37933349609375\n",
      "426 step d_loss1: 0.4360860586166382, d_loss2: 0.3397296965122223, g_loss: 7.343721389770508\n",
      "427 step d_loss1: 0.38024914264678955, d_loss2: 0.37213587760925293, g_loss: 8.14622974395752\n",
      "428 step d_loss1: 0.3552156090736389, d_loss2: 0.39888066053390503, g_loss: 9.714034080505371\n",
      "429 step d_loss1: 0.37935590744018555, d_loss2: 0.37348586320877075, g_loss: 8.10032844543457\n",
      "430 step d_loss1: 0.37303027510643005, d_loss2: 0.386494517326355, g_loss: 8.736762046813965\n",
      "431 step d_loss1: 0.36697036027908325, d_loss2: 0.3935520052909851, g_loss: 8.313858985900879\n",
      "432 step d_loss1: 0.3321874737739563, d_loss2: 0.3715255856513977, g_loss: 18.90508270263672\n",
      "433 step d_loss1: 0.3629247546195984, d_loss2: 0.3758191764354706, g_loss: 8.490854263305664\n",
      "434 step d_loss1: 0.3451184332370758, d_loss2: 0.3896658718585968, g_loss: 9.982414245605469\n",
      "435 step d_loss1: 0.400555819272995, d_loss2: 0.3515707552433014, g_loss: 9.092191696166992\n",
      "436 step d_loss1: 0.3933965563774109, d_loss2: 0.3547526001930237, g_loss: 10.770329475402832\n",
      "437 step d_loss1: 0.36919429898262024, d_loss2: 0.3782162666320801, g_loss: 8.1615571975708\n",
      "438 step d_loss1: 0.3698466420173645, d_loss2: 0.383388489484787, g_loss: 9.55125904083252\n",
      "439 step d_loss1: 0.36879974603652954, d_loss2: 0.37793028354644775, g_loss: 8.172456741333008\n",
      "440 step d_loss1: 0.360856294631958, d_loss2: 0.3840266466140747, g_loss: 6.670760154724121\n",
      "441 step d_loss1: 0.3725913465023041, d_loss2: 0.3896443247795105, g_loss: 9.810566902160645\n",
      "442 step d_loss1: 0.39210787415504456, d_loss2: 0.3647676110267639, g_loss: 5.96872091293335\n",
      "443 step d_loss1: 0.3713003695011139, d_loss2: 0.3794088065624237, g_loss: 9.084504127502441\n",
      "444 step d_loss1: 0.35691556334495544, d_loss2: 0.39853158593177795, g_loss: 10.842894554138184\n",
      "445 step d_loss1: 0.3683154582977295, d_loss2: 0.38316771388053894, g_loss: 7.701662063598633\n",
      "446 step d_loss1: 0.3455283045768738, d_loss2: 0.3844860792160034, g_loss: 8.5540189743042\n",
      "447 step d_loss1: 0.3697761297225952, d_loss2: 0.394953191280365, g_loss: 22.271284103393555\n",
      "448 step d_loss1: 0.3678115904331207, d_loss2: 0.37982937693595886, g_loss: 9.115790367126465\n",
      "449 step d_loss1: 0.38725364208221436, d_loss2: 0.37090176343917847, g_loss: 10.8336820602417\n",
      "450 step d_loss1: 0.3491138815879822, d_loss2: 0.3887888789176941, g_loss: 19.80670738220215\n",
      "451 step d_loss1: 0.3736025393009186, d_loss2: 0.37116748094558716, g_loss: 10.360799789428711\n",
      "452 step d_loss1: 0.36504971981048584, d_loss2: 0.378353476524353, g_loss: 8.441047668457031\n",
      "453 step d_loss1: 0.38366803526878357, d_loss2: 0.36686328053474426, g_loss: 12.02598762512207\n",
      "454 step d_loss1: 0.3065696358680725, d_loss2: 0.42753541469573975, g_loss: 9.828702926635742\n",
      "455 step d_loss1: 0.3819105327129364, d_loss2: 0.3775268793106079, g_loss: 8.617082595825195\n",
      "456 step d_loss1: 0.38165897130966187, d_loss2: 0.3716525733470917, g_loss: 11.029594421386719\n",
      "457 step d_loss1: 0.3649948239326477, d_loss2: 0.37787526845932007, g_loss: 10.662287712097168\n",
      "458 step d_loss1: 0.3647545874118805, d_loss2: 0.3796749711036682, g_loss: 8.817683219909668\n",
      "459 step d_loss1: 0.37187424302101135, d_loss2: 0.3709309995174408, g_loss: 9.393484115600586\n",
      "460 step d_loss1: 0.37067654728889465, d_loss2: 0.37098148465156555, g_loss: 7.569211483001709\n",
      "461 step d_loss1: 0.36887824535369873, d_loss2: 0.37219634652137756, g_loss: 9.006180763244629\n",
      "462 step d_loss1: 0.3705597221851349, d_loss2: 0.37512874603271484, g_loss: 7.783195495605469\n",
      "463 step d_loss1: 0.33027634024620056, d_loss2: 0.39275413751602173, g_loss: 21.23851776123047\n",
      "464 step d_loss1: 0.364102303981781, d_loss2: 0.3724348247051239, g_loss: 9.57422161102295\n",
      "465 step d_loss1: 0.38594284653663635, d_loss2: 0.3631277084350586, g_loss: 8.88609790802002\n",
      "466 step d_loss1: 0.37549465894699097, d_loss2: 0.37223368883132935, g_loss: 9.399611473083496\n",
      "467 step d_loss1: 0.3751712739467621, d_loss2: 0.36377811431884766, g_loss: 6.713740348815918\n",
      "468 step d_loss1: 0.3775672912597656, d_loss2: 0.37149205803871155, g_loss: 9.111018180847168\n",
      "469 step d_loss1: 0.34347182512283325, d_loss2: 0.3970373570919037, g_loss: 9.557208061218262\n",
      "470 step d_loss1: 0.36700722575187683, d_loss2: 0.3694269061088562, g_loss: 7.192306995391846\n",
      "471 step d_loss1: 0.3774736523628235, d_loss2: 0.37301206588745117, g_loss: 7.13106107711792\n",
      "472 step d_loss1: 0.3714463412761688, d_loss2: 0.3703973591327667, g_loss: 7.5976691246032715\n",
      "473 step d_loss1: 0.35719791054725647, d_loss2: 0.37014976143836975, g_loss: 8.432984352111816\n",
      "474 step d_loss1: 0.3341686427593231, d_loss2: 0.399661123752594, g_loss: 6.47634744644165\n",
      "475 step d_loss1: 0.38774245977401733, d_loss2: 0.3612143397331238, g_loss: 6.654017925262451\n",
      "476 step d_loss1: 0.376476913690567, d_loss2: 0.36120668053627014, g_loss: 7.046477794647217\n",
      "477 step d_loss1: 0.34053727984428406, d_loss2: 0.3774491250514984, g_loss: 15.1598482131958\n",
      "478 step d_loss1: 0.3703649640083313, d_loss2: 0.3835374414920807, g_loss: 8.43755054473877\n",
      "479 step d_loss1: 0.3558031916618347, d_loss2: 0.3432525098323822, g_loss: 7.328834056854248\n",
      "480 step d_loss1: 0.34753453731536865, d_loss2: 0.39944741129875183, g_loss: 8.496283531188965\n",
      "481 step d_loss1: 0.2893367409706116, d_loss2: 0.3791605830192566, g_loss: 9.624138832092285\n",
      "482 step d_loss1: 0.27194949984550476, d_loss2: 0.3199840784072876, g_loss: 8.024456977844238\n",
      "483 step d_loss1: 0.33140528202056885, d_loss2: 0.3719097375869751, g_loss: 8.528815269470215\n",
      "484 step d_loss1: 0.37189561128616333, d_loss2: 0.32293444871902466, g_loss: 9.9485445022583\n",
      "485 step d_loss1: 0.32762962579727173, d_loss2: 0.3608759641647339, g_loss: 9.302879333496094\n",
      "486 step d_loss1: 0.39298996329307556, d_loss2: 0.4001544713973999, g_loss: 7.705073833465576\n",
      "487 step d_loss1: 0.38992902636528015, d_loss2: 0.32449665665626526, g_loss: 6.8636980056762695\n",
      "488 step d_loss1: 0.3785877823829651, d_loss2: 0.32552310824394226, g_loss: 10.872577667236328\n",
      "489 step d_loss1: 0.30640292167663574, d_loss2: 0.35879531502723694, g_loss: 9.772527694702148\n",
      "490 step d_loss1: 0.3824981451034546, d_loss2: 0.3139587640762329, g_loss: 8.620414733886719\n",
      "491 step d_loss1: 0.26322489976882935, d_loss2: 0.3844240605831146, g_loss: 7.800253868103027\n",
      "492 step d_loss1: 0.2965617775917053, d_loss2: 0.37046322226524353, g_loss: 8.813472747802734\n",
      "493 step d_loss1: 0.2742796242237091, d_loss2: 0.31517037749290466, g_loss: 9.131453514099121\n",
      "494 step d_loss1: 0.37231624126434326, d_loss2: 0.3598022758960724, g_loss: 7.0325398445129395\n",
      "495 step d_loss1: 0.32894963026046753, d_loss2: 0.32720693945884705, g_loss: 8.620949745178223\n",
      "496 step d_loss1: 0.39968329668045044, d_loss2: 0.4135169982910156, g_loss: 7.859259605407715\n",
      "497 step d_loss1: 0.35792097449302673, d_loss2: 0.3294435441493988, g_loss: 10.495735168457031\n",
      "498 step d_loss1: 0.3140796422958374, d_loss2: 0.3489322364330292, g_loss: 8.287413597106934\n",
      "499 step d_loss1: 0.3640235960483551, d_loss2: 0.3017778694629669, g_loss: 17.45140838623047\n",
      "500 step d_loss1: 0.44671881198883057, d_loss2: 0.32602810859680176, g_loss: 9.790916442871094\n",
      "501 step d_loss1: 0.40148550271987915, d_loss2: 0.3259122371673584, g_loss: 8.121262550354004\n",
      "502 step d_loss1: 0.27377933263778687, d_loss2: 0.4077635705471039, g_loss: 10.38627815246582\n",
      "503 step d_loss1: 0.35120534896850586, d_loss2: 0.36221179366111755, g_loss: 7.271785736083984\n",
      "504 step d_loss1: 0.36170920729637146, d_loss2: 0.3283423185348511, g_loss: 10.883387565612793\n",
      "505 step d_loss1: 0.4293588101863861, d_loss2: 0.35821735858917236, g_loss: 7.093912124633789\n",
      "506 step d_loss1: 0.3979845345020294, d_loss2: 0.35930776596069336, g_loss: 6.355286121368408\n",
      "507 step d_loss1: 0.361444354057312, d_loss2: 0.35615500807762146, g_loss: 7.814837455749512\n",
      "508 step d_loss1: 0.3686119616031647, d_loss2: 0.36414670944213867, g_loss: 11.209273338317871\n",
      "509 step d_loss1: 0.2968449890613556, d_loss2: 0.3438609838485718, g_loss: 29.79671287536621\n",
      "510 step d_loss1: 0.3782597780227661, d_loss2: 0.4246177077293396, g_loss: 9.1033353805542\n",
      "511 step d_loss1: 0.27037084102630615, d_loss2: 0.34789860248565674, g_loss: 9.471832275390625\n",
      "512 step d_loss1: 0.2990522086620331, d_loss2: 0.3628385663032532, g_loss: 11.284299850463867\n",
      "513 step d_loss1: 0.38640648126602173, d_loss2: 0.3198545575141907, g_loss: 8.344895362854004\n",
      "514 step d_loss1: 0.44633668661117554, d_loss2: 0.3355357050895691, g_loss: 7.38140869140625\n",
      "515 step d_loss1: 0.4102533459663391, d_loss2: 0.349753737449646, g_loss: 7.005706787109375\n",
      "516 step d_loss1: 0.2655668556690216, d_loss2: 0.37057554721832275, g_loss: 15.683843612670898\n",
      "517 step d_loss1: 0.3809583783149719, d_loss2: 0.34245824813842773, g_loss: 8.652722358703613\n",
      "518 step d_loss1: 0.2505601644515991, d_loss2: 0.3094135522842407, g_loss: 10.386240005493164\n",
      "519 step d_loss1: 0.32634204626083374, d_loss2: 0.38736945390701294, g_loss: 8.754581451416016\n",
      "520 step d_loss1: 0.18509416282176971, d_loss2: 0.30684584379196167, g_loss: 9.39902400970459\n",
      "521 step d_loss1: 0.30082547664642334, d_loss2: 0.2836498022079468, g_loss: 15.100445747375488\n",
      "522 step d_loss1: 0.3863747715950012, d_loss2: 0.2840617895126343, g_loss: 10.092098236083984\n",
      "523 step d_loss1: 0.30087587237358093, d_loss2: 0.2679840922355652, g_loss: 10.124775886535645\n",
      "524 step d_loss1: 0.2975613474845886, d_loss2: 0.3764926791191101, g_loss: 7.219038963317871\n",
      "525 step d_loss1: 0.3334156572818756, d_loss2: 0.3063170313835144, g_loss: 8.474713325500488\n",
      "526 step d_loss1: 0.45315900444984436, d_loss2: 0.34092986583709717, g_loss: 9.462268829345703\n",
      "527 step d_loss1: 0.24574753642082214, d_loss2: 0.4833809733390808, g_loss: 7.983983039855957\n",
      "528 step d_loss1: 0.46763578057289124, d_loss2: 0.31328222155570984, g_loss: 8.198197364807129\n",
      "529 step d_loss1: 0.38942646980285645, d_loss2: 0.32475507259368896, g_loss: 22.934581756591797\n",
      "530 step d_loss1: 0.4319773316383362, d_loss2: 0.364446222782135, g_loss: 16.24976348876953\n",
      "531 step d_loss1: 0.3946073651313782, d_loss2: 0.36778247356414795, g_loss: 9.757365226745605\n",
      "532 step d_loss1: 0.2976939082145691, d_loss2: 0.310641348361969, g_loss: 8.313773155212402\n",
      "533 step d_loss1: 0.39884936809539795, d_loss2: 0.3562789559364319, g_loss: 9.774791717529297\n",
      "534 step d_loss1: 0.38797497749328613, d_loss2: 0.38304877281188965, g_loss: 8.615209579467773\n",
      "535 step d_loss1: 0.3606969118118286, d_loss2: 0.35570698976516724, g_loss: 8.445992469787598\n",
      "536 step d_loss1: 0.26026538014411926, d_loss2: 0.33808618783950806, g_loss: 30.12122344970703\n",
      "537 step d_loss1: 0.18020015954971313, d_loss2: 0.3067501187324524, g_loss: 11.594189643859863\n",
      "538 step d_loss1: 0.4137312173843384, d_loss2: 0.2995796799659729, g_loss: 14.894035339355469\n",
      "539 step d_loss1: 0.30697882175445557, d_loss2: 0.49818629026412964, g_loss: 14.20247745513916\n",
      "540 step d_loss1: 0.22225980460643768, d_loss2: 0.42141789197921753, g_loss: 9.942184448242188\n",
      "541 step d_loss1: 0.33500194549560547, d_loss2: 0.3619184195995331, g_loss: 10.33154010772705\n",
      "542 step d_loss1: 0.37674611806869507, d_loss2: 0.35509294271469116, g_loss: 12.389999389648438\n",
      "543 step d_loss1: 0.32571202516555786, d_loss2: 0.36461126804351807, g_loss: 17.918428421020508\n",
      "544 step d_loss1: 0.37150174379348755, d_loss2: 0.471231609582901, g_loss: 8.284472465515137\n",
      "545 step d_loss1: 0.35450559854507446, d_loss2: 0.42395293712615967, g_loss: 10.957707405090332\n",
      "546 step d_loss1: 0.26546716690063477, d_loss2: 0.3459629714488983, g_loss: 9.83413028717041\n",
      "547 step d_loss1: 0.42623084783554077, d_loss2: 0.40213555097579956, g_loss: 10.705252647399902\n",
      "548 step d_loss1: 0.3072293996810913, d_loss2: 0.35480353236198425, g_loss: 9.462730407714844\n",
      "549 step d_loss1: 0.39874133467674255, d_loss2: 0.3189614415168762, g_loss: 9.993589401245117\n",
      "550 step d_loss1: 0.3171956241130829, d_loss2: 0.43743816018104553, g_loss: 7.772334098815918\n",
      "551 step d_loss1: 0.3185781240463257, d_loss2: 0.3746629059314728, g_loss: 8.317818641662598\n",
      "552 step d_loss1: 0.3366530239582062, d_loss2: 0.32895147800445557, g_loss: 9.6633939743042\n",
      "553 step d_loss1: 0.32882365584373474, d_loss2: 0.38354384899139404, g_loss: 10.493793487548828\n",
      "554 step d_loss1: 0.2258121222257614, d_loss2: 0.36165404319763184, g_loss: 7.815284252166748\n",
      "555 step d_loss1: 0.37741971015930176, d_loss2: 0.4091775417327881, g_loss: 11.03974437713623\n",
      "556 step d_loss1: 0.5705974698066711, d_loss2: 0.26476576924324036, g_loss: 8.145992279052734\n",
      "557 step d_loss1: 0.2813796103000641, d_loss2: 0.35153234004974365, g_loss: 8.569048881530762\n",
      "558 step d_loss1: 0.4316965937614441, d_loss2: 0.45329993963241577, g_loss: 9.616730690002441\n",
      "559 step d_loss1: 0.28940820693969727, d_loss2: 0.2916935086250305, g_loss: 11.501099586486816\n",
      "560 step d_loss1: 0.28127244114875793, d_loss2: 0.32046279311180115, g_loss: 9.045495986938477\n",
      "561 step d_loss1: 0.3437577486038208, d_loss2: 0.4023650586605072, g_loss: 7.635775089263916\n",
      "562 step d_loss1: 0.5190443396568298, d_loss2: 0.2784165143966675, g_loss: 25.196090698242188\n",
      "563 step d_loss1: 0.4132727384567261, d_loss2: 0.3841841220855713, g_loss: 9.161494255065918\n",
      "564 step d_loss1: 0.3788740038871765, d_loss2: 0.27598410844802856, g_loss: 8.404141426086426\n",
      "565 step d_loss1: 0.36341726779937744, d_loss2: 0.3140529990196228, g_loss: 10.561775207519531\n",
      "566 step d_loss1: 0.3889179229736328, d_loss2: 0.3656601905822754, g_loss: 12.208683967590332\n",
      "567 step d_loss1: 0.4752156734466553, d_loss2: 0.3385058343410492, g_loss: 8.561423301696777\n",
      "568 step d_loss1: 0.5472452640533447, d_loss2: 0.2938501536846161, g_loss: 7.755802154541016\n",
      "569 step d_loss1: 0.33066561818122864, d_loss2: 0.4421508312225342, g_loss: 7.119682312011719\n",
      "570 step d_loss1: 0.4479456841945648, d_loss2: 0.3382914662361145, g_loss: 8.270696640014648\n",
      "571 step d_loss1: 0.2822335362434387, d_loss2: 0.4345445930957794, g_loss: 8.514801979064941\n",
      "572 step d_loss1: 0.34390202164649963, d_loss2: 0.3961431384086609, g_loss: 9.582947731018066\n",
      "573 step d_loss1: 0.4549734592437744, d_loss2: 0.3350118100643158, g_loss: 6.0861921310424805\n",
      "574 step d_loss1: 0.31581175327301025, d_loss2: 0.41253530979156494, g_loss: 8.72514820098877\n",
      "575 step d_loss1: 0.3973529040813446, d_loss2: 0.360586941242218, g_loss: 6.37954568862915\n",
      "576 step d_loss1: 0.3319793939590454, d_loss2: 0.35007140040397644, g_loss: 7.717909812927246\n",
      "577 step d_loss1: 0.37654078006744385, d_loss2: 0.3472864329814911, g_loss: 8.506962776184082\n",
      "578 step d_loss1: 0.3311055898666382, d_loss2: 0.40968915820121765, g_loss: 9.270135879516602\n",
      "579 step d_loss1: 0.37627431750297546, d_loss2: 0.3641912043094635, g_loss: 6.22329568862915\n",
      "580 step d_loss1: 0.2540381848812103, d_loss2: 0.29839247465133667, g_loss: 9.288060188293457\n",
      "581 step d_loss1: 0.4740580916404724, d_loss2: 0.27869337797164917, g_loss: 7.00212287902832\n",
      "582 step d_loss1: 0.3806585669517517, d_loss2: 0.3765658438205719, g_loss: 9.359912872314453\n",
      "583 step d_loss1: 0.337007075548172, d_loss2: 0.3809778392314911, g_loss: 9.527528762817383\n",
      "584 step d_loss1: 0.2728941738605499, d_loss2: 0.39567455649375916, g_loss: 9.763335227966309\n",
      "585 step d_loss1: 0.2417685091495514, d_loss2: 0.41511765122413635, g_loss: 8.040619850158691\n",
      "586 step d_loss1: 0.3805752098560333, d_loss2: 0.33869680762290955, g_loss: 9.473907470703125\n",
      "587 step d_loss1: 0.20422357320785522, d_loss2: 0.2732742428779602, g_loss: 8.824091911315918\n",
      "588 step d_loss1: 0.35854119062423706, d_loss2: 0.4248621165752411, g_loss: 8.369645118713379\n",
      "589 step d_loss1: 0.5471662282943726, d_loss2: 0.2562825679779053, g_loss: 9.677247047424316\n",
      "590 step d_loss1: 0.37652814388275146, d_loss2: 0.38794445991516113, g_loss: 22.035587310791016\n",
      "591 step d_loss1: 0.4049004018306732, d_loss2: 0.3686622083187103, g_loss: 6.405322074890137\n",
      "592 step d_loss1: 0.46762263774871826, d_loss2: 0.3803769648075104, g_loss: 9.501365661621094\n",
      "593 step d_loss1: 0.3251460790634155, d_loss2: 0.4286814332008362, g_loss: 6.94413423538208\n",
      "594 step d_loss1: 0.33777305483818054, d_loss2: 0.4086934030056, g_loss: 8.817455291748047\n",
      "595 step d_loss1: 0.4157032370567322, d_loss2: 0.32717615365982056, g_loss: 9.089580535888672\n",
      "596 step d_loss1: 0.2563953101634979, d_loss2: 0.334587961435318, g_loss: 10.541077613830566\n",
      "597 step d_loss1: 0.3222285509109497, d_loss2: 0.3529724180698395, g_loss: 8.873899459838867\n",
      "598 step d_loss1: 0.2505113184452057, d_loss2: 0.31725215911865234, g_loss: 8.589807510375977\n",
      "599 step d_loss1: 0.40591251850128174, d_loss2: 0.35512101650238037, g_loss: 9.007448196411133\n",
      "600 step d_loss1: 0.40766850113868713, d_loss2: 0.3482779860496521, g_loss: 14.74599838256836\n",
      "601 step d_loss1: 0.28569403290748596, d_loss2: 0.5442736148834229, g_loss: 7.738879203796387\n",
      "602 step d_loss1: 0.31702715158462524, d_loss2: 0.35758861899375916, g_loss: 9.204228401184082\n",
      "603 step d_loss1: 0.4611462354660034, d_loss2: 0.2820778489112854, g_loss: 8.226912498474121\n",
      "604 step d_loss1: 0.44437646865844727, d_loss2: 0.31694960594177246, g_loss: 8.547992706298828\n",
      "605 step d_loss1: 0.3670917749404907, d_loss2: 0.31855523586273193, g_loss: 19.239656448364258\n",
      "606 step d_loss1: 0.31486570835113525, d_loss2: 0.31327709555625916, g_loss: 9.671443939208984\n",
      "607 step d_loss1: 0.32394886016845703, d_loss2: 0.37540367245674133, g_loss: 8.018594741821289\n",
      "608 step d_loss1: 0.3082447052001953, d_loss2: 0.4093490540981293, g_loss: 7.29321813583374\n",
      "609 step d_loss1: 0.3629668056964874, d_loss2: 0.3430701792240143, g_loss: 5.959836006164551\n",
      "610 step d_loss1: 0.35205772519111633, d_loss2: 0.2978682518005371, g_loss: 10.00372314453125\n",
      "611 step d_loss1: 0.3210147023200989, d_loss2: 0.3881767988204956, g_loss: 9.056619644165039\n",
      "612 step d_loss1: 0.26503482460975647, d_loss2: 0.35502299666404724, g_loss: 9.634220123291016\n",
      "613 step d_loss1: 0.3566151559352875, d_loss2: 0.30555182695388794, g_loss: 7.553558826446533\n",
      "614 step d_loss1: 0.2340264618396759, d_loss2: 0.37568655610084534, g_loss: 10.343677520751953\n",
      "615 step d_loss1: 0.31495729088783264, d_loss2: 0.36085066199302673, g_loss: 10.365493774414062\n",
      "616 step d_loss1: 0.33782756328582764, d_loss2: 0.33354225754737854, g_loss: 7.799539089202881\n",
      "617 step d_loss1: 0.39664965867996216, d_loss2: 0.41966181993484497, g_loss: 8.086997032165527\n",
      "618 step d_loss1: 0.34593915939331055, d_loss2: 0.40460512042045593, g_loss: 8.025152206420898\n",
      "619 step d_loss1: 0.3910811245441437, d_loss2: 0.4668915867805481, g_loss: 7.846750259399414\n",
      "620 step d_loss1: 0.3682023584842682, d_loss2: 0.349976122379303, g_loss: 16.060779571533203\n",
      "621 step d_loss1: 0.41184747219085693, d_loss2: 0.46529772877693176, g_loss: 7.6576714515686035\n",
      "622 step d_loss1: 0.2692131996154785, d_loss2: 0.30484166741371155, g_loss: 14.741729736328125\n",
      "623 step d_loss1: 0.28347092866897583, d_loss2: 0.3372710943222046, g_loss: 21.535762786865234\n",
      "624 step d_loss1: 0.45077186822891235, d_loss2: 0.36471855640411377, g_loss: 8.847893714904785\n",
      "625 step d_loss1: 0.4340507686138153, d_loss2: 0.39975911378860474, g_loss: 10.74033260345459\n",
      "626 step d_loss1: 0.37239813804626465, d_loss2: 0.37597331404685974, g_loss: 8.5468111038208\n",
      "627 step d_loss1: 0.37331801652908325, d_loss2: 0.35029321908950806, g_loss: 8.91196060180664\n",
      "628 step d_loss1: 0.4293454885482788, d_loss2: 0.36148571968078613, g_loss: 6.818542957305908\n",
      "629 step d_loss1: 0.4407169222831726, d_loss2: 0.4169929027557373, g_loss: 7.879692554473877\n",
      "630 step d_loss1: 0.2645114064216614, d_loss2: 0.3759559094905853, g_loss: 7.345938682556152\n",
      "631 step d_loss1: 0.3840502202510834, d_loss2: 0.33585482835769653, g_loss: 8.566889762878418\n",
      "632 step d_loss1: 0.3544556200504303, d_loss2: 0.3786141574382782, g_loss: 9.625678062438965\n",
      "633 step d_loss1: 0.3593440353870392, d_loss2: 0.31884217262268066, g_loss: 8.90972900390625\n",
      "634 step d_loss1: 0.3928380608558655, d_loss2: 0.332073450088501, g_loss: 8.113042831420898\n",
      "635 step d_loss1: 0.32482650876045227, d_loss2: 0.336824893951416, g_loss: 9.310626983642578\n",
      "636 step d_loss1: 0.3835115134716034, d_loss2: 0.3367219865322113, g_loss: 8.086904525756836\n",
      "637 step d_loss1: 0.28027844429016113, d_loss2: 0.36950963735580444, g_loss: 7.898225784301758\n",
      "638 step d_loss1: 0.3229427933692932, d_loss2: 0.3417256772518158, g_loss: 7.286759376525879\n",
      "639 step d_loss1: 0.23019206523895264, d_loss2: 0.28309667110443115, g_loss: 9.523335456848145\n",
      "640 step d_loss1: 0.5330461263656616, d_loss2: 0.3003574013710022, g_loss: 7.6480631828308105\n",
      "641 step d_loss1: 0.21993900835514069, d_loss2: 0.29377493262290955, g_loss: 8.901457786560059\n",
      "642 step d_loss1: 0.3519086539745331, d_loss2: 0.313801646232605, g_loss: 19.878704071044922\n",
      "643 step d_loss1: 0.20010152459144592, d_loss2: 0.22093184292316437, g_loss: 9.709527015686035\n",
      "644 step d_loss1: 0.4523620307445526, d_loss2: 0.24676959216594696, g_loss: 8.210319519042969\n",
      "645 step d_loss1: 0.2571375370025635, d_loss2: 0.3707713484764099, g_loss: 8.402624130249023\n",
      "646 step d_loss1: 0.4488394558429718, d_loss2: 0.3422856032848358, g_loss: 8.933374404907227\n",
      "647 step d_loss1: 0.27749142050743103, d_loss2: 0.3659282922744751, g_loss: 14.288899421691895\n",
      "648 step d_loss1: 0.3466205894947052, d_loss2: 0.3390299379825592, g_loss: 8.86456298828125\n",
      "649 step d_loss1: 0.4545924663543701, d_loss2: 0.35092926025390625, g_loss: 7.832492828369141\n",
      "650 step d_loss1: 0.3441017270088196, d_loss2: 0.4646499752998352, g_loss: 7.827671051025391\n",
      "651 step d_loss1: 0.23044352233409882, d_loss2: 0.29919835925102234, g_loss: 9.180682182312012\n",
      "652 step d_loss1: 0.44667211174964905, d_loss2: 0.28168490529060364, g_loss: 7.826732635498047\n",
      "653 step d_loss1: 0.5168508291244507, d_loss2: 0.32060426473617554, g_loss: 6.8734130859375\n",
      "654 step d_loss1: 0.21155545115470886, d_loss2: 0.333993524312973, g_loss: 10.04800033569336\n",
      "655 step d_loss1: 0.3513742685317993, d_loss2: 0.35029444098472595, g_loss: 7.480588912963867\n",
      "656 step d_loss1: 0.21024209260940552, d_loss2: 0.315064013004303, g_loss: 9.136228561401367\n",
      "657 step d_loss1: 0.4868853986263275, d_loss2: 0.3540504574775696, g_loss: 7.420106410980225\n",
      "658 step d_loss1: 0.42618268728256226, d_loss2: 0.3914804458618164, g_loss: 14.334909439086914\n",
      "659 step d_loss1: 0.423045814037323, d_loss2: 0.4111120104789734, g_loss: 9.392181396484375\n",
      "660 step d_loss1: 0.1590002179145813, d_loss2: 0.38529738783836365, g_loss: 10.660188674926758\n",
      "661 step d_loss1: 0.39321833848953247, d_loss2: 0.359019935131073, g_loss: 7.137749195098877\n",
      "662 step d_loss1: 0.17951998114585876, d_loss2: 0.29596173763275146, g_loss: 7.726702690124512\n",
      "663 step d_loss1: 0.35842791199684143, d_loss2: 0.29295119643211365, g_loss: 9.531198501586914\n",
      "664 step d_loss1: 0.46564677357673645, d_loss2: 0.3376900553703308, g_loss: 7.318981170654297\n",
      "665 step d_loss1: 0.19384534657001495, d_loss2: 0.31687507033348083, g_loss: 7.491771697998047\n",
      "666 step d_loss1: 0.4927622079849243, d_loss2: 0.30677828192710876, g_loss: 9.322868347167969\n",
      "667 step d_loss1: 0.48814305663108826, d_loss2: 0.4009605348110199, g_loss: 7.253699779510498\n",
      "668 step d_loss1: 0.33312416076660156, d_loss2: 0.39104747772216797, g_loss: 7.3347625732421875\n",
      "669 step d_loss1: 0.44834622740745544, d_loss2: 0.3554323613643646, g_loss: 9.364304542541504\n",
      "670 step d_loss1: 0.5421460866928101, d_loss2: 0.32586869597435, g_loss: 7.934167861938477\n",
      "671 step d_loss1: 0.3037024438381195, d_loss2: 0.4075257480144501, g_loss: 19.27034568786621\n",
      "672 step d_loss1: 0.23601599037647247, d_loss2: 0.3249044120311737, g_loss: 9.220328330993652\n",
      "673 step d_loss1: 0.4636468291282654, d_loss2: 0.3416471779346466, g_loss: 8.441773414611816\n",
      "674 step d_loss1: 0.2628036141395569, d_loss2: 0.3457338213920593, g_loss: 24.519411087036133\n",
      "675 step d_loss1: 0.4275948405265808, d_loss2: 0.3388274610042572, g_loss: 7.88457727432251\n",
      "676 step d_loss1: 0.4387984275817871, d_loss2: 0.32760828733444214, g_loss: 7.875993728637695\n",
      "677 step d_loss1: 0.4072169363498688, d_loss2: 0.3430362641811371, g_loss: 9.091327667236328\n",
      "678 step d_loss1: 0.40345996618270874, d_loss2: 0.37657931447029114, g_loss: 6.553499221801758\n",
      "679 step d_loss1: 0.3864760100841522, d_loss2: 0.41703343391418457, g_loss: 8.240436553955078\n",
      "680 step d_loss1: 0.2812138497829437, d_loss2: 0.3945370316505432, g_loss: 9.385445594787598\n",
      "681 step d_loss1: 0.17416207492351532, d_loss2: 0.3215000033378601, g_loss: 9.683812141418457\n",
      "682 step d_loss1: 0.37009572982788086, d_loss2: 0.34388554096221924, g_loss: 8.083035469055176\n",
      "683 step d_loss1: 0.30776160955429077, d_loss2: 0.3109859228134155, g_loss: 7.326224327087402\n",
      "684 step d_loss1: 0.15591424703598022, d_loss2: 0.29670652747154236, g_loss: 10.116443634033203\n",
      "685 step d_loss1: 0.1931232362985611, d_loss2: 0.3823949694633484, g_loss: 9.019563674926758\n",
      "686 step d_loss1: 0.4243074357509613, d_loss2: 0.4020821452140808, g_loss: 7.601554870605469\n",
      "687 step d_loss1: 0.3618336617946625, d_loss2: 0.37711942195892334, g_loss: 6.979332447052002\n",
      "688 step d_loss1: 0.4131007194519043, d_loss2: 0.27054524421691895, g_loss: 8.883329391479492\n",
      "689 step d_loss1: 0.5269451141357422, d_loss2: 0.29068800806999207, g_loss: 6.753299713134766\n",
      "690 step d_loss1: 0.36318519711494446, d_loss2: 0.3629850149154663, g_loss: 8.879981994628906\n",
      "691 step d_loss1: 0.29595106840133667, d_loss2: 0.351643830537796, g_loss: 9.847152709960938\n",
      "692 step d_loss1: 0.3974320590496063, d_loss2: 0.34326040744781494, g_loss: 8.889178276062012\n",
      "693 step d_loss1: 0.2978980541229248, d_loss2: 0.3380662500858307, g_loss: 9.10191535949707\n",
      "694 step d_loss1: 0.32421422004699707, d_loss2: 0.3552805781364441, g_loss: 8.792019844055176\n",
      "695 step d_loss1: 0.15831585228443146, d_loss2: 0.405653715133667, g_loss: 10.713953018188477\n",
      "696 step d_loss1: 0.44343042373657227, d_loss2: 0.3704802691936493, g_loss: 8.746173858642578\n",
      "697 step d_loss1: 0.17616692185401917, d_loss2: 0.40240195393562317, g_loss: 8.617513656616211\n",
      "698 step d_loss1: 0.29696202278137207, d_loss2: 0.4001927375793457, g_loss: 7.749866485595703\n",
      "699 step d_loss1: 0.4232814311981201, d_loss2: 0.3872835636138916, g_loss: 13.943751335144043\n",
      "700 step d_loss1: 0.599210798740387, d_loss2: 0.3159080743789673, g_loss: 8.72390079498291\n",
      "701 step d_loss1: 0.5105183124542236, d_loss2: 0.3059930205345154, g_loss: 7.23183536529541\n",
      "702 step d_loss1: 0.43087533116340637, d_loss2: 0.3271764814853668, g_loss: 7.817497253417969\n",
      "703 step d_loss1: 0.39435747265815735, d_loss2: 0.3795078992843628, g_loss: 8.637221336364746\n",
      "704 step d_loss1: 0.41255515813827515, d_loss2: 0.35260316729545593, g_loss: 7.796605587005615\n",
      "705 step d_loss1: 0.40834686160087585, d_loss2: 0.3578988015651703, g_loss: 6.51475191116333\n",
      "706 step d_loss1: 0.3813161253929138, d_loss2: 0.37049034237861633, g_loss: 8.215774536132812\n",
      "707 step d_loss1: 0.3748830556869507, d_loss2: 0.3616100549697876, g_loss: 7.644421577453613\n",
      "708 step d_loss1: 0.3351920247077942, d_loss2: 0.3760853409767151, g_loss: 18.057058334350586\n",
      "709 step d_loss1: 0.35512346029281616, d_loss2: 0.35814905166625977, g_loss: 7.078919410705566\n",
      "710 step d_loss1: 0.3286735713481903, d_loss2: 0.35569703578948975, g_loss: 8.498918533325195\n",
      "711 step d_loss1: 0.347541868686676, d_loss2: 0.4045851528644562, g_loss: 11.143473625183105\n",
      "712 step d_loss1: 0.375267893075943, d_loss2: 0.34992313385009766, g_loss: 14.742772102355957\n",
      "713 step d_loss1: 0.3834095001220703, d_loss2: 0.34844666719436646, g_loss: 7.941319465637207\n",
      "714 step d_loss1: 0.3230310380458832, d_loss2: 0.3709602355957031, g_loss: 7.387065410614014\n",
      "715 step d_loss1: 0.34308600425720215, d_loss2: 0.39040282368659973, g_loss: 8.699431419372559\n",
      "716 step d_loss1: 0.37678825855255127, d_loss2: 0.39025115966796875, g_loss: 10.067703247070312\n",
      "717 step d_loss1: 0.3876529633998871, d_loss2: 0.33625760674476624, g_loss: 6.747163772583008\n",
      "718 step d_loss1: 0.38512730598449707, d_loss2: 0.3473683297634125, g_loss: 6.508624076843262\n",
      "719 step d_loss1: 0.32589906454086304, d_loss2: 0.38514259457588196, g_loss: 6.7555413246154785\n",
      "720 step d_loss1: 0.37743061780929565, d_loss2: 0.3626081049442291, g_loss: 7.145203590393066\n",
      "721 step d_loss1: 0.35882487893104553, d_loss2: 0.34559524059295654, g_loss: 7.807801246643066\n",
      "722 step d_loss1: 0.2972661554813385, d_loss2: 0.4019213914871216, g_loss: 10.010001182556152\n",
      "723 step d_loss1: 0.4081515073776245, d_loss2: 0.33503422141075134, g_loss: 6.8185200691223145\n",
      "724 step d_loss1: 0.3722359240055084, d_loss2: 0.36731016635894775, g_loss: 7.41779088973999\n",
      "725 step d_loss1: 0.41604751348495483, d_loss2: 0.3491339385509491, g_loss: 12.01773738861084\n",
      "726 step d_loss1: 0.3297605514526367, d_loss2: 0.459197461605072, g_loss: 6.803237438201904\n",
      "727 step d_loss1: 0.35952767729759216, d_loss2: 0.36203432083129883, g_loss: 11.715899467468262\n",
      "728 step d_loss1: 0.3376905024051666, d_loss2: 0.3373434245586395, g_loss: 8.933594703674316\n",
      "729 step d_loss1: 0.3627041280269623, d_loss2: 0.36361560225486755, g_loss: 9.232982635498047\n",
      "730 step d_loss1: 0.34807199239730835, d_loss2: 0.41548243165016174, g_loss: 20.827899932861328\n",
      "731 step d_loss1: 0.3751363754272461, d_loss2: 0.3198322355747223, g_loss: 8.418251991271973\n",
      "732 step d_loss1: 0.3482130467891693, d_loss2: 0.32534703612327576, g_loss: 7.945287704467773\n",
      "733 step d_loss1: 0.33065417408943176, d_loss2: 0.37662503123283386, g_loss: 8.571335792541504\n",
      "734 step d_loss1: 0.35563647747039795, d_loss2: 0.35641732811927795, g_loss: 8.27063274383545\n",
      "735 step d_loss1: 0.3842851221561432, d_loss2: 0.3457811772823334, g_loss: 6.945587158203125\n",
      "736 step d_loss1: 0.3079338073730469, d_loss2: 0.3777464032173157, g_loss: 7.2357001304626465\n",
      "737 step d_loss1: 0.3880927562713623, d_loss2: 0.3732282519340515, g_loss: 7.468939304351807\n",
      "738 step d_loss1: 0.35715922713279724, d_loss2: 0.35438159108161926, g_loss: 7.48523473739624\n",
      "739 step d_loss1: 0.3826347291469574, d_loss2: 0.35564398765563965, g_loss: 7.95113468170166\n",
      "740 step d_loss1: 0.3256930708885193, d_loss2: 0.35197633504867554, g_loss: 8.556425094604492\n",
      "741 step d_loss1: 0.3835504949092865, d_loss2: 0.35599246621131897, g_loss: 7.836828231811523\n",
      "742 step d_loss1: 0.32135528326034546, d_loss2: 0.33713194727897644, g_loss: 8.125900268554688\n",
      "743 step d_loss1: 0.3255641460418701, d_loss2: 0.3633686602115631, g_loss: 9.631032943725586\n",
      "744 step d_loss1: 0.403907835483551, d_loss2: 0.3345297574996948, g_loss: 6.899787902832031\n",
      "745 step d_loss1: 0.31254899501800537, d_loss2: 0.36399781703948975, g_loss: 7.676648139953613\n",
      "746 step d_loss1: 0.2700694501399994, d_loss2: 0.3829980194568634, g_loss: 6.838038921356201\n",
      "747 step d_loss1: 0.3236030638217926, d_loss2: 0.4453953206539154, g_loss: 20.247697830200195\n",
      "748 step d_loss1: 0.29709887504577637, d_loss2: 0.380656898021698, g_loss: 9.206913948059082\n",
      "749 step d_loss1: 0.3622095286846161, d_loss2: 0.3150043487548828, g_loss: 8.74055290222168\n",
      "750 step d_loss1: 0.39843854308128357, d_loss2: 0.3434041142463684, g_loss: 7.293357849121094\n",
      "751 step d_loss1: 0.37136852741241455, d_loss2: 0.4218440055847168, g_loss: 8.412059783935547\n",
      "752 step d_loss1: 0.27795150876045227, d_loss2: 0.3787052631378174, g_loss: 14.845233917236328\n",
      "753 step d_loss1: 0.30672362446784973, d_loss2: 0.328050434589386, g_loss: 18.862703323364258\n",
      "754 step d_loss1: 0.4283851981163025, d_loss2: 0.33552950620651245, g_loss: 6.146473407745361\n",
      "755 step d_loss1: 0.2993130683898926, d_loss2: 0.3481689691543579, g_loss: 15.244720458984375\n",
      "756 step d_loss1: 0.3475615084171295, d_loss2: 0.3457050919532776, g_loss: 7.634709358215332\n",
      "757 step d_loss1: 0.3225606679916382, d_loss2: 0.30922406911849976, g_loss: 8.499099731445312\n",
      "758 step d_loss1: 0.38602954149246216, d_loss2: 0.4395488500595093, g_loss: 7.566760063171387\n",
      "759 step d_loss1: 0.3889564573764801, d_loss2: 0.37786683440208435, g_loss: 19.427804946899414\n",
      "760 step d_loss1: 0.29855334758758545, d_loss2: 0.4900332987308502, g_loss: 9.646034240722656\n",
      "761 step d_loss1: 0.4837620258331299, d_loss2: 0.3260060250759125, g_loss: 14.055325508117676\n",
      "762 step d_loss1: 0.44650325179100037, d_loss2: 0.33094850182533264, g_loss: 6.909994602203369\n",
      "763 step d_loss1: 0.3575599491596222, d_loss2: 0.4056599736213684, g_loss: 6.348143100738525\n",
      "764 step d_loss1: 0.4042905569076538, d_loss2: 0.3535808026790619, g_loss: 6.856177806854248\n",
      "765 step d_loss1: 0.37778741121292114, d_loss2: 0.36312586069107056, g_loss: 15.841180801391602\n",
      "766 step d_loss1: 0.371677964925766, d_loss2: 0.35126927495002747, g_loss: 9.623123168945312\n",
      "767 step d_loss1: 0.3183874189853668, d_loss2: 0.41386014223098755, g_loss: 12.007144927978516\n",
      "768 step d_loss1: 0.3480244278907776, d_loss2: 0.3514406383037567, g_loss: 7.6678056716918945\n",
      "769 step d_loss1: 0.3537937104701996, d_loss2: 0.37705186009407043, g_loss: 8.54208755493164\n",
      "770 step d_loss1: 0.3419871926307678, d_loss2: 0.3415320813655853, g_loss: 7.363006114959717\n",
      "771 step d_loss1: 0.40162336826324463, d_loss2: 0.3314252197742462, g_loss: 6.689131259918213\n",
      "772 step d_loss1: 0.3480820655822754, d_loss2: 0.3580925464630127, g_loss: 8.29018497467041\n",
      "773 step d_loss1: 0.34480711817741394, d_loss2: 0.3320268988609314, g_loss: 8.37316608428955\n",
      "774 step d_loss1: 0.32759374380111694, d_loss2: 0.4180777668952942, g_loss: 7.928777694702148\n",
      "775 step d_loss1: 0.3079257905483246, d_loss2: 0.4021565616130829, g_loss: 9.900517463684082\n",
      "776 step d_loss1: 0.28536340594291687, d_loss2: 0.38848188519477844, g_loss: 26.188383102416992\n",
      "777 step d_loss1: 0.426264226436615, d_loss2: 0.3403749167919159, g_loss: 6.520772933959961\n",
      "778 step d_loss1: 0.39427649974823, d_loss2: 0.38226211071014404, g_loss: 7.781590938568115\n",
      "779 step d_loss1: 0.3516905605792999, d_loss2: 0.38689711689949036, g_loss: 9.476292610168457\n",
      "780 step d_loss1: 0.33043497800827026, d_loss2: 0.3181009292602539, g_loss: 13.86506462097168\n",
      "781 step d_loss1: 0.4420221149921417, d_loss2: 0.38677993416786194, g_loss: 9.627595901489258\n",
      "782 step d_loss1: 0.38434097170829773, d_loss2: 0.31174904108047485, g_loss: 7.939103603363037\n",
      "783 step d_loss1: 0.33568617701530457, d_loss2: 0.36943501234054565, g_loss: 7.565627574920654\n",
      "784 step d_loss1: 0.3563682734966278, d_loss2: 0.3624696135520935, g_loss: 8.43051815032959\n",
      "785 step d_loss1: 0.38619858026504517, d_loss2: 0.3521611988544464, g_loss: 6.857787609100342\n",
      "786 step d_loss1: 0.3494233787059784, d_loss2: 0.3643065392971039, g_loss: 6.783699035644531\n",
      "787 step d_loss1: 0.3260732591152191, d_loss2: 0.34522950649261475, g_loss: 7.405794620513916\n",
      "788 step d_loss1: 0.3674544394016266, d_loss2: 0.29915812611579895, g_loss: 6.944141387939453\n",
      "789 step d_loss1: 0.3362448513507843, d_loss2: 0.36055222153663635, g_loss: 7.855852127075195\n",
      "790 step d_loss1: 0.35995617508888245, d_loss2: 0.32322216033935547, g_loss: 6.701403617858887\n",
      "791 step d_loss1: 0.3041762113571167, d_loss2: 0.35458552837371826, g_loss: 8.413726806640625\n",
      "792 step d_loss1: 0.2866174578666687, d_loss2: 0.42160117626190186, g_loss: 8.909163475036621\n",
      "793 step d_loss1: 0.3326822519302368, d_loss2: 0.3313975930213928, g_loss: 7.236141204833984\n",
      "794 step d_loss1: 0.3523488938808441, d_loss2: 0.36847567558288574, g_loss: 6.629801273345947\n",
      "795 step d_loss1: 0.3293466567993164, d_loss2: 0.33567318320274353, g_loss: 8.053287506103516\n",
      "796 step d_loss1: 0.3238026797771454, d_loss2: 0.33922114968299866, g_loss: 9.530664443969727\n",
      "797 step d_loss1: 0.3491817116737366, d_loss2: 0.36501726508140564, g_loss: 8.989632606506348\n",
      "798 step d_loss1: 0.3624519407749176, d_loss2: 0.35457643866539, g_loss: 10.724552154541016\n",
      "799 step d_loss1: 0.3279262185096741, d_loss2: 0.3739657998085022, g_loss: 6.60216760635376\n",
      "800 step d_loss1: 0.2963714301586151, d_loss2: 0.36927592754364014, g_loss: 10.990846633911133\n",
      "801 step d_loss1: 0.38470152020454407, d_loss2: 0.3388698101043701, g_loss: 8.955864906311035\n",
      "802 step d_loss1: 0.306564599275589, d_loss2: 0.3246392011642456, g_loss: 8.645788192749023\n",
      "803 step d_loss1: 0.42822831869125366, d_loss2: 0.3469671607017517, g_loss: 8.798565864562988\n",
      "804 step d_loss1: 0.33506330847740173, d_loss2: 0.36561447381973267, g_loss: 8.716730117797852\n",
      "805 step d_loss1: 0.40341582894325256, d_loss2: 0.4102572202682495, g_loss: 7.057356834411621\n",
      "806 step d_loss1: 0.32714203000068665, d_loss2: 0.352403849363327, g_loss: 9.714405059814453\n",
      "807 step d_loss1: 0.32238101959228516, d_loss2: 0.3494095504283905, g_loss: 8.82438850402832\n",
      "808 step d_loss1: 0.39021188020706177, d_loss2: 0.35788649320602417, g_loss: 7.641904354095459\n",
      "809 step d_loss1: 0.38565507531166077, d_loss2: 0.3571404218673706, g_loss: 12.021882057189941\n",
      "810 step d_loss1: 0.35247719287872314, d_loss2: 0.43297332525253296, g_loss: 5.875832557678223\n",
      "811 step d_loss1: 0.35030534863471985, d_loss2: 0.3940252363681793, g_loss: 9.356464385986328\n",
      "812 step d_loss1: 0.31353771686553955, d_loss2: 0.31022828817367554, g_loss: 9.92122745513916\n",
      "813 step d_loss1: 0.37625235319137573, d_loss2: 0.3307443857192993, g_loss: 8.40542984008789\n",
      "814 step d_loss1: 0.30080169439315796, d_loss2: 0.3442731499671936, g_loss: 7.367129802703857\n",
      "815 step d_loss1: 0.3637844920158386, d_loss2: 0.32994818687438965, g_loss: 6.841426849365234\n",
      "816 step d_loss1: 0.3551504909992218, d_loss2: 0.3556431233882904, g_loss: 6.294057369232178\n",
      "817 step d_loss1: 0.30106377601623535, d_loss2: 0.32387420535087585, g_loss: 6.974170684814453\n",
      "818 step d_loss1: 0.29502177238464355, d_loss2: 0.32780811190605164, g_loss: 6.827134132385254\n",
      "819 step d_loss1: 0.32039177417755127, d_loss2: 0.35988229513168335, g_loss: 8.345942497253418\n",
      "820 step d_loss1: 0.25898927450180054, d_loss2: 0.32368582487106323, g_loss: 15.429746627807617\n",
      "821 step d_loss1: 0.3732978403568268, d_loss2: 0.31104350090026855, g_loss: 6.23210334777832\n",
      "822 step d_loss1: 0.36722561717033386, d_loss2: 0.33782055974006653, g_loss: 6.87713098526001\n",
      "823 step d_loss1: 0.2948538362979889, d_loss2: 0.30396097898483276, g_loss: 8.83572769165039\n",
      "824 step d_loss1: 0.2795681953430176, d_loss2: 0.4181484878063202, g_loss: 7.8153767585754395\n",
      "825 step d_loss1: 0.36644959449768066, d_loss2: 0.29211243987083435, g_loss: 9.46024227142334\n",
      "826 step d_loss1: 0.32528844475746155, d_loss2: 0.2988373339176178, g_loss: 8.250361442565918\n",
      "827 step d_loss1: 0.36217889189720154, d_loss2: 0.4606749713420868, g_loss: 5.468343257904053\n",
      "828 step d_loss1: 0.22161085903644562, d_loss2: 0.43864506483078003, g_loss: 17.47317886352539\n",
      "829 step d_loss1: 0.3853188753128052, d_loss2: 0.324930876493454, g_loss: 7.491138458251953\n",
      "830 step d_loss1: 0.4233277142047882, d_loss2: 0.3017384111881256, g_loss: 8.256646156311035\n",
      "831 step d_loss1: 0.41292011737823486, d_loss2: 0.3339005708694458, g_loss: 9.089615821838379\n",
      "832 step d_loss1: 0.237019345164299, d_loss2: 0.4148223400115967, g_loss: 17.429685592651367\n",
      "833 step d_loss1: 0.4492473006248474, d_loss2: 0.33184781670570374, g_loss: 7.41620397567749\n",
      "834 step d_loss1: 0.39782777428627014, d_loss2: 0.4356248080730438, g_loss: 5.428859710693359\n",
      "835 step d_loss1: 0.3498944044113159, d_loss2: 0.41589972376823425, g_loss: 6.579227924346924\n",
      "836 step d_loss1: 0.25907865166664124, d_loss2: 0.37450477480888367, g_loss: 14.13387393951416\n",
      "837 step d_loss1: 0.37036237120628357, d_loss2: 0.29438236355781555, g_loss: 7.693601608276367\n",
      "838 step d_loss1: 0.4169009029865265, d_loss2: 0.3085267245769501, g_loss: 6.553551197052002\n",
      "839 step d_loss1: 0.30760374665260315, d_loss2: 0.45974376797676086, g_loss: 26.993310928344727\n",
      "840 step d_loss1: 0.3945558965206146, d_loss2: 0.36362195014953613, g_loss: 8.88219165802002\n",
      "841 step d_loss1: 0.4153432250022888, d_loss2: 0.35845640301704407, g_loss: 6.5753302574157715\n",
      "842 step d_loss1: 0.3269670009613037, d_loss2: 0.4020540416240692, g_loss: 18.11325454711914\n",
      "843 step d_loss1: 0.4034688174724579, d_loss2: 0.3331242799758911, g_loss: 9.101410865783691\n",
      "844 step d_loss1: 0.3505692183971405, d_loss2: 0.32293081283569336, g_loss: 7.075507640838623\n",
      "845 step d_loss1: 0.364105224609375, d_loss2: 0.34390583634376526, g_loss: 6.342040061950684\n",
      "846 step d_loss1: 0.3623322546482086, d_loss2: 0.35558730363845825, g_loss: 7.324016094207764\n",
      "847 step d_loss1: 0.3328873813152313, d_loss2: 0.3268764317035675, g_loss: 6.245278358459473\n",
      "848 step d_loss1: 0.3032025396823883, d_loss2: 0.3347824215888977, g_loss: 7.580728530883789\n",
      "849 step d_loss1: 0.30858510732650757, d_loss2: 0.3705742359161377, g_loss: 8.249983787536621\n",
      "850 step d_loss1: 0.39309486746788025, d_loss2: 0.3205464780330658, g_loss: 6.256470680236816\n",
      "851 step d_loss1: 0.2503186762332916, d_loss2: 0.35935118794441223, g_loss: 9.382681846618652\n",
      "852 step d_loss1: 0.3024097979068756, d_loss2: 0.3502739369869232, g_loss: 8.03994083404541\n",
      "853 step d_loss1: 0.3804129958152771, d_loss2: 0.3725840151309967, g_loss: 6.3493523597717285\n",
      "854 step d_loss1: 0.3586195111274719, d_loss2: 0.3275371789932251, g_loss: 9.17065143585205\n",
      "855 step d_loss1: 0.3343150317668915, d_loss2: 0.3503596782684326, g_loss: 9.806114196777344\n",
      "856 step d_loss1: 0.35404467582702637, d_loss2: 0.28011488914489746, g_loss: 7.613325119018555\n",
      "857 step d_loss1: 0.2999257743358612, d_loss2: 0.29583990573883057, g_loss: 8.318077087402344\n",
      "858 step d_loss1: 0.3160794973373413, d_loss2: 0.34917962551116943, g_loss: 8.14321231842041\n",
      "859 step d_loss1: 0.29904887080192566, d_loss2: 0.3314278721809387, g_loss: 7.775666236877441\n",
      "860 step d_loss1: 0.25320225954055786, d_loss2: 0.3124642074108124, g_loss: 7.333073139190674\n",
      "861 step d_loss1: 0.2888222932815552, d_loss2: 0.29636844992637634, g_loss: 9.010933876037598\n",
      "862 step d_loss1: 0.2801904082298279, d_loss2: 0.21179333329200745, g_loss: 8.328113555908203\n",
      "863 step d_loss1: 0.3553026020526886, d_loss2: 0.4088571071624756, g_loss: 7.743330955505371\n",
      "864 step d_loss1: 0.42809805274009705, d_loss2: 0.25845369696617126, g_loss: 6.982851982116699\n",
      "865 step d_loss1: 0.40180331468582153, d_loss2: 0.3360498547554016, g_loss: 9.314488410949707\n",
      "866 step d_loss1: 0.32356375455856323, d_loss2: 0.3797004222869873, g_loss: 7.485968589782715\n",
      "867 step d_loss1: 0.20789366960525513, d_loss2: 0.34719792008399963, g_loss: 24.015119552612305\n",
      "868 step d_loss1: 0.2649511694908142, d_loss2: 0.3077472150325775, g_loss: 9.623922348022461\n",
      "869 step d_loss1: 0.31949689984321594, d_loss2: 0.41777241230010986, g_loss: 9.603408813476562\n",
      "870 step d_loss1: 0.3666152060031891, d_loss2: 0.37356695532798767, g_loss: 7.6367998123168945\n",
      "871 step d_loss1: 0.39154672622680664, d_loss2: 0.33956626057624817, g_loss: 9.464315414428711\n",
      "872 step d_loss1: 0.3160785734653473, d_loss2: 0.3886018693447113, g_loss: 14.29407787322998\n",
      "873 step d_loss1: 0.3250192403793335, d_loss2: 0.3184700310230255, g_loss: 8.59764289855957\n",
      "874 step d_loss1: 0.31843796372413635, d_loss2: 0.33087357878685, g_loss: 9.64738655090332\n",
      "875 step d_loss1: 0.37576913833618164, d_loss2: 0.2619663178920746, g_loss: 7.954327583312988\n",
      "876 step d_loss1: 0.2651931047439575, d_loss2: 0.3030853867530823, g_loss: 9.487497329711914\n",
      "877 step d_loss1: 0.3458406925201416, d_loss2: 0.3391132354736328, g_loss: 12.907387733459473\n",
      "878 step d_loss1: 0.4331543445587158, d_loss2: 0.38513046503067017, g_loss: 7.723602771759033\n",
      "879 step d_loss1: 0.3129172921180725, d_loss2: 0.3915426433086395, g_loss: 8.328225135803223\n",
      "880 step d_loss1: 0.4060647189617157, d_loss2: 0.44271570444107056, g_loss: 5.957599639892578\n",
      "881 step d_loss1: 0.36050236225128174, d_loss2: 0.39582183957099915, g_loss: 7.146297454833984\n",
      "882 step d_loss1: 0.30667930841445923, d_loss2: 0.32555311918258667, g_loss: 7.437549591064453\n",
      "883 step d_loss1: 0.29738983511924744, d_loss2: 0.3289237916469574, g_loss: 9.085810661315918\n",
      "884 step d_loss1: 0.3449922800064087, d_loss2: 0.3025590181350708, g_loss: 9.29935073852539\n",
      "885 step d_loss1: 0.3939308524131775, d_loss2: 0.353751003742218, g_loss: 8.366951942443848\n",
      "886 step d_loss1: 0.3771153688430786, d_loss2: 0.39613625407218933, g_loss: 6.832542896270752\n",
      "887 step d_loss1: 0.48910775780677795, d_loss2: 0.3761448562145233, g_loss: 7.0280632972717285\n",
      "888 step d_loss1: 0.28385111689567566, d_loss2: 0.400865763425827, g_loss: 6.932106018066406\n",
      "889 step d_loss1: 0.35240915417671204, d_loss2: 0.39567315578460693, g_loss: 11.43795394897461\n",
      "890 step d_loss1: 0.32531481981277466, d_loss2: 0.31168773770332336, g_loss: 8.58627700805664\n",
      "891 step d_loss1: 0.32309895753860474, d_loss2: 0.2951357364654541, g_loss: 10.354231834411621\n",
      "892 step d_loss1: 0.4011324346065521, d_loss2: 0.3892090618610382, g_loss: 8.096036911010742\n",
      "893 step d_loss1: 0.2889920771121979, d_loss2: 0.3262908458709717, g_loss: 7.359030723571777\n",
      "894 step d_loss1: 0.27121680974960327, d_loss2: 0.31645551323890686, g_loss: 8.794428825378418\n",
      "895 step d_loss1: 0.3801891803741455, d_loss2: 0.38070908188819885, g_loss: 9.808103561401367\n",
      "896 step d_loss1: 0.42376354336738586, d_loss2: 0.34419000148773193, g_loss: 8.77316951751709\n",
      "897 step d_loss1: 0.35540080070495605, d_loss2: 0.34930965304374695, g_loss: 7.837964057922363\n",
      "898 step d_loss1: 0.3639439344406128, d_loss2: 0.3536526560783386, g_loss: 7.428549766540527\n",
      "899 step d_loss1: 0.24943509697914124, d_loss2: 0.308133065700531, g_loss: 8.96949291229248\n",
      "900 step d_loss1: 0.42089778184890747, d_loss2: 0.3515210747718811, g_loss: 6.044809341430664\n",
      "901 step d_loss1: 0.29029160737991333, d_loss2: 0.37417852878570557, g_loss: 7.568114757537842\n",
      "902 step d_loss1: 0.37159645557403564, d_loss2: 0.37013667821884155, g_loss: 9.039831161499023\n",
      "903 step d_loss1: 0.46485331654548645, d_loss2: 0.28464165329933167, g_loss: 7.160931587219238\n",
      "904 step d_loss1: 0.29423296451568604, d_loss2: 0.4296098053455353, g_loss: 6.577815055847168\n",
      "905 step d_loss1: 0.23343601822853088, d_loss2: 0.301830530166626, g_loss: 6.582904815673828\n",
      "906 step d_loss1: 0.26248136162757874, d_loss2: 0.29013925790786743, g_loss: 9.363807678222656\n",
      "907 step d_loss1: 0.4689977467060089, d_loss2: 0.32687899470329285, g_loss: 7.856302261352539\n",
      "908 step d_loss1: 0.31991147994995117, d_loss2: 0.36734911799430847, g_loss: 6.196399688720703\n",
      "909 step d_loss1: 0.36118805408477783, d_loss2: 0.32648763060569763, g_loss: 6.569702625274658\n",
      "910 step d_loss1: 0.4429164230823517, d_loss2: 0.3125860095024109, g_loss: 6.1042890548706055\n",
      "911 step d_loss1: 0.3779589831829071, d_loss2: 0.34639832377433777, g_loss: 6.068591117858887\n",
      "912 step d_loss1: 0.24080601334571838, d_loss2: 0.36985233426094055, g_loss: 8.43323802947998\n",
      "913 step d_loss1: 0.3145204186439514, d_loss2: 0.37553346157073975, g_loss: 7.196645736694336\n",
      "914 step d_loss1: 0.2553003430366516, d_loss2: 0.33907774090766907, g_loss: 9.486824989318848\n",
      "915 step d_loss1: 0.30488091707229614, d_loss2: 0.3039841949939728, g_loss: 9.803665161132812\n",
      "916 step d_loss1: 0.2051631212234497, d_loss2: 0.36864882707595825, g_loss: 20.389816284179688\n",
      "917 step d_loss1: 0.3327537775039673, d_loss2: 0.24567300081253052, g_loss: 6.732134819030762\n",
      "918 step d_loss1: 0.39202383160591125, d_loss2: 0.338240385055542, g_loss: 9.307358741760254\n",
      "919 step d_loss1: 0.31615719199180603, d_loss2: 0.2861064672470093, g_loss: 7.201726913452148\n",
      "920 step d_loss1: 0.2759568989276886, d_loss2: 0.4457484185695648, g_loss: 7.993728160858154\n",
      "921 step d_loss1: 0.3325972557067871, d_loss2: 0.31003236770629883, g_loss: 7.22035551071167\n",
      "922 step d_loss1: 0.18816816806793213, d_loss2: 0.3464779257774353, g_loss: 10.767877578735352\n",
      "923 step d_loss1: 0.42416122555732727, d_loss2: 0.38740459084510803, g_loss: 7.258558750152588\n",
      "924 step d_loss1: 0.32002630829811096, d_loss2: 0.36620309948921204, g_loss: 18.78331184387207\n",
      "925 step d_loss1: 0.5419439673423767, d_loss2: 0.36446020007133484, g_loss: 7.189040660858154\n",
      "926 step d_loss1: 0.45485347509384155, d_loss2: 0.35434257984161377, g_loss: 6.481176376342773\n",
      "927 step d_loss1: 0.32596370577812195, d_loss2: 0.42265427112579346, g_loss: 7.298196315765381\n",
      "928 step d_loss1: 0.3363351821899414, d_loss2: 0.35147494077682495, g_loss: 8.863797187805176\n",
      "929 step d_loss1: 0.365580290555954, d_loss2: 0.2821314036846161, g_loss: 7.367173194885254\n",
      "930 step d_loss1: 0.36036524176597595, d_loss2: 0.36013904213905334, g_loss: 9.908015251159668\n",
      "931 step d_loss1: 0.2982933521270752, d_loss2: 0.35276585817337036, g_loss: 8.966938018798828\n",
      "932 step d_loss1: 0.27853459119796753, d_loss2: 0.2920992076396942, g_loss: 8.721590042114258\n",
      "933 step d_loss1: 0.43466806411743164, d_loss2: 0.33901655673980713, g_loss: 7.119379043579102\n",
      "934 step d_loss1: 0.24707767367362976, d_loss2: 0.30801621079444885, g_loss: 8.98305892944336\n",
      "935 step d_loss1: 0.4105503559112549, d_loss2: 0.3650331199169159, g_loss: 6.248467445373535\n",
      "936 step d_loss1: 0.29589200019836426, d_loss2: 0.4191567301750183, g_loss: 8.203018188476562\n",
      "937 step d_loss1: 0.41560786962509155, d_loss2: 0.37185704708099365, g_loss: 7.087050914764404\n",
      "938 step d_loss1: 0.2606458067893982, d_loss2: 0.3995998799800873, g_loss: 7.1832275390625\n",
      "939 step d_loss1: 0.3544620871543884, d_loss2: 0.388671875, g_loss: 6.8198699951171875\n",
      "940 step d_loss1: 0.3867637813091278, d_loss2: 0.3404172658920288, g_loss: 6.684337615966797\n",
      "941 step d_loss1: 0.3650330901145935, d_loss2: 0.39505788683891296, g_loss: 5.436792850494385\n",
      "942 step d_loss1: 0.27662554383277893, d_loss2: 0.4096304476261139, g_loss: 28.26984977722168\n",
      "943 step d_loss1: 0.32964158058166504, d_loss2: 0.39678093791007996, g_loss: 7.781102657318115\n",
      "944 step d_loss1: 0.36556345224380493, d_loss2: 0.34352049231529236, g_loss: 6.336026668548584\n",
      "945 step d_loss1: 0.29538625478744507, d_loss2: 0.3477311432361603, g_loss: 8.828818321228027\n",
      "946 step d_loss1: 0.34088635444641113, d_loss2: 0.4612162709236145, g_loss: 11.993946075439453\n",
      "947 step d_loss1: 0.30718475580215454, d_loss2: 0.2719040811061859, g_loss: 8.970295906066895\n",
      "948 step d_loss1: 0.3228403925895691, d_loss2: 0.3629915118217468, g_loss: 10.721500396728516\n",
      "949 step d_loss1: 0.4501018822193146, d_loss2: 0.38212502002716064, g_loss: 7.298105716705322\n",
      "950 step d_loss1: 0.21846936643123627, d_loss2: 0.2875412106513977, g_loss: 8.367981910705566\n",
      "951 step d_loss1: 0.21942578256130219, d_loss2: 0.398748517036438, g_loss: 9.859448432922363\n",
      "952 step d_loss1: 0.45629265904426575, d_loss2: 0.33683085441589355, g_loss: 8.359210968017578\n",
      "953 step d_loss1: 0.30083370208740234, d_loss2: 0.36026856303215027, g_loss: 8.60494327545166\n",
      "954 step d_loss1: 0.4813137352466583, d_loss2: 0.3470373749732971, g_loss: 7.713179111480713\n",
      "955 step d_loss1: 0.39066118001937866, d_loss2: 0.3320431411266327, g_loss: 6.7826385498046875\n",
      "956 step d_loss1: 0.364099383354187, d_loss2: 0.34094324707984924, g_loss: 10.251662254333496\n",
      "957 step d_loss1: 0.332820862531662, d_loss2: 0.2690964937210083, g_loss: 8.325316429138184\n",
      "958 step d_loss1: 0.2353028655052185, d_loss2: 0.325356125831604, g_loss: 16.71199607849121\n",
      "959 step d_loss1: 0.43218228220939636, d_loss2: 0.33291763067245483, g_loss: 7.142897605895996\n",
      "960 step d_loss1: 0.25336766242980957, d_loss2: 0.4241415560245514, g_loss: 8.200940132141113\n",
      "961 step d_loss1: 0.28705894947052, d_loss2: 0.276459276676178, g_loss: 8.546451568603516\n",
      "962 step d_loss1: 0.27590957283973694, d_loss2: 0.27946972846984863, g_loss: 8.60816764831543\n",
      "963 step d_loss1: 0.578937828540802, d_loss2: 0.30337196588516235, g_loss: 6.547285079956055\n",
      "964 step d_loss1: 0.33849844336509705, d_loss2: 0.3673563003540039, g_loss: 7.804291725158691\n",
      "965 step d_loss1: 0.2818070352077484, d_loss2: 0.34675130248069763, g_loss: 7.426843643188477\n",
      "966 step d_loss1: 0.3112960159778595, d_loss2: 0.5096104145050049, g_loss: 8.1553955078125\n",
      "967 step d_loss1: 0.308999240398407, d_loss2: 0.34918275475502014, g_loss: 7.862736225128174\n",
      "968 step d_loss1: 0.4348963499069214, d_loss2: 0.30340293049812317, g_loss: 7.501283168792725\n",
      "969 step d_loss1: 0.32765036821365356, d_loss2: 0.3875606060028076, g_loss: 17.168935775756836\n",
      "970 step d_loss1: 0.4492618441581726, d_loss2: 0.3687819540500641, g_loss: 7.429462432861328\n",
      "971 step d_loss1: 0.2929159998893738, d_loss2: 0.35303109884262085, g_loss: 6.923871040344238\n",
      "972 step d_loss1: 0.29188719391822815, d_loss2: 0.28795725107192993, g_loss: 16.59126853942871\n",
      "973 step d_loss1: 0.45838266611099243, d_loss2: 0.25684046745300293, g_loss: 8.06279468536377\n",
      "974 step d_loss1: 0.3276669383049011, d_loss2: 0.29207590222358704, g_loss: 8.381083488464355\n",
      "975 step d_loss1: 0.20095504820346832, d_loss2: 0.4991161525249481, g_loss: 19.450881958007812\n",
      "976 step d_loss1: 0.34067678451538086, d_loss2: 0.31729236245155334, g_loss: 11.619688987731934\n",
      "977 step d_loss1: 0.5164302587509155, d_loss2: 0.315113365650177, g_loss: 7.098428249359131\n",
      "978 step d_loss1: 0.32076627016067505, d_loss2: 0.2923935353755951, g_loss: 8.644057273864746\n",
      "979 step d_loss1: 0.2686423659324646, d_loss2: 0.35800760984420776, g_loss: 17.382535934448242\n",
      "980 step d_loss1: 0.3499516546726227, d_loss2: 0.24861924350261688, g_loss: 9.067008972167969\n",
      "981 step d_loss1: 0.2605319619178772, d_loss2: 0.3382893204689026, g_loss: 19.518362045288086\n",
      "982 step d_loss1: 0.4400745630264282, d_loss2: 0.339897096157074, g_loss: 7.008581161499023\n",
      "983 step d_loss1: 0.40812361240386963, d_loss2: 0.42081356048583984, g_loss: 6.462568283081055\n",
      "984 step d_loss1: 0.3193005323410034, d_loss2: 0.3996772766113281, g_loss: 6.7744855880737305\n",
      "985 step d_loss1: 0.44767633080482483, d_loss2: 0.3640751242637634, g_loss: 6.8080244064331055\n",
      "986 step d_loss1: 0.38304832577705383, d_loss2: 0.37970489263534546, g_loss: 5.9145426750183105\n",
      "987 step d_loss1: 0.27046412229537964, d_loss2: 0.3348553776741028, g_loss: 9.470049858093262\n",
      "988 step d_loss1: 0.31421715021133423, d_loss2: 0.4042421579360962, g_loss: 6.2606892585754395\n",
      "989 step d_loss1: 0.2821444272994995, d_loss2: 0.27785807847976685, g_loss: 8.15250015258789\n",
      "990 step d_loss1: 0.2911742329597473, d_loss2: 0.38508477807044983, g_loss: 11.212752342224121\n",
      "991 step d_loss1: 0.20234403014183044, d_loss2: 0.4427826404571533, g_loss: 20.275999069213867\n",
      "992 step d_loss1: 0.4390121102333069, d_loss2: 0.31611862778663635, g_loss: 7.045751571655273\n",
      "993 step d_loss1: 0.38079366087913513, d_loss2: 0.29239505529403687, g_loss: 7.756415843963623\n",
      "994 step d_loss1: 0.29949626326560974, d_loss2: 0.34333062171936035, g_loss: 8.31515884399414\n",
      "995 step d_loss1: 0.3204805254936218, d_loss2: 0.2937786281108856, g_loss: 8.802330017089844\n",
      "996 step d_loss1: 0.22143638134002686, d_loss2: 0.37075328826904297, g_loss: 11.079659461975098\n",
      "997 step d_loss1: 0.39200156927108765, d_loss2: 0.2985629141330719, g_loss: 8.873973846435547\n",
      "998 step d_loss1: 0.39714810252189636, d_loss2: 0.3787286877632141, g_loss: 8.033077239990234\n",
      "999 step d_loss1: 0.37015634775161743, d_loss2: 0.2983967065811157, g_loss: 7.547880172729492\n",
      "1000 step d_loss1: 0.29586538672447205, d_loss2: 0.48605960607528687, g_loss: 8.612777709960938\n",
      "1001 step d_loss1: 0.3085422217845917, d_loss2: 0.41648155450820923, g_loss: 8.610042572021484\n",
      "1002 step d_loss1: 0.40758180618286133, d_loss2: 0.2801753580570221, g_loss: 8.528133392333984\n",
      "1003 step d_loss1: 0.41138726472854614, d_loss2: 0.4040167033672333, g_loss: 6.813806533813477\n",
      "1004 step d_loss1: 0.3392668068408966, d_loss2: 0.3159526586532593, g_loss: 8.522716522216797\n",
      "1005 step d_loss1: 0.305676132440567, d_loss2: 0.39619821310043335, g_loss: 7.097358703613281\n",
      "1006 step d_loss1: 0.2849445044994354, d_loss2: 0.33756816387176514, g_loss: 6.594418525695801\n",
      "1007 step d_loss1: 0.3430740535259247, d_loss2: 0.5105805397033691, g_loss: 30.49319839477539\n",
      "1008 step d_loss1: 0.444510817527771, d_loss2: 0.32363322377204895, g_loss: 9.511948585510254\n",
      "1009 step d_loss1: 0.21926932036876678, d_loss2: 0.39784032106399536, g_loss: 6.6344804763793945\n",
      "1010 step d_loss1: 0.4479600787162781, d_loss2: 0.34061160683631897, g_loss: 5.990845680236816\n",
      "1011 step d_loss1: 0.29725486040115356, d_loss2: 0.3356401324272156, g_loss: 9.57530403137207\n",
      "1012 step d_loss1: 0.28872692584991455, d_loss2: 0.3101484775543213, g_loss: 8.365106582641602\n",
      "1013 step d_loss1: 0.4772140681743622, d_loss2: 0.3918750286102295, g_loss: 6.157243251800537\n",
      "1014 step d_loss1: 0.33847784996032715, d_loss2: 0.44840678572654724, g_loss: 7.074687480926514\n",
      "1015 step d_loss1: 0.2794342637062073, d_loss2: 0.30115464329719543, g_loss: 8.365463256835938\n",
      "1016 step d_loss1: 0.3040855824947357, d_loss2: 0.2890262007713318, g_loss: 7.246674537658691\n",
      "1017 step d_loss1: 0.4049205780029297, d_loss2: 0.2650988698005676, g_loss: 7.06330680847168\n",
      "1018 step d_loss1: 0.29767125844955444, d_loss2: 0.30291852355003357, g_loss: 8.412968635559082\n",
      "1019 step d_loss1: 0.27716687321662903, d_loss2: 0.25144773721694946, g_loss: 6.006148815155029\n",
      "1020 step d_loss1: 0.36494821310043335, d_loss2: 0.2926146388053894, g_loss: 8.035697937011719\n",
      "1021 step d_loss1: 0.3134859502315521, d_loss2: 0.3321966528892517, g_loss: 8.045140266418457\n",
      "1022 step d_loss1: 0.36144763231277466, d_loss2: 0.32248610258102417, g_loss: 6.6683783531188965\n",
      "1023 step d_loss1: 0.24664567410945892, d_loss2: 0.3673495352268219, g_loss: 8.323925018310547\n",
      "1024 step d_loss1: 0.19771423935890198, d_loss2: 0.4889110028743744, g_loss: 20.951967239379883\n",
      "1025 step d_loss1: 0.35058271884918213, d_loss2: 0.3348750174045563, g_loss: 7.6382246017456055\n",
      "1026 step d_loss1: 0.32014456391334534, d_loss2: 0.3472505807876587, g_loss: 10.124064445495605\n",
      "1027 step d_loss1: 0.30218830704689026, d_loss2: 0.27956292033195496, g_loss: 11.747913360595703\n",
      "1028 step d_loss1: 0.3677789866924286, d_loss2: 0.3038678765296936, g_loss: 8.64572525024414\n",
      "1029 step d_loss1: 0.3871067464351654, d_loss2: 0.33362454175949097, g_loss: 15.646350860595703\n",
      "1030 step d_loss1: 0.5210358500480652, d_loss2: 0.35236480832099915, g_loss: 6.649542808532715\n",
      "1031 step d_loss1: 0.33826684951782227, d_loss2: 0.40364521741867065, g_loss: 37.55477523803711\n",
      "1032 step d_loss1: 0.462570458650589, d_loss2: 0.2913651466369629, g_loss: 7.744685173034668\n",
      "1033 step d_loss1: 0.2956855893135071, d_loss2: 0.3663344383239746, g_loss: 8.104573249816895\n",
      "1034 step d_loss1: 0.30323919653892517, d_loss2: 0.2888970673084259, g_loss: 8.656763076782227\n",
      "1035 step d_loss1: 0.28932860493659973, d_loss2: 0.3823000192642212, g_loss: 9.65380859375\n",
      "1036 step d_loss1: 0.3079156279563904, d_loss2: 0.31920021772384644, g_loss: 7.4414777755737305\n",
      "1037 step d_loss1: 0.1967262327671051, d_loss2: 0.22001583874225616, g_loss: 10.537544250488281\n",
      "1038 step d_loss1: 0.36276137828826904, d_loss2: 0.305056095123291, g_loss: 7.978196620941162\n",
      "1039 step d_loss1: 0.31191667914390564, d_loss2: 0.25684142112731934, g_loss: 8.065711975097656\n",
      "1040 step d_loss1: 0.17632950842380524, d_loss2: 0.4317176640033722, g_loss: 9.83432388305664\n",
      "1041 step d_loss1: 0.389799565076828, d_loss2: 0.5278950929641724, g_loss: 7.436954975128174\n",
      "1042 step d_loss1: 0.43809446692466736, d_loss2: 0.2933632433414459, g_loss: 8.660624504089355\n",
      "1043 step d_loss1: 0.37639981508255005, d_loss2: 0.26037856936454773, g_loss: 8.837624549865723\n",
      "1044 step d_loss1: 0.2801317572593689, d_loss2: 0.3194749355316162, g_loss: 8.587833404541016\n",
      "1045 step d_loss1: 0.2524013817310333, d_loss2: 0.28556105494499207, g_loss: 7.649807929992676\n",
      "1046 step d_loss1: 0.3729230463504791, d_loss2: 0.395201712846756, g_loss: 6.4955244064331055\n",
      "1047 step d_loss1: 0.19508543610572815, d_loss2: 0.3320574462413788, g_loss: 9.2129545211792\n",
      "1048 step d_loss1: 0.43476417660713196, d_loss2: 0.2965583801269531, g_loss: 7.388476371765137\n",
      "1049 step d_loss1: 0.28863632678985596, d_loss2: 0.3182951509952545, g_loss: 8.358316421508789\n",
      "1050 step d_loss1: 0.2607838809490204, d_loss2: 0.3663296401500702, g_loss: 14.815564155578613\n",
      "1051 step d_loss1: 0.3318781852722168, d_loss2: 0.33730119466781616, g_loss: 9.689558029174805\n",
      "1052 step d_loss1: 0.4986853003501892, d_loss2: 0.37718117237091064, g_loss: 7.1406474113464355\n",
      "1053 step d_loss1: 0.3030606806278229, d_loss2: 0.38171008229255676, g_loss: 7.985329627990723\n",
      "1054 step d_loss1: 0.34277865290641785, d_loss2: 0.33522531390190125, g_loss: 13.552994728088379\n",
      "1055 step d_loss1: 0.4637894332408905, d_loss2: 0.2972413897514343, g_loss: 6.571296215057373\n",
      "1056 step d_loss1: 0.38403552770614624, d_loss2: 0.38387954235076904, g_loss: 8.999161720275879\n",
      "1057 step d_loss1: 0.3389754295349121, d_loss2: 0.38263484835624695, g_loss: 7.264383792877197\n",
      "1058 step d_loss1: 0.40006113052368164, d_loss2: 0.48447948694229126, g_loss: 8.188610076904297\n",
      "1059 step d_loss1: 0.335726261138916, d_loss2: 0.39993420243263245, g_loss: 6.556723594665527\n",
      "1060 step d_loss1: 0.29497280716896057, d_loss2: 0.32875242829322815, g_loss: 5.754336357116699\n",
      "1061 step d_loss1: 0.3246377110481262, d_loss2: 0.30913591384887695, g_loss: 8.638993263244629\n",
      "1062 step d_loss1: 0.39219656586647034, d_loss2: 0.38597631454467773, g_loss: 6.32367467880249\n",
      "1063 step d_loss1: 0.4605412781238556, d_loss2: 0.3204839825630188, g_loss: 6.437132358551025\n",
      "1064 step d_loss1: 0.3241989016532898, d_loss2: 0.3713577091693878, g_loss: 7.679696083068848\n",
      "1065 step d_loss1: 0.27937495708465576, d_loss2: 0.4618404507637024, g_loss: 8.26801872253418\n",
      "1066 step d_loss1: 0.2575574815273285, d_loss2: 0.32782357931137085, g_loss: 8.64207649230957\n",
      "1067 step d_loss1: 0.35893458127975464, d_loss2: 0.32506614923477173, g_loss: 6.779316425323486\n",
      "1068 step d_loss1: 0.3219441771507263, d_loss2: 0.2692122757434845, g_loss: 7.591403961181641\n",
      "1069 step d_loss1: 0.3747621774673462, d_loss2: 0.3747859299182892, g_loss: 6.514415740966797\n",
      "1070 step d_loss1: 0.33232444524765015, d_loss2: 0.27102965116500854, g_loss: 7.945037364959717\n",
      "1071 step d_loss1: 0.24377170205116272, d_loss2: 0.3785032033920288, g_loss: 9.32989501953125\n",
      "1072 step d_loss1: 0.382621705532074, d_loss2: 0.3379519581794739, g_loss: 9.732818603515625\n",
      "1073 step d_loss1: 0.27554580569267273, d_loss2: 0.4100572466850281, g_loss: 7.324941158294678\n",
      "1074 step d_loss1: 0.32965362071990967, d_loss2: 0.27490583062171936, g_loss: 7.533959865570068\n",
      "1075 step d_loss1: 0.40318188071250916, d_loss2: 0.35526129603385925, g_loss: 6.916001319885254\n",
      "1076 step d_loss1: 0.2391728162765503, d_loss2: 0.34274816513061523, g_loss: 7.366065502166748\n",
      "1077 step d_loss1: 0.3031831979751587, d_loss2: 0.3107031583786011, g_loss: 10.379692077636719\n",
      "1078 step d_loss1: 0.5155747532844543, d_loss2: 0.34425458312034607, g_loss: 7.029491901397705\n",
      "1079 step d_loss1: 0.26649749279022217, d_loss2: 0.3550553321838379, g_loss: 8.850180625915527\n",
      "1080 step d_loss1: 0.3741839528083801, d_loss2: 0.4762263298034668, g_loss: 7.849559783935547\n",
      "1081 step d_loss1: 0.43229496479034424, d_loss2: 0.28906843066215515, g_loss: 7.319502353668213\n",
      "1082 step d_loss1: 0.4182422161102295, d_loss2: 0.41624879837036133, g_loss: 7.157294273376465\n",
      "1083 step d_loss1: 0.34553536772727966, d_loss2: 0.4124765396118164, g_loss: 6.992194652557373\n",
      "1084 step d_loss1: 0.4469949007034302, d_loss2: 0.347881942987442, g_loss: 7.080667018890381\n",
      "1085 step d_loss1: 0.18591006100177765, d_loss2: 0.46559494733810425, g_loss: 12.464471817016602\n",
      "1086 step d_loss1: 0.5286424160003662, d_loss2: 0.2996246814727783, g_loss: 5.7505645751953125\n",
      "1087 step d_loss1: 0.4095049500465393, d_loss2: 0.3144441545009613, g_loss: 6.373702526092529\n",
      "1088 step d_loss1: 0.3130108416080475, d_loss2: 0.4252029061317444, g_loss: 8.69072151184082\n",
      "1089 step d_loss1: 0.36368030309677124, d_loss2: 0.3052525520324707, g_loss: 6.576216697692871\n",
      "1090 step d_loss1: 0.3907856345176697, d_loss2: 0.31685584783554077, g_loss: 6.490996360778809\n",
      "1091 step d_loss1: 0.3534194827079773, d_loss2: 0.3419284224510193, g_loss: 16.022689819335938\n",
      "1092 step d_loss1: 0.29700806736946106, d_loss2: 0.4580538868904114, g_loss: 8.67392349243164\n",
      "1093 step d_loss1: 0.35995352268218994, d_loss2: 0.36923882365226746, g_loss: 6.36834716796875\n",
      "1094 step d_loss1: 0.3238765597343445, d_loss2: 0.31142744421958923, g_loss: 6.692948818206787\n",
      "1095 step d_loss1: 0.3462015688419342, d_loss2: 0.3885270357131958, g_loss: 5.939748764038086\n",
      "1096 step d_loss1: 0.39120084047317505, d_loss2: 0.3403857946395874, g_loss: 6.793637752532959\n",
      "1097 step d_loss1: 0.3792033791542053, d_loss2: 0.36214354634284973, g_loss: 11.455507278442383\n",
      "1098 step d_loss1: 0.2437390685081482, d_loss2: 0.3788892924785614, g_loss: 7.994858264923096\n",
      "1099 step d_loss1: 0.30848759412765503, d_loss2: 0.44347602128982544, g_loss: 12.972871780395508\n",
      "1100 step d_loss1: 0.4874873161315918, d_loss2: 0.3111109733581543, g_loss: 5.828794956207275\n",
      "1101 step d_loss1: 0.3231784701347351, d_loss2: 0.3072656989097595, g_loss: 10.515239715576172\n",
      "1102 step d_loss1: 0.25408053398132324, d_loss2: 0.3663802742958069, g_loss: 7.259852409362793\n",
      "1103 step d_loss1: 0.3828493654727936, d_loss2: 0.40137308835983276, g_loss: 7.832015037536621\n",
      "1104 step d_loss1: 0.3877300024032593, d_loss2: 0.39652886986732483, g_loss: 6.350574970245361\n",
      "1105 step d_loss1: 0.36242038011550903, d_loss2: 0.407288521528244, g_loss: 7.94080114364624\n",
      "1106 step d_loss1: 0.3413173258304596, d_loss2: 0.4943372309207916, g_loss: 6.499155521392822\n",
      "1107 step d_loss1: 0.31881535053253174, d_loss2: 0.33410632610321045, g_loss: 9.305920600891113\n",
      "1108 step d_loss1: 0.3924236595630646, d_loss2: 0.3326832950115204, g_loss: 6.151529788970947\n",
      "1109 step d_loss1: 0.28864243626594543, d_loss2: 0.3607349097728729, g_loss: 7.6326518058776855\n",
      "1110 step d_loss1: 0.5151438117027283, d_loss2: 0.28775179386138916, g_loss: 5.813684940338135\n",
      "1111 step d_loss1: 0.22059768438339233, d_loss2: 0.458251416683197, g_loss: 9.172442436218262\n",
      "1112 step d_loss1: 0.21825876832008362, d_loss2: 0.32356491684913635, g_loss: 22.919052124023438\n",
      "1113 step d_loss1: 0.4390949010848999, d_loss2: 0.30527952313423157, g_loss: 7.1672163009643555\n",
      "1114 step d_loss1: 0.415364146232605, d_loss2: 0.3047865629196167, g_loss: 7.958316802978516\n",
      "1115 step d_loss1: 0.3666832447052002, d_loss2: 0.3234683871269226, g_loss: 7.412118434906006\n",
      "1116 step d_loss1: 0.3204489052295685, d_loss2: 0.46297428011894226, g_loss: 10.526117324829102\n",
      "1117 step d_loss1: 0.4000522792339325, d_loss2: 0.31282976269721985, g_loss: 7.362094402313232\n",
      "1118 step d_loss1: 0.4476619064807892, d_loss2: 0.31894147396087646, g_loss: 6.060784816741943\n",
      "1119 step d_loss1: 0.3635557293891907, d_loss2: 0.32995638251304626, g_loss: 7.652771949768066\n",
      "1120 step d_loss1: 0.21788641810417175, d_loss2: 0.37732207775115967, g_loss: 14.577032089233398\n",
      "1121 step d_loss1: 0.39868614077568054, d_loss2: 0.34600865840911865, g_loss: 7.294888973236084\n",
      "1122 step d_loss1: 0.30541911721229553, d_loss2: 0.3698662221431732, g_loss: 8.232928276062012\n",
      "1123 step d_loss1: 0.34827280044555664, d_loss2: 0.30902737379074097, g_loss: 8.991254806518555\n",
      "1124 step d_loss1: 0.30003121495246887, d_loss2: 0.3262091875076294, g_loss: 9.185247421264648\n",
      "1125 step d_loss1: 0.29925236105918884, d_loss2: 0.3098521828651428, g_loss: 8.391791343688965\n",
      "1126 step d_loss1: 0.2615464925765991, d_loss2: 0.26156720519065857, g_loss: 9.02093505859375\n",
      "1127 step d_loss1: 0.36598023772239685, d_loss2: 0.3595733344554901, g_loss: 8.300457954406738\n",
      "1128 step d_loss1: 0.22670282423496246, d_loss2: 0.349507212638855, g_loss: 13.625829696655273\n",
      "1129 step d_loss1: 0.3551386296749115, d_loss2: 0.27440840005874634, g_loss: 6.1318488121032715\n",
      "1130 step d_loss1: 0.2855216860771179, d_loss2: 0.3151606619358063, g_loss: 8.766355514526367\n",
      "1131 step d_loss1: 0.1906345784664154, d_loss2: 0.4734533131122589, g_loss: 9.338057518005371\n",
      "1132 step d_loss1: 0.30328232049942017, d_loss2: 0.2819359302520752, g_loss: 9.161340713500977\n",
      "1133 step d_loss1: 0.5823583602905273, d_loss2: 0.32205837965011597, g_loss: 6.640697479248047\n",
      "1134 step d_loss1: 0.3547736406326294, d_loss2: 0.4083392918109894, g_loss: 6.812660217285156\n",
      "1135 step d_loss1: 0.37573879957199097, d_loss2: 0.2794766426086426, g_loss: 7.6576433181762695\n",
      "1136 step d_loss1: 0.40317949652671814, d_loss2: 0.3563130497932434, g_loss: 6.695963382720947\n",
      "1137 step d_loss1: 0.3423295021057129, d_loss2: 0.32416343688964844, g_loss: 7.140812873840332\n",
      "1138 step d_loss1: 0.2578577995300293, d_loss2: 0.3636484444141388, g_loss: 10.203543663024902\n",
      "1139 step d_loss1: 0.25825396180152893, d_loss2: 0.2480807602405548, g_loss: 13.914325714111328\n",
      "1140 step d_loss1: 0.4369119703769684, d_loss2: 0.3295285701751709, g_loss: 7.447951793670654\n",
      "1141 step d_loss1: 0.3391271233558655, d_loss2: 0.3405713438987732, g_loss: 7.757755279541016\n",
      "1142 step d_loss1: 0.34193241596221924, d_loss2: 0.3414115011692047, g_loss: 7.874536514282227\n",
      "1143 step d_loss1: 0.17253875732421875, d_loss2: 0.40245428681373596, g_loss: 22.78826141357422\n",
      "1144 step d_loss1: 0.37109237909317017, d_loss2: 0.40033650398254395, g_loss: 6.511956214904785\n",
      "1145 step d_loss1: 0.26527485251426697, d_loss2: 0.4780097007751465, g_loss: 29.498754501342773\n",
      "1146 step d_loss1: 0.5719254016876221, d_loss2: 0.2632385492324829, g_loss: 6.897002696990967\n",
      "1147 step d_loss1: 0.44598907232284546, d_loss2: 0.3375769853591919, g_loss: 7.327486991882324\n",
      "1148 step d_loss1: 0.2277718037366867, d_loss2: 0.431814044713974, g_loss: 9.195194244384766\n",
      "1149 step d_loss1: 0.42867764830589294, d_loss2: 0.3117203712463379, g_loss: 6.302218914031982\n",
      "1150 step d_loss1: 0.3929136395454407, d_loss2: 0.3122527003288269, g_loss: 9.654336929321289\n",
      "1151 step d_loss1: 0.37323203682899475, d_loss2: 0.327758252620697, g_loss: 6.515926837921143\n",
      "1152 step d_loss1: 0.37463438510894775, d_loss2: 0.317220538854599, g_loss: 6.2606587409973145\n",
      "1153 step d_loss1: 0.33136090636253357, d_loss2: 0.32270029187202454, g_loss: 7.945939064025879\n",
      "1154 step d_loss1: 0.15818502008914948, d_loss2: 0.3202841877937317, g_loss: 8.14529037475586\n",
      "1155 step d_loss1: 0.26210540533065796, d_loss2: 0.28131648898124695, g_loss: 7.4766740798950195\n",
      "1156 step d_loss1: 0.35332852602005005, d_loss2: 0.2927478849887848, g_loss: 8.721478462219238\n",
      "1157 step d_loss1: 0.21067969501018524, d_loss2: 0.2572455108165741, g_loss: 6.9747490882873535\n",
      "1158 step d_loss1: 0.22338902950286865, d_loss2: 0.35262617468833923, g_loss: 9.033358573913574\n",
      "1159 step d_loss1: 0.2922300398349762, d_loss2: 0.42914825677871704, g_loss: 7.321716785430908\n",
      "1160 step d_loss1: 0.2545287013053894, d_loss2: 0.3348211348056793, g_loss: 13.837462425231934\n",
      "1161 step d_loss1: 0.5403611063957214, d_loss2: 0.31621384620666504, g_loss: 6.087044715881348\n",
      "1162 step d_loss1: 0.5400426387786865, d_loss2: 0.27375543117523193, g_loss: 6.803309440612793\n",
      "1163 step d_loss1: 0.2680249810218811, d_loss2: 0.5122101902961731, g_loss: 5.782609939575195\n",
      "1164 step d_loss1: 0.3913770318031311, d_loss2: 0.27330467104911804, g_loss: 14.46251392364502\n",
      "1165 step d_loss1: 0.26236197352409363, d_loss2: 0.45457470417022705, g_loss: 12.261796951293945\n",
      "1166 step d_loss1: 0.38136088848114014, d_loss2: 0.369795024394989, g_loss: 8.179322242736816\n",
      "1167 step d_loss1: 0.3360458016395569, d_loss2: 0.2862388789653778, g_loss: 8.870357513427734\n",
      "1168 step d_loss1: 0.48890143632888794, d_loss2: 0.309416800737381, g_loss: 6.364174842834473\n",
      "1169 step d_loss1: 0.40900343656539917, d_loss2: 0.385670006275177, g_loss: 8.16735553741455\n",
      "1170 step d_loss1: 0.35440200567245483, d_loss2: 0.3955138027667999, g_loss: 5.988924980163574\n",
      "1171 step d_loss1: 0.3582124412059784, d_loss2: 0.35143664479255676, g_loss: 6.470886707305908\n",
      "1172 step d_loss1: 0.37941938638687134, d_loss2: 0.40896251797676086, g_loss: 7.456418037414551\n",
      "1173 step d_loss1: 0.28212058544158936, d_loss2: 0.33277055621147156, g_loss: 7.418816566467285\n",
      "1174 step d_loss1: 0.45183229446411133, d_loss2: 0.3839649558067322, g_loss: 6.182465076446533\n",
      "1175 step d_loss1: 0.3302988111972809, d_loss2: 0.37223905324935913, g_loss: 6.881836414337158\n",
      "1176 step d_loss1: 0.4173232316970825, d_loss2: 0.35971638560295105, g_loss: 8.710620880126953\n",
      "1177 step d_loss1: 0.4228455424308777, d_loss2: 0.3373035192489624, g_loss: 7.367896556854248\n",
      "1178 step d_loss1: 0.2285875529050827, d_loss2: 0.4943138659000397, g_loss: 19.833158493041992\n",
      "1179 step d_loss1: 0.3085048794746399, d_loss2: 0.3377324938774109, g_loss: 19.54061508178711\n",
      "1180 step d_loss1: 0.22335201501846313, d_loss2: 0.28941068053245544, g_loss: 14.76810073852539\n",
      "1181 step d_loss1: 0.5047486424446106, d_loss2: 0.2279306948184967, g_loss: 8.832708358764648\n",
      "1182 step d_loss1: 0.3171788454055786, d_loss2: 0.35271626710891724, g_loss: 6.770316123962402\n",
      "1183 step d_loss1: 0.28618133068084717, d_loss2: 0.3558353781700134, g_loss: 6.848857879638672\n",
      "1184 step d_loss1: 0.3520650267601013, d_loss2: 0.3479095995426178, g_loss: 7.063267707824707\n",
      "1185 step d_loss1: 0.4258245825767517, d_loss2: 0.2885952591896057, g_loss: 10.163660049438477\n",
      "1186 step d_loss1: 0.19682076573371887, d_loss2: 0.2910569906234741, g_loss: 7.775214195251465\n",
      "1187 step d_loss1: 0.31909066438674927, d_loss2: 0.3174763321876526, g_loss: 6.941751480102539\n",
      "1188 step d_loss1: 0.20180341601371765, d_loss2: 0.3888828754425049, g_loss: 9.899758338928223\n",
      "1189 step d_loss1: 0.24993744492530823, d_loss2: 0.320142924785614, g_loss: 13.426515579223633\n",
      "1190 step d_loss1: 0.44696056842803955, d_loss2: 0.3218478858470917, g_loss: 8.759926795959473\n",
      "1191 step d_loss1: 0.5062383413314819, d_loss2: 0.34709978103637695, g_loss: 6.5283918380737305\n",
      "1192 step d_loss1: 0.2149316370487213, d_loss2: 0.38657906651496887, g_loss: 8.854963302612305\n",
      "1193 step d_loss1: 0.18940027058124542, d_loss2: 0.47461751103401184, g_loss: 7.215873718261719\n",
      "1194 step d_loss1: 0.2800392806529999, d_loss2: 0.36606454849243164, g_loss: 17.88862419128418\n",
      "1195 step d_loss1: 0.28740641474723816, d_loss2: 0.2819386422634125, g_loss: 10.201387405395508\n",
      "1196 step d_loss1: 0.495597779750824, d_loss2: 0.2850582003593445, g_loss: 7.874039649963379\n",
      "1197 step d_loss1: 0.3211546540260315, d_loss2: 0.2795715630054474, g_loss: 7.275873184204102\n",
      "1198 step d_loss1: 0.3227475881576538, d_loss2: 0.23543338477611542, g_loss: 6.97442102432251\n",
      "1199 step d_loss1: 0.4094124436378479, d_loss2: 0.37096765637397766, g_loss: 6.413430213928223\n",
      "1200 step d_loss1: 0.22069944441318512, d_loss2: 0.3472220301628113, g_loss: 7.449802398681641\n",
      "1201 step d_loss1: 0.5163100957870483, d_loss2: 0.3129792809486389, g_loss: 6.793440341949463\n",
      "1202 step d_loss1: 0.359905868768692, d_loss2: 0.2757245600223541, g_loss: 7.940895080566406\n",
      "1203 step d_loss1: 0.21439868211746216, d_loss2: 0.3817337453365326, g_loss: 7.995453834533691\n",
      "1204 step d_loss1: 0.3540162146091461, d_loss2: 0.33953213691711426, g_loss: 6.987208843231201\n",
      "1205 step d_loss1: 0.4299328327178955, d_loss2: 0.38936349749565125, g_loss: 7.378513813018799\n",
      "1206 step d_loss1: 0.3049255311489105, d_loss2: 0.2768712639808655, g_loss: 7.153487205505371\n",
      "1207 step d_loss1: 0.30164361000061035, d_loss2: 0.29217076301574707, g_loss: 7.940165042877197\n",
      "1208 step d_loss1: 0.2619771659374237, d_loss2: 0.47588300704956055, g_loss: 7.423279762268066\n",
      "1209 step d_loss1: 0.4006478190422058, d_loss2: 0.3122299909591675, g_loss: 7.494757652282715\n",
      "1210 step d_loss1: 0.444715678691864, d_loss2: 0.33270859718322754, g_loss: 6.204645156860352\n",
      "1211 step d_loss1: 0.410841166973114, d_loss2: 0.31314370036125183, g_loss: 7.5510125160217285\n",
      "1212 step d_loss1: 0.2743814289569855, d_loss2: 0.40619078278541565, g_loss: 7.4451093673706055\n",
      "1213 step d_loss1: 0.3258293569087982, d_loss2: 0.2789230942726135, g_loss: 7.794946193695068\n",
      "1214 step d_loss1: 0.3721797466278076, d_loss2: 0.3233051598072052, g_loss: 6.566046714782715\n",
      "1215 step d_loss1: 0.16322334110736847, d_loss2: 0.34091609716415405, g_loss: 8.990896224975586\n",
      "1216 step d_loss1: 0.19134175777435303, d_loss2: 0.3182399272918701, g_loss: 7.130125999450684\n",
      "1217 step d_loss1: 0.3472635746002197, d_loss2: 0.3681110143661499, g_loss: 6.352197170257568\n",
      "1218 step d_loss1: 0.44810765981674194, d_loss2: 0.3895930051803589, g_loss: 7.797412395477295\n",
      "1219 step d_loss1: 0.5032504200935364, d_loss2: 0.3440735936164856, g_loss: 6.038424491882324\n",
      "1220 step d_loss1: 0.31279557943344116, d_loss2: 0.35357770323753357, g_loss: 6.257598876953125\n",
      "1221 step d_loss1: 0.3015482723712921, d_loss2: 0.4590679407119751, g_loss: 7.862903594970703\n",
      "1222 step d_loss1: 0.2709721028804779, d_loss2: 0.26874902844429016, g_loss: 9.120477676391602\n",
      "1223 step d_loss1: 0.3811889588832855, d_loss2: 0.359884113073349, g_loss: 6.695649147033691\n",
      "1224 step d_loss1: 0.4781617522239685, d_loss2: 0.3359450697898865, g_loss: 6.687026023864746\n",
      "1225 step d_loss1: 0.2932985723018646, d_loss2: 0.347813218832016, g_loss: 7.716061592102051\n",
      "1226 step d_loss1: 0.35279691219329834, d_loss2: 0.36815521121025085, g_loss: 9.226922988891602\n",
      "1227 step d_loss1: 0.19669406116008759, d_loss2: 0.507283627986908, g_loss: 13.011995315551758\n",
      "1228 step d_loss1: 0.5621755123138428, d_loss2: 0.32090210914611816, g_loss: 8.513246536254883\n",
      "1229 step d_loss1: 0.34909820556640625, d_loss2: 0.34207627177238464, g_loss: 11.906936645507812\n",
      "1230 step d_loss1: 0.2639951705932617, d_loss2: 0.3553818166255951, g_loss: 18.894765853881836\n",
      "1231 step d_loss1: 0.5090242624282837, d_loss2: 0.30477389693260193, g_loss: 6.667308330535889\n",
      "1232 step d_loss1: 0.23186279833316803, d_loss2: 0.35955750942230225, g_loss: 7.558391571044922\n",
      "1233 step d_loss1: 0.3755496144294739, d_loss2: 0.371887743473053, g_loss: 10.934028625488281\n",
      "1234 step d_loss1: 0.4595831334590912, d_loss2: 0.3016168773174286, g_loss: 5.976377487182617\n",
      "1235 step d_loss1: 0.39084458351135254, d_loss2: 0.30953100323677063, g_loss: 6.830321788787842\n",
      "1236 step d_loss1: 0.26885589957237244, d_loss2: 0.36807867884635925, g_loss: 7.132495880126953\n",
      "1237 step d_loss1: 0.40268152952194214, d_loss2: 0.29168492555618286, g_loss: 8.400494575500488\n",
      "1238 step d_loss1: 0.3926888406276703, d_loss2: 0.3097164034843445, g_loss: 6.902515888214111\n",
      "1239 step d_loss1: 0.33798590302467346, d_loss2: 0.3391292989253998, g_loss: 7.0137715339660645\n",
      "1240 step d_loss1: 0.21219344437122345, d_loss2: 0.2921675145626068, g_loss: 7.969841957092285\n",
      "1241 step d_loss1: 0.3042391836643219, d_loss2: 0.3545958995819092, g_loss: 8.11546802520752\n",
      "1242 step d_loss1: 0.37243252992630005, d_loss2: 0.28983429074287415, g_loss: 6.94608736038208\n",
      "1243 step d_loss1: 0.20651336014270782, d_loss2: 0.3178870379924774, g_loss: 8.684757232666016\n",
      "1244 step d_loss1: 0.2500387132167816, d_loss2: 0.29994910955429077, g_loss: 9.093903541564941\n",
      "1245 step d_loss1: 0.19124260544776917, d_loss2: 0.3586042523384094, g_loss: 8.396827697753906\n",
      "1246 step d_loss1: 0.5056688189506531, d_loss2: 0.2738363742828369, g_loss: 7.544466972351074\n",
      "1247 step d_loss1: 0.3857780992984772, d_loss2: 0.3034265339374542, g_loss: 10.357345581054688\n",
      "1248 step d_loss1: 0.3786546587944031, d_loss2: 0.3287447690963745, g_loss: 6.4568328857421875\n",
      "1249 step d_loss1: 0.4789822995662689, d_loss2: 0.2767614722251892, g_loss: 7.83721399307251\n",
      "1250 step d_loss1: 0.1943475902080536, d_loss2: 0.3396119177341461, g_loss: 8.720672607421875\n",
      "1251 step d_loss1: 0.3739580512046814, d_loss2: 0.2898651361465454, g_loss: 8.904010772705078\n",
      "1252 step d_loss1: 0.24325722455978394, d_loss2: 0.38833433389663696, g_loss: 6.205114364624023\n",
      "1253 step d_loss1: 0.2056289166212082, d_loss2: 0.3392697870731354, g_loss: 8.041483879089355\n",
      "1254 step d_loss1: 0.4149984121322632, d_loss2: 0.3507249653339386, g_loss: 7.465962886810303\n",
      "1255 step d_loss1: 0.4409589469432831, d_loss2: 0.2756982147693634, g_loss: 6.246659278869629\n",
      "1256 step d_loss1: 0.17939645051956177, d_loss2: 0.4326704740524292, g_loss: 17.31769561767578\n",
      "1257 step d_loss1: 0.12085749953985214, d_loss2: 0.5401528477668762, g_loss: 12.239744186401367\n",
      "1258 step d_loss1: 0.4946526288986206, d_loss2: 0.2753543257713318, g_loss: 6.863568305969238\n",
      "1259 step d_loss1: 0.4273952841758728, d_loss2: 0.32553961873054504, g_loss: 6.771834373474121\n",
      "1260 step d_loss1: 0.40347498655319214, d_loss2: 0.4196956157684326, g_loss: 7.657907485961914\n",
      "1261 step d_loss1: 0.36916211247444153, d_loss2: 0.33723118901252747, g_loss: 6.3058366775512695\n",
      "1262 step d_loss1: 0.3495565354824066, d_loss2: 0.3254914879798889, g_loss: 6.113806247711182\n",
      "1263 step d_loss1: 0.4343683421611786, d_loss2: 0.32607465982437134, g_loss: 6.152505874633789\n",
      "1264 step d_loss1: 0.29422733187675476, d_loss2: 0.38296517729759216, g_loss: 5.835750579833984\n",
      "1265 step d_loss1: 0.3918410837650299, d_loss2: 0.39173945784568787, g_loss: 7.605393409729004\n",
      "1266 step d_loss1: 0.3303849399089813, d_loss2: 0.33379197120666504, g_loss: 5.6436872482299805\n",
      "1267 step d_loss1: 0.23977792263031006, d_loss2: 0.38123244047164917, g_loss: 8.145898818969727\n",
      "1268 step d_loss1: 0.27230605483055115, d_loss2: 0.3389654755592346, g_loss: 8.990072250366211\n",
      "1269 step d_loss1: 0.5101215243339539, d_loss2: 0.3009938895702362, g_loss: 6.080361843109131\n",
      "1270 step d_loss1: 0.4153006076812744, d_loss2: 0.41614392399787903, g_loss: 8.152083396911621\n",
      "1271 step d_loss1: 0.33451974391937256, d_loss2: 0.3196962773799896, g_loss: 6.847223281860352\n",
      "1272 step d_loss1: 0.24443700909614563, d_loss2: 0.26950788497924805, g_loss: 8.516534805297852\n",
      "1273 step d_loss1: 0.28691548109054565, d_loss2: 0.4352661073207855, g_loss: 8.742929458618164\n",
      "1274 step d_loss1: 0.1909741908311844, d_loss2: 0.43573591113090515, g_loss: 7.789147853851318\n",
      "1275 step d_loss1: 0.3526063561439514, d_loss2: 0.30939534306526184, g_loss: 7.367450714111328\n",
      "1276 step d_loss1: 0.34455662965774536, d_loss2: 0.41300490498542786, g_loss: 8.12739372253418\n",
      "1277 step d_loss1: 0.40694618225097656, d_loss2: 0.43087905645370483, g_loss: 6.370354175567627\n",
      "1278 step d_loss1: 0.28188344836235046, d_loss2: 0.3257988393306732, g_loss: 9.361539840698242\n",
      "1279 step d_loss1: 0.5362454056739807, d_loss2: 0.31022578477859497, g_loss: 6.542503356933594\n",
      "1280 step d_loss1: 0.4398180842399597, d_loss2: 0.3388786315917969, g_loss: 6.759853839874268\n",
      "1281 step d_loss1: 0.33096686005592346, d_loss2: 0.3239145874977112, g_loss: 7.034122467041016\n",
      "1282 step d_loss1: 0.19832123816013336, d_loss2: 0.4197685718536377, g_loss: 9.07242202758789\n",
      "1283 step d_loss1: 0.5160025358200073, d_loss2: 0.27520063519477844, g_loss: 9.96333122253418\n",
      "1284 step d_loss1: 0.1495567113161087, d_loss2: 0.36440083384513855, g_loss: 10.122182846069336\n",
      "1285 step d_loss1: 0.41936182975769043, d_loss2: 0.3520883023738861, g_loss: 10.209826469421387\n",
      "1286 step d_loss1: 0.4420135021209717, d_loss2: 0.2682209014892578, g_loss: 6.653844833374023\n",
      "1287 step d_loss1: 0.3668598234653473, d_loss2: 0.30745917558670044, g_loss: 6.770848751068115\n",
      "1288 step d_loss1: 0.19517061114311218, d_loss2: 0.337209552526474, g_loss: 13.154918670654297\n",
      "1289 step d_loss1: 0.26519620418548584, d_loss2: 0.2758473753929138, g_loss: 7.952063083648682\n",
      "1290 step d_loss1: 0.1531287431716919, d_loss2: 0.37635543942451477, g_loss: 10.463773727416992\n",
      "1291 step d_loss1: 0.5183187127113342, d_loss2: 0.22313269972801208, g_loss: 6.694155693054199\n",
      "1292 step d_loss1: 0.2067677080631256, d_loss2: 0.3543028235435486, g_loss: 8.104269981384277\n",
      "1293 step d_loss1: 0.3164089024066925, d_loss2: 0.30148056149482727, g_loss: 7.995877265930176\n",
      "1294 step d_loss1: 0.10785119235515594, d_loss2: 0.355379194021225, g_loss: 10.078513145446777\n",
      "1295 step d_loss1: 0.6335541009902954, d_loss2: 0.2948290705680847, g_loss: 6.834710121154785\n",
      "1296 step d_loss1: 0.4300595819950104, d_loss2: 0.3396749794483185, g_loss: 7.613317012786865\n",
      "1297 step d_loss1: 0.24934770166873932, d_loss2: 0.37147438526153564, g_loss: 14.748451232910156\n",
      "1298 step d_loss1: 0.27908939123153687, d_loss2: 0.28646320104599, g_loss: 7.9564690589904785\n",
      "1299 step d_loss1: 0.39110684394836426, d_loss2: 0.39824748039245605, g_loss: 5.862469673156738\n",
      "1300 step d_loss1: 0.5307282209396362, d_loss2: 0.35634633898735046, g_loss: 9.136027336120605\n",
      "1301 step d_loss1: 0.4653829038143158, d_loss2: 0.3577916622161865, g_loss: 6.2482590675354\n",
      "1302 step d_loss1: 0.3700733482837677, d_loss2: 0.31781747937202454, g_loss: 7.698716640472412\n",
      "1303 step d_loss1: 0.33320194482803345, d_loss2: 0.40564072132110596, g_loss: 6.80206298828125\n",
      "1304 step d_loss1: 0.3893679678440094, d_loss2: 0.3185644745826721, g_loss: 7.361583709716797\n",
      "1305 step d_loss1: 0.3252515196800232, d_loss2: 0.33751633763313293, g_loss: 6.879147529602051\n",
      "1306 step d_loss1: 0.31711500883102417, d_loss2: 0.36462992429733276, g_loss: 7.271574974060059\n",
      "1307 step d_loss1: 0.366163432598114, d_loss2: 0.30108800530433655, g_loss: 6.759249687194824\n",
      "1308 step d_loss1: 0.22377325594425201, d_loss2: 0.2687374949455261, g_loss: 6.384121417999268\n",
      "1309 step d_loss1: 0.38017737865448, d_loss2: 0.414231538772583, g_loss: 6.061131954193115\n",
      "1310 step d_loss1: 0.3603023886680603, d_loss2: 0.3385235667228699, g_loss: 8.254895210266113\n",
      "1311 step d_loss1: 0.1832326054573059, d_loss2: 0.30839353799819946, g_loss: 9.918020248413086\n",
      "1312 step d_loss1: 0.3192281723022461, d_loss2: 0.2941542863845825, g_loss: 6.525397300720215\n",
      "1313 step d_loss1: 0.18240220844745636, d_loss2: 0.2938944697380066, g_loss: 7.780888557434082\n",
      "1314 step d_loss1: 0.2591754198074341, d_loss2: 0.3704744279384613, g_loss: 6.828547477722168\n",
      "1315 step d_loss1: 0.494737446308136, d_loss2: 0.32792794704437256, g_loss: 6.5710906982421875\n",
      "1316 step d_loss1: 0.3632204830646515, d_loss2: 0.29379624128341675, g_loss: 7.776527404785156\n",
      "1317 step d_loss1: 0.3356791138648987, d_loss2: 0.2768617272377014, g_loss: 7.496698379516602\n",
      "1318 step d_loss1: 0.29878100752830505, d_loss2: 0.33450794219970703, g_loss: 6.636181831359863\n",
      "1319 step d_loss1: 0.40076199173927307, d_loss2: 0.2583494782447815, g_loss: 6.95758056640625\n",
      "1320 step d_loss1: 0.16370177268981934, d_loss2: 0.39110687375068665, g_loss: 7.723340034484863\n",
      "1321 step d_loss1: 0.26223641633987427, d_loss2: 0.3669353425502777, g_loss: 6.3926777839660645\n",
      "1322 step d_loss1: 0.30433085560798645, d_loss2: 0.4043901562690735, g_loss: 8.644577980041504\n",
      "1323 step d_loss1: 0.4286035895347595, d_loss2: 0.3513232469558716, g_loss: 8.656365394592285\n",
      "1324 step d_loss1: 0.3522966206073761, d_loss2: 0.2956467568874359, g_loss: 8.087451934814453\n",
      "1325 step d_loss1: 0.3654828667640686, d_loss2: 0.2917407155036926, g_loss: 6.982579708099365\n",
      "1326 step d_loss1: 0.24604935944080353, d_loss2: 0.44907015562057495, g_loss: 8.629504203796387\n",
      "1327 step d_loss1: 0.21854135394096375, d_loss2: 0.28585195541381836, g_loss: 8.181272506713867\n",
      "1328 step d_loss1: 0.2465147227048874, d_loss2: 0.328853964805603, g_loss: 8.138230323791504\n",
      "1329 step d_loss1: 0.3276714086532593, d_loss2: 0.2803718149662018, g_loss: 8.234518051147461\n",
      "1330 step d_loss1: 0.35618215799331665, d_loss2: 0.36240291595458984, g_loss: 5.80716609954834\n",
      "1331 step d_loss1: 0.5996264815330505, d_loss2: 0.29682379961013794, g_loss: 6.9476213455200195\n",
      "1332 step d_loss1: 0.2529897689819336, d_loss2: 0.5595918893814087, g_loss: 11.568995475769043\n",
      "1333 step d_loss1: 0.4671333134174347, d_loss2: 0.3475125730037689, g_loss: 8.825186729431152\n",
      "1334 step d_loss1: 0.3957042396068573, d_loss2: 0.3963717818260193, g_loss: 8.612921714782715\n",
      "1335 step d_loss1: 0.23554712533950806, d_loss2: 0.34875839948654175, g_loss: 16.365793228149414\n",
      "1336 step d_loss1: 0.39738163352012634, d_loss2: 0.3105132281780243, g_loss: 7.871641635894775\n",
      "1337 step d_loss1: 0.3399491608142853, d_loss2: 0.5614830255508423, g_loss: 5.550457000732422\n",
      "1338 step d_loss1: 0.48849251866340637, d_loss2: 0.3377300500869751, g_loss: 8.115809440612793\n",
      "1339 step d_loss1: 0.13810347020626068, d_loss2: 0.41612908244132996, g_loss: 20.945005416870117\n",
      "1340 step d_loss1: 0.31200283765792847, d_loss2: 0.32910865545272827, g_loss: 11.500310897827148\n",
      "1341 step d_loss1: 0.45899391174316406, d_loss2: 0.3210664391517639, g_loss: 8.273357391357422\n",
      "1342 step d_loss1: 0.407867431640625, d_loss2: 0.3814895749092102, g_loss: 5.236649513244629\n",
      "1343 step d_loss1: 0.2776032090187073, d_loss2: 0.28584814071655273, g_loss: 14.946017265319824\n",
      "1344 step d_loss1: 0.18333496153354645, d_loss2: 0.49419528245925903, g_loss: 12.24259090423584\n",
      "1345 step d_loss1: 0.14798970520496368, d_loss2: 0.3885798752307892, g_loss: 10.900225639343262\n",
      "1346 step d_loss1: 0.5480551719665527, d_loss2: 0.270666241645813, g_loss: 7.13155460357666\n",
      "1347 step d_loss1: 0.525816023349762, d_loss2: 0.3488401174545288, g_loss: 5.29440975189209\n",
      "1348 step d_loss1: 0.46267032623291016, d_loss2: 0.2948892116546631, g_loss: 7.216917991638184\n",
      "1349 step d_loss1: 0.39677560329437256, d_loss2: 0.3793279528617859, g_loss: 7.88577127456665\n",
      "1350 step d_loss1: 0.4128110706806183, d_loss2: 0.333102285861969, g_loss: 7.446789264678955\n",
      "1351 step d_loss1: 0.3544350862503052, d_loss2: 0.38066014647483826, g_loss: 7.791256904602051\n",
      "1352 step d_loss1: 0.32643595337867737, d_loss2: 0.3308117687702179, g_loss: 6.727630615234375\n",
      "1353 step d_loss1: 0.3762156367301941, d_loss2: 0.3062756359577179, g_loss: 5.823896884918213\n",
      "1354 step d_loss1: 0.28945353627204895, d_loss2: 0.3046620786190033, g_loss: 7.437539577484131\n",
      "1355 step d_loss1: 0.22117501497268677, d_loss2: 0.32939547300338745, g_loss: 13.055075645446777\n",
      "1356 step d_loss1: 0.2009003758430481, d_loss2: 0.4337517023086548, g_loss: 18.542221069335938\n",
      "1357 step d_loss1: 0.5027632713317871, d_loss2: 0.26213538646698, g_loss: 6.059835433959961\n",
      "1358 step d_loss1: 0.3234390616416931, d_loss2: 0.3971938192844391, g_loss: 7.880399703979492\n",
      "1359 step d_loss1: 0.3856745660305023, d_loss2: 0.2982204556465149, g_loss: 6.239668846130371\n",
      "1360 step d_loss1: 0.25137659907341003, d_loss2: 0.31114816665649414, g_loss: 7.7849812507629395\n",
      "1361 step d_loss1: 0.3262823224067688, d_loss2: 0.32376882433891296, g_loss: 7.323543071746826\n",
      "1362 step d_loss1: 0.29543009400367737, d_loss2: 0.3204817771911621, g_loss: 6.365592002868652\n",
      "1363 step d_loss1: 0.21594996750354767, d_loss2: 0.31001192331314087, g_loss: 7.820219039916992\n",
      "1364 step d_loss1: 0.3570481538772583, d_loss2: 0.30562257766723633, g_loss: 4.932339191436768\n",
      "1365 step d_loss1: 0.28102296590805054, d_loss2: 0.23856693506240845, g_loss: 6.874260425567627\n",
      "1366 step d_loss1: 0.1974988877773285, d_loss2: 0.36097976565361023, g_loss: 7.567849159240723\n",
      "1367 step d_loss1: 0.17643257975578308, d_loss2: 0.366486132144928, g_loss: 16.18198585510254\n",
      "1368 step d_loss1: 0.24180737137794495, d_loss2: 0.27479514479637146, g_loss: 8.176220893859863\n",
      "1369 step d_loss1: 0.3987281620502472, d_loss2: 0.3788195550441742, g_loss: 7.036797046661377\n",
      "1370 step d_loss1: 0.2476734220981598, d_loss2: 0.3375991880893707, g_loss: 6.145114898681641\n",
      "1371 step d_loss1: 0.529984176158905, d_loss2: 0.3196178376674652, g_loss: 6.093525409698486\n",
      "1372 step d_loss1: 0.2448464334011078, d_loss2: 0.3683427572250366, g_loss: 8.09075927734375\n",
      "1373 step d_loss1: 0.3651098906993866, d_loss2: 0.5347567796707153, g_loss: 4.733784198760986\n",
      "1374 step d_loss1: 0.28512072563171387, d_loss2: 0.2713821530342102, g_loss: 8.358598709106445\n",
      "1375 step d_loss1: 0.2889081537723541, d_loss2: 0.3360174894332886, g_loss: 8.147441864013672\n",
      "1376 step d_loss1: 0.13979311287403107, d_loss2: 0.3270828425884247, g_loss: 8.912667274475098\n",
      "1377 step d_loss1: 0.4276241660118103, d_loss2: 0.25029924511909485, g_loss: 7.232766628265381\n",
      "1378 step d_loss1: 0.504345178604126, d_loss2: 0.29255300760269165, g_loss: 6.826566696166992\n",
      "1379 step d_loss1: 0.42443376779556274, d_loss2: 0.283107727766037, g_loss: 6.4309234619140625\n",
      "1380 step d_loss1: 0.26645225286483765, d_loss2: 0.3873814642429352, g_loss: 6.913697242736816\n",
      "1381 step d_loss1: 0.16724734008312225, d_loss2: 0.6355277299880981, g_loss: 12.091678619384766\n",
      "1382 step d_loss1: 0.5511521697044373, d_loss2: 0.33138319849967957, g_loss: 7.784122943878174\n",
      "1383 step d_loss1: 0.29361793398857117, d_loss2: 0.3273427188396454, g_loss: 8.096275329589844\n",
      "1384 step d_loss1: 0.3159988522529602, d_loss2: 0.4851572513580322, g_loss: 24.220266342163086\n",
      "1385 step d_loss1: 0.2814394235610962, d_loss2: 0.4107574224472046, g_loss: 16.835384368896484\n",
      "1386 step d_loss1: 0.5590594410896301, d_loss2: 0.35153084993362427, g_loss: 7.415744304656982\n",
      "1387 step d_loss1: 0.5555833578109741, d_loss2: 0.31534749269485474, g_loss: 7.9663238525390625\n",
      "1388 step d_loss1: 0.38361215591430664, d_loss2: 0.3361768126487732, g_loss: 6.523157119750977\n",
      "1389 step d_loss1: 0.3269343674182892, d_loss2: 0.38834816217422485, g_loss: 6.61455774307251\n",
      "1390 step d_loss1: 0.2731751799583435, d_loss2: 0.2946735620498657, g_loss: 12.364479064941406\n",
      "1391 step d_loss1: 0.41589364409446716, d_loss2: 0.3747972249984741, g_loss: 5.385049343109131\n",
      "1392 step d_loss1: 0.43420302867889404, d_loss2: 0.31225916743278503, g_loss: 7.207198619842529\n",
      "1393 step d_loss1: 0.34218087792396545, d_loss2: 0.3305501937866211, g_loss: 7.094650745391846\n",
      "1394 step d_loss1: 0.35771679878234863, d_loss2: 0.3998985290527344, g_loss: 7.632151126861572\n",
      "1395 step d_loss1: 0.14085525274276733, d_loss2: 0.3773525059223175, g_loss: 8.371501922607422\n",
      "1396 step d_loss1: 0.3678908050060272, d_loss2: 0.2643863260746002, g_loss: 6.13669490814209\n",
      "1397 step d_loss1: 0.39226052165031433, d_loss2: 0.27491775155067444, g_loss: 7.1074538230896\n",
      "1398 step d_loss1: 0.3666226863861084, d_loss2: 0.3405393362045288, g_loss: 9.741625785827637\n",
      "1399 step d_loss1: 0.3909880518913269, d_loss2: 0.3028642535209656, g_loss: 7.104674816131592\n",
      "1400 step d_loss1: 0.2924403250217438, d_loss2: 0.42854392528533936, g_loss: 6.633070468902588\n",
      "1401 step d_loss1: 0.24199408292770386, d_loss2: 0.3612155616283417, g_loss: 7.884947299957275\n",
      "1402 step d_loss1: 0.2868892550468445, d_loss2: 0.3232933282852173, g_loss: 6.707694053649902\n",
      "1403 step d_loss1: 0.31706732511520386, d_loss2: 0.34605222940444946, g_loss: 5.9258012771606445\n",
      "1404 step d_loss1: 0.24531333148479462, d_loss2: 0.4293070137500763, g_loss: 5.599196434020996\n",
      "1405 step d_loss1: 0.26580747961997986, d_loss2: 0.2401200830936432, g_loss: 8.087244987487793\n",
      "1406 step d_loss1: 0.15713641047477722, d_loss2: 0.27545300126075745, g_loss: 9.968903541564941\n",
      "1407 step d_loss1: 0.26765376329421997, d_loss2: 0.25516676902770996, g_loss: 10.450226783752441\n",
      "1408 step d_loss1: 0.3588491380214691, d_loss2: 0.286356121301651, g_loss: 7.5540947914123535\n",
      "1409 step d_loss1: 0.5724129676818848, d_loss2: 0.3119218945503235, g_loss: 6.559135913848877\n",
      "1410 step d_loss1: 0.3039073050022125, d_loss2: 0.3467974066734314, g_loss: 7.674332141876221\n",
      "1411 step d_loss1: 0.5357869863510132, d_loss2: 0.3753854036331177, g_loss: 5.460665225982666\n",
      "1412 step d_loss1: 0.3270961344242096, d_loss2: 0.49457669258117676, g_loss: 9.821310043334961\n",
      "1413 step d_loss1: 0.46157005429267883, d_loss2: 0.3463529944419861, g_loss: 9.873449325561523\n",
      "1414 step d_loss1: 0.41352760791778564, d_loss2: 0.35028403997421265, g_loss: 7.158932209014893\n",
      "1415 step d_loss1: 0.3428992033004761, d_loss2: 0.36366137862205505, g_loss: 7.570075035095215\n",
      "1416 step d_loss1: 0.3966827988624573, d_loss2: 0.32639971375465393, g_loss: 7.963039875030518\n",
      "1417 step d_loss1: 0.34979352355003357, d_loss2: 0.5227629542350769, g_loss: 4.297694206237793\n",
      "1418 step d_loss1: 0.3965051770210266, d_loss2: 0.27726805210113525, g_loss: 7.0616888999938965\n",
      "1419 step d_loss1: 0.28390398621559143, d_loss2: 0.38059887290000916, g_loss: 5.708934783935547\n",
      "1420 step d_loss1: 0.2098224014043808, d_loss2: 0.3447166979312897, g_loss: 8.37318229675293\n",
      "1421 step d_loss1: 0.38881945610046387, d_loss2: 0.33001333475112915, g_loss: 6.014233589172363\n",
      "1422 step d_loss1: 0.4745810925960541, d_loss2: 0.31390875577926636, g_loss: 5.932413578033447\n",
      "1423 step d_loss1: 0.22340445220470428, d_loss2: 0.3407176733016968, g_loss: 8.029524803161621\n",
      "1424 step d_loss1: 0.33246928453445435, d_loss2: 0.2840399444103241, g_loss: 5.687083721160889\n",
      "1425 step d_loss1: 0.38848602771759033, d_loss2: 0.3336067199707031, g_loss: 9.45556354522705\n",
      "1426 step d_loss1: 0.37796875834465027, d_loss2: 0.4030238389968872, g_loss: 6.721431255340576\n",
      "1427 step d_loss1: 0.27227580547332764, d_loss2: 0.37544167041778564, g_loss: 7.106132984161377\n",
      "1428 step d_loss1: 0.3040226399898529, d_loss2: 0.28152063488960266, g_loss: 6.5076189041137695\n",
      "1429 step d_loss1: 0.2213442325592041, d_loss2: 0.32692191004753113, g_loss: 8.4733304977417\n",
      "1430 step d_loss1: 0.43418073654174805, d_loss2: 0.2771630585193634, g_loss: 5.728507995605469\n",
      "1431 step d_loss1: 0.2615925073623657, d_loss2: 0.4603886902332306, g_loss: 6.2355852127075195\n",
      "1432 step d_loss1: 0.3914199769496918, d_loss2: 0.3528086543083191, g_loss: 6.475003242492676\n",
      "1433 step d_loss1: 0.4116276800632477, d_loss2: 0.3701733946800232, g_loss: 5.39821720123291\n",
      "1434 step d_loss1: 0.2668566405773163, d_loss2: 0.3275867998600006, g_loss: 6.489745140075684\n",
      "1435 step d_loss1: 0.2637048065662384, d_loss2: 0.27158689498901367, g_loss: 6.291012763977051\n",
      "1436 step d_loss1: 0.14200617372989655, d_loss2: 0.3858984410762787, g_loss: 10.505990028381348\n",
      "1437 step d_loss1: 0.44533252716064453, d_loss2: 0.40091651678085327, g_loss: 8.420909881591797\n",
      "1438 step d_loss1: 0.30154410004615784, d_loss2: 0.533096969127655, g_loss: 18.626083374023438\n",
      "1439 step d_loss1: 0.32862845063209534, d_loss2: 0.41681843996047974, g_loss: 6.9322381019592285\n",
      "1440 step d_loss1: 0.3735915720462799, d_loss2: 0.3059423565864563, g_loss: 9.324378967285156\n",
      "1441 step d_loss1: 0.3315916657447815, d_loss2: 0.32775717973709106, g_loss: 7.651625156402588\n",
      "1442 step d_loss1: 0.35883504152297974, d_loss2: 0.277802973985672, g_loss: 7.126784324645996\n",
      "1443 step d_loss1: 0.2601311206817627, d_loss2: 0.4116120934486389, g_loss: 9.618345260620117\n",
      "1444 step d_loss1: 0.32035112380981445, d_loss2: 0.42540228366851807, g_loss: 7.59458065032959\n",
      "1445 step d_loss1: 0.5384820699691772, d_loss2: 0.253650039434433, g_loss: 7.533899307250977\n",
      "1446 step d_loss1: 0.4891420006752014, d_loss2: 0.3361082971096039, g_loss: 6.770026206970215\n",
      "1447 step d_loss1: 0.36975669860839844, d_loss2: 0.26300132274627686, g_loss: 6.9301252365112305\n",
      "1448 step d_loss1: 0.37273481488227844, d_loss2: 0.39049407839775085, g_loss: 5.614398002624512\n",
      "1449 step d_loss1: 0.21986249089241028, d_loss2: 0.27837851643562317, g_loss: 7.782804489135742\n",
      "1450 step d_loss1: 0.2025427222251892, d_loss2: 0.4055250287055969, g_loss: 10.835838317871094\n",
      "1451 step d_loss1: 0.32449835538864136, d_loss2: 0.3117477297782898, g_loss: 8.739761352539062\n",
      "1452 step d_loss1: 0.30945712327957153, d_loss2: 0.22734612226486206, g_loss: 6.830673694610596\n",
      "1453 step d_loss1: 0.40733572840690613, d_loss2: 0.33167174458503723, g_loss: 6.595620155334473\n",
      "1454 step d_loss1: 0.16927242279052734, d_loss2: 0.39118483662605286, g_loss: 13.638409614562988\n",
      "1455 step d_loss1: 0.19424167275428772, d_loss2: 0.2592924237251282, g_loss: 13.30885124206543\n",
      "1456 step d_loss1: 0.3590408265590668, d_loss2: 0.35171449184417725, g_loss: 5.945818901062012\n",
      "1457 step d_loss1: 0.3986281752586365, d_loss2: 0.3066239655017853, g_loss: 6.634452819824219\n",
      "1458 step d_loss1: 0.21423141658306122, d_loss2: 0.2897791564464569, g_loss: 9.148954391479492\n",
      "1459 step d_loss1: 0.15762673318386078, d_loss2: 0.32506263256073, g_loss: 10.320897102355957\n",
      "1460 step d_loss1: 0.2866501808166504, d_loss2: 0.2415383756160736, g_loss: 7.021360874176025\n",
      "1461 step d_loss1: 0.4798603057861328, d_loss2: 0.2470489740371704, g_loss: 7.559131622314453\n",
      "1462 step d_loss1: 0.19464148581027985, d_loss2: 0.3606608211994171, g_loss: 8.45197582244873\n",
      "1463 step d_loss1: 0.4172511696815491, d_loss2: 0.37255075573921204, g_loss: 8.945186614990234\n",
      "1464 step d_loss1: 0.2968449294567108, d_loss2: 0.28200995922088623, g_loss: 6.630552291870117\n",
      "1465 step d_loss1: 0.2741082012653351, d_loss2: 0.406830370426178, g_loss: 12.948725700378418\n",
      "1466 step d_loss1: 0.37706321477890015, d_loss2: 0.23849627375602722, g_loss: 8.213128089904785\n",
      "1467 step d_loss1: 0.26299843192100525, d_loss2: 0.483831524848938, g_loss: 12.290916442871094\n",
      "1468 step d_loss1: 0.367399126291275, d_loss2: 0.5549397468566895, g_loss: 6.823303699493408\n",
      "1469 step d_loss1: 0.40976038575172424, d_loss2: 0.30058127641677856, g_loss: 7.5582709312438965\n",
      "1470 step d_loss1: 0.48767220973968506, d_loss2: 0.2993641495704651, g_loss: 7.067441463470459\n",
      "1471 step d_loss1: 0.3095071613788605, d_loss2: 0.28932270407676697, g_loss: 16.404741287231445\n",
      "1472 step d_loss1: 0.27113738656044006, d_loss2: 0.3943514823913574, g_loss: 6.918643951416016\n",
      "1473 step d_loss1: 0.32394570112228394, d_loss2: 0.3524266481399536, g_loss: 6.148202896118164\n",
      "1474 step d_loss1: 0.445186585187912, d_loss2: 0.3214397132396698, g_loss: 6.728000640869141\n",
      "1475 step d_loss1: 0.48309779167175293, d_loss2: 0.2881004810333252, g_loss: 6.489898204803467\n",
      "1476 step d_loss1: 0.17065048217773438, d_loss2: 0.46394243836402893, g_loss: 15.538192749023438\n",
      "1477 step d_loss1: 0.4263249635696411, d_loss2: 0.3684963285923004, g_loss: 6.871904373168945\n",
      "1478 step d_loss1: 0.33103033900260925, d_loss2: 0.40059444308280945, g_loss: 6.361062049865723\n",
      "1479 step d_loss1: 0.3905874490737915, d_loss2: 0.3172120153903961, g_loss: 9.117778778076172\n",
      "1480 step d_loss1: 0.28415805101394653, d_loss2: 0.2878378629684448, g_loss: 12.981269836425781\n",
      "1481 step d_loss1: 0.27676600217819214, d_loss2: 0.321405291557312, g_loss: 8.802093505859375\n",
      "1482 step d_loss1: 0.44436684250831604, d_loss2: 0.24453262984752655, g_loss: 7.206549644470215\n",
      "1483 step d_loss1: 0.22691377997398376, d_loss2: 0.31557199358940125, g_loss: 7.948741436004639\n",
      "1484 step d_loss1: 0.25946593284606934, d_loss2: 0.37517237663269043, g_loss: 8.543267250061035\n",
      "1485 step d_loss1: 0.364435613155365, d_loss2: 0.2882440388202667, g_loss: 7.455806732177734\n",
      "1486 step d_loss1: 0.20967867970466614, d_loss2: 0.28113415837287903, g_loss: 8.11346435546875\n",
      "1487 step d_loss1: 0.27684834599494934, d_loss2: 0.48551231622695923, g_loss: 5.106167793273926\n",
      "1488 step d_loss1: 0.42224082350730896, d_loss2: 0.2668897807598114, g_loss: 6.723520278930664\n",
      "1489 step d_loss1: 0.5901496410369873, d_loss2: 0.2627828121185303, g_loss: 6.0528244972229\n",
      "1490 step d_loss1: 0.347240686416626, d_loss2: 0.3651348948478699, g_loss: 5.619986534118652\n",
      "1491 step d_loss1: 0.23564204573631287, d_loss2: 0.4340483546257019, g_loss: 8.41234302520752\n",
      "1492 step d_loss1: 0.39227867126464844, d_loss2: 0.28671279549598694, g_loss: 5.875781059265137\n",
      "1493 step d_loss1: 0.4140368700027466, d_loss2: 0.3046278953552246, g_loss: 5.997945785522461\n",
      "1494 step d_loss1: 0.29446375370025635, d_loss2: 0.3593500852584839, g_loss: 7.939517498016357\n",
      "1495 step d_loss1: 0.30135229229927063, d_loss2: 0.3978128433227539, g_loss: 7.657712936401367\n",
      "1496 step d_loss1: 0.3114011883735657, d_loss2: 0.39804407954216003, g_loss: 6.855168342590332\n",
      "1497 step d_loss1: 0.1746637523174286, d_loss2: 0.3438933491706848, g_loss: 9.007333755493164\n",
      "1498 step d_loss1: 0.5875442624092102, d_loss2: 0.2721042335033417, g_loss: 5.726053714752197\n",
      "1499 step d_loss1: 0.40229082107543945, d_loss2: 0.2928670644760132, g_loss: 7.4227294921875\n",
      "1500 step d_loss1: 0.2575356662273407, d_loss2: 0.3627281188964844, g_loss: 7.636566638946533\n",
      "1501 step d_loss1: 0.3229248523712158, d_loss2: 0.3127305209636688, g_loss: 7.17905330657959\n",
      "1502 step d_loss1: 0.38122567534446716, d_loss2: 0.31501248478889465, g_loss: 6.530741214752197\n",
      "1503 step d_loss1: 0.3631272614002228, d_loss2: 0.4444456100463867, g_loss: 6.026005268096924\n",
      "1504 step d_loss1: 0.32422882318496704, d_loss2: 0.5638456344604492, g_loss: 5.561377048492432\n",
      "1505 step d_loss1: 0.3119864761829376, d_loss2: 0.29761403799057007, g_loss: 8.27736759185791\n",
      "1506 step d_loss1: 0.2917264699935913, d_loss2: 0.3242459297180176, g_loss: 9.837438583374023\n",
      "1507 step d_loss1: 0.19137442111968994, d_loss2: 0.2380344271659851, g_loss: 14.069490432739258\n",
      "1508 step d_loss1: 0.24944868683815002, d_loss2: 0.3625468611717224, g_loss: 8.107218742370605\n",
      "1509 step d_loss1: 0.4616619348526001, d_loss2: 0.361769437789917, g_loss: 7.075835704803467\n",
      "1510 step d_loss1: 0.55966717004776, d_loss2: 0.28131139278411865, g_loss: 8.124611854553223\n",
      "1511 step d_loss1: 0.24679310619831085, d_loss2: 0.33707624673843384, g_loss: 15.78186321258545\n",
      "1512 step d_loss1: 0.2984873950481415, d_loss2: 0.28799378871917725, g_loss: 7.957335472106934\n",
      "1513 step d_loss1: 0.32226288318634033, d_loss2: 0.30782341957092285, g_loss: 7.663320064544678\n",
      "1514 step d_loss1: 0.2276700884103775, d_loss2: 0.4448575973510742, g_loss: 8.656777381896973\n",
      "1515 step d_loss1: 0.22198264300823212, d_loss2: 0.39413982629776, g_loss: 7.414989948272705\n",
      "1516 step d_loss1: 0.5304562449455261, d_loss2: 0.32766979932785034, g_loss: 6.488852500915527\n",
      "1517 step d_loss1: 0.45723438262939453, d_loss2: 0.3268229067325592, g_loss: 6.831515312194824\n",
      "1518 step d_loss1: 0.39378175139427185, d_loss2: 0.32269853353500366, g_loss: 5.5121283531188965\n",
      "1519 step d_loss1: 0.1537528932094574, d_loss2: 0.31624120473861694, g_loss: 7.588103294372559\n",
      "1520 step d_loss1: 0.4386941194534302, d_loss2: 0.2909347712993622, g_loss: 7.098443031311035\n",
      "1521 step d_loss1: 0.16464996337890625, d_loss2: 0.2797848880290985, g_loss: 13.526479721069336\n",
      "1522 step d_loss1: 0.2497321367263794, d_loss2: 0.33654895424842834, g_loss: 6.908792972564697\n",
      "1523 step d_loss1: 0.4022136628627777, d_loss2: 0.46656879782676697, g_loss: 6.567074298858643\n",
      "1524 step d_loss1: 0.39317625761032104, d_loss2: 0.33700400590896606, g_loss: 5.971184730529785\n",
      "1525 step d_loss1: 0.2642870843410492, d_loss2: 0.31654560565948486, g_loss: 7.08621072769165\n",
      "1526 step d_loss1: 0.19186824560165405, d_loss2: 0.33387652039527893, g_loss: 8.608953475952148\n",
      "1527 step d_loss1: 0.5039876699447632, d_loss2: 0.2746752202510834, g_loss: 6.822660446166992\n",
      "1528 step d_loss1: 0.4446764886379242, d_loss2: 0.35482022166252136, g_loss: 7.407601356506348\n",
      "1529 step d_loss1: 0.14576348662376404, d_loss2: 0.6398537755012512, g_loss: 12.43526554107666\n",
      "1530 step d_loss1: 0.34151995182037354, d_loss2: 0.3519412577152252, g_loss: 6.58844518661499\n",
      "1531 step d_loss1: 0.3542214035987854, d_loss2: 0.35894426703453064, g_loss: 6.779851913452148\n",
      "1532 step d_loss1: 0.2728366255760193, d_loss2: 0.41859114170074463, g_loss: 6.087197303771973\n",
      "1533 step d_loss1: 0.3078954517841339, d_loss2: 0.45097142457962036, g_loss: 8.387492179870605\n",
      "1534 step d_loss1: 0.5333481431007385, d_loss2: 0.3042118549346924, g_loss: 5.945544242858887\n",
      "1535 step d_loss1: 0.5066161155700684, d_loss2: 0.3140709400177002, g_loss: 7.052626132965088\n",
      "1536 step d_loss1: 0.15966814756393433, d_loss2: 0.41756877303123474, g_loss: 13.710097312927246\n",
      "1537 step d_loss1: 0.27057182788848877, d_loss2: 0.3732791543006897, g_loss: 15.987218856811523\n",
      "1538 step d_loss1: 0.42582252621650696, d_loss2: 0.24544206261634827, g_loss: 7.731251239776611\n",
      "1539 step d_loss1: 0.38149091601371765, d_loss2: 0.35830575227737427, g_loss: 6.712344646453857\n",
      "1540 step d_loss1: 0.3339286744594574, d_loss2: 0.6021637320518494, g_loss: 5.51546049118042\n",
      "1541 step d_loss1: 0.3577738404273987, d_loss2: 0.39443057775497437, g_loss: 6.838093280792236\n",
      "1542 step d_loss1: 0.2512935400009155, d_loss2: 0.4154523015022278, g_loss: 8.196985244750977\n",
      "1543 step d_loss1: 0.17094554007053375, d_loss2: 0.3142824172973633, g_loss: 11.839999198913574\n",
      "1544 step d_loss1: 0.22775430977344513, d_loss2: 0.3448432683944702, g_loss: 21.159141540527344\n",
      "1545 step d_loss1: 0.6585148572921753, d_loss2: 0.1918271780014038, g_loss: 7.042284965515137\n",
      "1546 step d_loss1: 0.5231785178184509, d_loss2: 0.2464074045419693, g_loss: 7.875207901000977\n",
      "1547 step d_loss1: 0.48803573846817017, d_loss2: 0.3823069930076599, g_loss: 7.247145175933838\n",
      "1548 step d_loss1: 0.3223433792591095, d_loss2: 0.3734588623046875, g_loss: 8.178045272827148\n",
      "1549 step d_loss1: 0.36048173904418945, d_loss2: 0.34340277314186096, g_loss: 6.907651424407959\n",
      "1550 step d_loss1: 0.32144540548324585, d_loss2: 0.3524721562862396, g_loss: 7.674267292022705\n",
      "1551 step d_loss1: 0.22083161771297455, d_loss2: 0.35632723569869995, g_loss: 12.814437866210938\n",
      "1552 step d_loss1: 0.18144041299819946, d_loss2: 0.2990330755710602, g_loss: 18.693464279174805\n",
      "1553 step d_loss1: 0.5312178134918213, d_loss2: 0.26368603110313416, g_loss: 7.7093305587768555\n",
      "1554 step d_loss1: 0.3680779039859772, d_loss2: 0.29506218433380127, g_loss: 6.962471961975098\n",
      "1555 step d_loss1: 0.2826264798641205, d_loss2: 0.24587807059288025, g_loss: 9.764091491699219\n",
      "1556 step d_loss1: 0.30957940220832825, d_loss2: 0.3387095034122467, g_loss: 6.881472110748291\n",
      "1557 step d_loss1: 0.3631252646446228, d_loss2: 0.290780246257782, g_loss: 6.54871129989624\n",
      "1558 step d_loss1: 0.24012842774391174, d_loss2: 0.4542900323867798, g_loss: 6.30667781829834\n",
      "1559 step d_loss1: 0.3451727330684662, d_loss2: 0.4033283293247223, g_loss: 5.178580284118652\n",
      "1560 step d_loss1: 0.4777701795101166, d_loss2: 0.27402710914611816, g_loss: 6.212830543518066\n",
      "1561 step d_loss1: 0.30918368697166443, d_loss2: 0.31173962354660034, g_loss: 7.512416362762451\n",
      "1562 step d_loss1: 0.35818660259246826, d_loss2: 0.41830384731292725, g_loss: 13.22690486907959\n",
      "1563 step d_loss1: 0.4143630266189575, d_loss2: 0.36393535137176514, g_loss: 6.7681403160095215\n",
      "1564 step d_loss1: 0.2855910658836365, d_loss2: 0.32004866003990173, g_loss: 7.004214763641357\n",
      "1565 step d_loss1: 0.27406245470046997, d_loss2: 0.3084676265716553, g_loss: 11.807849884033203\n",
      "1566 step d_loss1: 0.28900671005249023, d_loss2: 0.33928918838500977, g_loss: 6.622155666351318\n",
      "1567 step d_loss1: 0.37143412232398987, d_loss2: 0.3089945912361145, g_loss: 6.169856548309326\n",
      "1568 step d_loss1: 0.4105346202850342, d_loss2: 0.43166929483413696, g_loss: 7.152606010437012\n",
      "1569 step d_loss1: 0.17528276145458221, d_loss2: 0.4060620665550232, g_loss: 12.83935546875\n",
      "1570 step d_loss1: 0.20343494415283203, d_loss2: 0.2958214282989502, g_loss: 9.39857292175293\n",
      "1571 step d_loss1: 0.4633289873600006, d_loss2: 0.28659459948539734, g_loss: 6.734940528869629\n",
      "1572 step d_loss1: 0.4317264258861542, d_loss2: 0.2763589024543762, g_loss: 6.535979270935059\n",
      "1573 step d_loss1: 0.3613479435443878, d_loss2: 0.42703863978385925, g_loss: 5.831135272979736\n",
      "1574 step d_loss1: 0.389272004365921, d_loss2: 0.36232978105545044, g_loss: 6.061146259307861\n",
      "1575 step d_loss1: 0.19010725617408752, d_loss2: 0.5326892137527466, g_loss: 15.4662446975708\n",
      "1576 step d_loss1: 0.2943008244037628, d_loss2: 0.38325339555740356, g_loss: 8.772693634033203\n",
      "1577 step d_loss1: 0.3509830832481384, d_loss2: 0.269235759973526, g_loss: 7.608989715576172\n",
      "1578 step d_loss1: 0.21678948402404785, d_loss2: 0.2582690715789795, g_loss: 7.008030891418457\n",
      "1579 step d_loss1: 0.46036577224731445, d_loss2: 0.35124659538269043, g_loss: 7.027215957641602\n",
      "1580 step d_loss1: 0.34943887591362, d_loss2: 0.30047523975372314, g_loss: 6.764169692993164\n",
      "1581 step d_loss1: 0.21333721280097961, d_loss2: 0.2939688265323639, g_loss: 7.338005065917969\n",
      "1582 step d_loss1: 0.3667428493499756, d_loss2: 0.6446013450622559, g_loss: 16.465957641601562\n",
      "1583 step d_loss1: 0.13833817839622498, d_loss2: 0.4316215217113495, g_loss: 8.362800598144531\n",
      "1584 step d_loss1: 0.3568957448005676, d_loss2: 0.26245012879371643, g_loss: 6.987760543823242\n",
      "1585 step d_loss1: 0.32607501745224, d_loss2: 0.3530609905719757, g_loss: 8.328742980957031\n",
      "1586 step d_loss1: 0.16382433474063873, d_loss2: 0.659435510635376, g_loss: 11.779668807983398\n",
      "1587 step d_loss1: 0.507480800151825, d_loss2: 0.3026093542575836, g_loss: 6.777252197265625\n",
      "1588 step d_loss1: 0.5546187162399292, d_loss2: 0.31941094994544983, g_loss: 8.329679489135742\n",
      "1589 step d_loss1: 0.2685891389846802, d_loss2: 0.3242640495300293, g_loss: 8.11043930053711\n",
      "1590 step d_loss1: 0.37329596281051636, d_loss2: 0.333395779132843, g_loss: 8.034457206726074\n",
      "1591 step d_loss1: 0.48375236988067627, d_loss2: 0.30917859077453613, g_loss: 5.916428565979004\n",
      "1592 step d_loss1: 0.32611364126205444, d_loss2: 0.4210355281829834, g_loss: 7.764873027801514\n",
      "1593 step d_loss1: 0.3556595742702484, d_loss2: 0.26449668407440186, g_loss: 6.614077568054199\n",
      "1594 step d_loss1: 0.3619060218334198, d_loss2: 0.3169671893119812, g_loss: 6.296689987182617\n",
      "1595 step d_loss1: 0.3238782584667206, d_loss2: 0.33228906989097595, g_loss: 5.908946514129639\n",
      "1596 step d_loss1: 0.2872466742992401, d_loss2: 0.343168705701828, g_loss: 6.4606218338012695\n",
      "1597 step d_loss1: 0.27921590209007263, d_loss2: 0.2958522140979767, g_loss: 6.88892126083374\n",
      "1598 step d_loss1: 0.3647313117980957, d_loss2: 0.3381730318069458, g_loss: 8.16181468963623\n",
      "1599 step d_loss1: 0.2522902488708496, d_loss2: 0.4469429850578308, g_loss: 6.491564750671387\n",
      "1600 step d_loss1: 0.21349895000457764, d_loss2: 0.3603685796260834, g_loss: 16.875200271606445\n",
      "1601 step d_loss1: 0.289747029542923, d_loss2: 0.29687070846557617, g_loss: 8.02964973449707\n",
      "1602 step d_loss1: 0.46508514881134033, d_loss2: 0.27312159538269043, g_loss: 6.525696754455566\n",
      "1603 step d_loss1: 0.37637409567832947, d_loss2: 0.42620205879211426, g_loss: 7.494487285614014\n",
      "1604 step d_loss1: 0.19650790095329285, d_loss2: 0.5364171266555786, g_loss: 10.8477144241333\n",
      "1605 step d_loss1: 0.16616317629814148, d_loss2: 0.3258909285068512, g_loss: 37.14371871948242\n",
      "1606 step d_loss1: 0.3417016565799713, d_loss2: 0.2631014585494995, g_loss: 11.225565910339355\n",
      "1607 step d_loss1: 0.5970050096511841, d_loss2: 0.27673259377479553, g_loss: 6.957854270935059\n",
      "1608 step d_loss1: 0.40654927492141724, d_loss2: 0.3858931362628937, g_loss: 8.233269691467285\n",
      "1609 step d_loss1: 0.3504308760166168, d_loss2: 0.3532177209854126, g_loss: 7.90152645111084\n",
      "1610 step d_loss1: 0.481365829706192, d_loss2: 0.33283883333206177, g_loss: 5.906950950622559\n",
      "1611 step d_loss1: 0.4358600974082947, d_loss2: 0.3458840847015381, g_loss: 7.126317501068115\n",
      "1612 step d_loss1: 0.2718077600002289, d_loss2: 0.3739069998264313, g_loss: 10.143951416015625\n",
      "1613 step d_loss1: 0.26426243782043457, d_loss2: 0.3828277885913849, g_loss: 15.547759056091309\n",
      "1614 step d_loss1: 0.38903525471687317, d_loss2: 0.2598816752433777, g_loss: 8.59510612487793\n",
      "1615 step d_loss1: 0.25317084789276123, d_loss2: 0.24694760143756866, g_loss: 9.024402618408203\n",
      "1616 step d_loss1: 0.38276487588882446, d_loss2: 0.3461005687713623, g_loss: 8.001603126525879\n",
      "1617 step d_loss1: 0.15889085829257965, d_loss2: 0.39777860045433044, g_loss: 12.859539031982422\n",
      "1618 step d_loss1: 0.3682939112186432, d_loss2: 0.34561339020729065, g_loss: 9.547518730163574\n",
      "1619 step d_loss1: 0.34901541471481323, d_loss2: 0.2779095768928528, g_loss: 6.595945358276367\n",
      "1620 step d_loss1: 0.2906780242919922, d_loss2: 0.2457434982061386, g_loss: 7.025555610656738\n",
      "1621 step d_loss1: 0.3510414958000183, d_loss2: 0.2766267657279968, g_loss: 7.135928153991699\n",
      "1622 step d_loss1: 0.34860843420028687, d_loss2: 0.34216228127479553, g_loss: 6.63870096206665\n",
      "1623 step d_loss1: 0.40022921562194824, d_loss2: 0.31401798129081726, g_loss: 7.592321395874023\n",
      "1624 step d_loss1: 0.18870243430137634, d_loss2: 0.43137285113334656, g_loss: 7.351844787597656\n",
      "1625 step d_loss1: 0.3335026502609253, d_loss2: 0.2575196325778961, g_loss: 7.265613079071045\n",
      "1626 step d_loss1: 0.22917483747005463, d_loss2: 0.3530418276786804, g_loss: 6.940123558044434\n",
      "1627 step d_loss1: 0.30545035004615784, d_loss2: 0.34529855847358704, g_loss: 5.824394702911377\n",
      "1628 step d_loss1: 0.5059745907783508, d_loss2: 0.3896709084510803, g_loss: 8.353740692138672\n",
      "1629 step d_loss1: 0.3336952030658722, d_loss2: 0.26447808742523193, g_loss: 8.089080810546875\n",
      "1630 step d_loss1: 0.3903833031654358, d_loss2: 0.3650856018066406, g_loss: 8.289033889770508\n",
      "1631 step d_loss1: 0.4108143150806427, d_loss2: 0.3421909809112549, g_loss: 4.97996711730957\n",
      "1632 step d_loss1: 0.22003749012947083, d_loss2: 0.3205983638763428, g_loss: 9.043647766113281\n",
      "1633 step d_loss1: 0.3548886477947235, d_loss2: 0.25287339091300964, g_loss: 6.953136444091797\n",
      "1634 step d_loss1: 0.2189270555973053, d_loss2: 0.3209293484687805, g_loss: 5.349906921386719\n",
      "1635 step d_loss1: 0.31369686126708984, d_loss2: 0.2845914959907532, g_loss: 5.989047527313232\n",
      "1636 step d_loss1: 0.1908234804868698, d_loss2: 0.5891152620315552, g_loss: 7.507431507110596\n",
      "1637 step d_loss1: 0.3587845265865326, d_loss2: 0.3211842179298401, g_loss: 6.384487152099609\n",
      "1638 step d_loss1: 0.306511253118515, d_loss2: 0.34567582607269287, g_loss: 6.6928229331970215\n",
      "1639 step d_loss1: 0.29575249552726746, d_loss2: 0.3201693296432495, g_loss: 6.764222145080566\n",
      "1640 step d_loss1: 0.3415322005748749, d_loss2: 0.2981076240539551, g_loss: 7.097091197967529\n",
      "1641 step d_loss1: 0.38230764865875244, d_loss2: 0.4249962270259857, g_loss: 5.755612373352051\n",
      "1642 step d_loss1: 0.2695796489715576, d_loss2: 0.4283273220062256, g_loss: 7.078199863433838\n",
      "1643 step d_loss1: 0.25172773003578186, d_loss2: 0.285683274269104, g_loss: 6.490098476409912\n",
      "1644 step d_loss1: 0.2357107549905777, d_loss2: 0.4199603796005249, g_loss: 7.561756610870361\n",
      "1645 step d_loss1: 0.5679827928543091, d_loss2: 0.32346010208129883, g_loss: 6.195363521575928\n",
      "1646 step d_loss1: 0.47065451741218567, d_loss2: 0.3155707120895386, g_loss: 5.14522647857666\n",
      "1647 step d_loss1: 0.30783674120903015, d_loss2: 0.28990036249160767, g_loss: 8.12479305267334\n",
      "1648 step d_loss1: 0.4521091878414154, d_loss2: 0.2418668270111084, g_loss: 7.138345241546631\n",
      "1649 step d_loss1: 0.39637765288352966, d_loss2: 0.3023369312286377, g_loss: 5.943215847015381\n",
      "1650 step d_loss1: 0.3702061176300049, d_loss2: 0.2951471507549286, g_loss: 7.600037574768066\n",
      "1651 step d_loss1: 0.3448525667190552, d_loss2: 0.3344336748123169, g_loss: 6.098547458648682\n",
      "1652 step d_loss1: 0.15864698588848114, d_loss2: 0.5158705115318298, g_loss: 22.850143432617188\n",
      "1653 step d_loss1: 0.26495927572250366, d_loss2: 0.3336179852485657, g_loss: 9.494626998901367\n",
      "1654 step d_loss1: 0.3886461555957794, d_loss2: 0.2833070456981659, g_loss: 7.406017780303955\n",
      "1655 step d_loss1: 0.1831691712141037, d_loss2: 0.3684520125389099, g_loss: 8.287665367126465\n",
      "1656 step d_loss1: 0.3362463712692261, d_loss2: 0.4244058430194855, g_loss: 7.540099143981934\n",
      "1657 step d_loss1: 0.5505682826042175, d_loss2: 0.2701352536678314, g_loss: 5.619327545166016\n",
      "1658 step d_loss1: 0.1387486457824707, d_loss2: 0.31904545426368713, g_loss: 8.921883583068848\n",
      "1659 step d_loss1: 0.4069204330444336, d_loss2: 0.28478723764419556, g_loss: 6.420178413391113\n",
      "1660 step d_loss1: 0.15919330716133118, d_loss2: 0.44278886914253235, g_loss: 12.065486907958984\n",
      "1661 step d_loss1: 0.40814438462257385, d_loss2: 0.2618946433067322, g_loss: 7.318212032318115\n",
      "1662 step d_loss1: 0.4543035924434662, d_loss2: 0.32034486532211304, g_loss: 7.079718589782715\n",
      "1663 step d_loss1: 0.13998430967330933, d_loss2: 0.4579942524433136, g_loss: 9.555743217468262\n",
      "1664 step d_loss1: 0.3673333525657654, d_loss2: 0.29384100437164307, g_loss: 6.129967212677002\n",
      "1665 step d_loss1: 0.3519742786884308, d_loss2: 0.30055150389671326, g_loss: 7.496983528137207\n",
      "1666 step d_loss1: 0.34250950813293457, d_loss2: 0.35699522495269775, g_loss: 7.039693832397461\n",
      "1667 step d_loss1: 0.4729776680469513, d_loss2: 0.27773481607437134, g_loss: 8.237228393554688\n",
      "1668 step d_loss1: 0.3869817852973938, d_loss2: 0.31532350182533264, g_loss: 5.90940523147583\n",
      "1669 step d_loss1: 0.310406357049942, d_loss2: 0.3097067177295685, g_loss: 6.5606255531311035\n",
      "1670 step d_loss1: 0.2984229028224945, d_loss2: 0.3139107823371887, g_loss: 7.670205593109131\n",
      "1671 step d_loss1: 0.2931497395038605, d_loss2: 0.3175279498100281, g_loss: 6.9601898193359375\n",
      "1672 step d_loss1: 0.15005718171596527, d_loss2: 0.31001269817352295, g_loss: 8.13919448852539\n",
      "1673 step d_loss1: 0.19547446072101593, d_loss2: 0.34918636083602905, g_loss: 15.417825698852539\n",
      "1674 step d_loss1: 0.35028573870658875, d_loss2: 0.3627655506134033, g_loss: 6.834813594818115\n",
      "1675 step d_loss1: 0.3472321033477783, d_loss2: 0.31013089418411255, g_loss: 7.631653308868408\n",
      "1676 step d_loss1: 0.12004784494638443, d_loss2: 0.38859617710113525, g_loss: 15.188798904418945\n",
      "1677 step d_loss1: 0.5269237756729126, d_loss2: 0.2675502300262451, g_loss: 6.083934783935547\n",
      "1678 step d_loss1: 0.1998712569475174, d_loss2: 0.3619961738586426, g_loss: 15.57433032989502\n",
      "1679 step d_loss1: 0.46406835317611694, d_loss2: 0.37612706422805786, g_loss: 5.245889186859131\n",
      "1680 step d_loss1: 0.30202698707580566, d_loss2: 0.3437199890613556, g_loss: 9.805334091186523\n",
      "1681 step d_loss1: 0.3779529631137848, d_loss2: 0.2521355152130127, g_loss: 6.26527738571167\n",
      "1682 step d_loss1: 0.18876390159130096, d_loss2: 0.24298125505447388, g_loss: 7.106730937957764\n",
      "1683 step d_loss1: 0.15343186259269714, d_loss2: 0.225277841091156, g_loss: 6.923157215118408\n",
      "1684 step d_loss1: 0.10704337060451508, d_loss2: 0.5825941562652588, g_loss: 11.17012882232666\n",
      "1685 step d_loss1: 0.44000253081321716, d_loss2: 0.31073057651519775, g_loss: 6.891702651977539\n",
      "1686 step d_loss1: 0.15417778491973877, d_loss2: 0.32446596026420593, g_loss: 8.241426467895508\n",
      "1687 step d_loss1: 0.2633553445339203, d_loss2: 0.19164419174194336, g_loss: 15.239439964294434\n",
      "1688 step d_loss1: 0.3826468586921692, d_loss2: 0.43101024627685547, g_loss: 7.236588478088379\n",
      "1689 step d_loss1: 0.5302058458328247, d_loss2: 0.25066566467285156, g_loss: 8.298229217529297\n",
      "1690 step d_loss1: 0.2726176679134369, d_loss2: 0.4585843086242676, g_loss: 7.809521675109863\n",
      "1691 step d_loss1: 0.4488629996776581, d_loss2: 0.2869074046611786, g_loss: 8.284289360046387\n",
      "1692 step d_loss1: 0.26295724511146545, d_loss2: 0.3559540808200836, g_loss: 13.21969985961914\n",
      "1693 step d_loss1: 0.42801371216773987, d_loss2: 0.36653798818588257, g_loss: 7.793545722961426\n",
      "1694 step d_loss1: 0.23193208873271942, d_loss2: 0.4496244490146637, g_loss: 11.293249130249023\n",
      "1695 step d_loss1: 0.48614776134490967, d_loss2: 0.24606481194496155, g_loss: 7.173173904418945\n",
      "1696 step d_loss1: 0.30014386773109436, d_loss2: 0.35948726534843445, g_loss: 21.880455017089844\n",
      "1697 step d_loss1: 0.4649925231933594, d_loss2: 0.2965046763420105, g_loss: 8.302733421325684\n",
      "1698 step d_loss1: 0.42121538519859314, d_loss2: 0.3202834129333496, g_loss: 6.641331195831299\n",
      "1699 step d_loss1: 0.3646151125431061, d_loss2: 0.46560943126678467, g_loss: 5.664055347442627\n",
      "1700 step d_loss1: 0.337673157453537, d_loss2: 0.32935482263565063, g_loss: 14.701597213745117\n",
      "1701 step d_loss1: 0.3885648250579834, d_loss2: 0.32552391290664673, g_loss: 6.299232482910156\n",
      "1702 step d_loss1: 0.2207418978214264, d_loss2: 0.28762534260749817, g_loss: 9.907354354858398\n",
      "1703 step d_loss1: 0.19368773698806763, d_loss2: 0.3461605906486511, g_loss: 15.058382034301758\n",
      "1704 step d_loss1: 0.5540621280670166, d_loss2: 0.23501400649547577, g_loss: 6.970233917236328\n",
      "1705 step d_loss1: 0.30735552310943604, d_loss2: 0.38593387603759766, g_loss: 6.987730026245117\n",
      "1706 step d_loss1: 0.4083040952682495, d_loss2: 0.28945446014404297, g_loss: 8.381595611572266\n",
      "1707 step d_loss1: 0.36650776863098145, d_loss2: 0.3098432719707489, g_loss: 7.403785705566406\n",
      "1708 step d_loss1: 0.22845295071601868, d_loss2: 0.30339154601097107, g_loss: 9.003959655761719\n",
      "1709 step d_loss1: 0.1975313127040863, d_loss2: 0.26116594672203064, g_loss: 6.635753154754639\n",
      "1710 step d_loss1: 0.30521267652511597, d_loss2: 0.24920746684074402, g_loss: 6.9650139808654785\n",
      "1711 step d_loss1: 0.11346855759620667, d_loss2: 0.6145715713500977, g_loss: 11.629429817199707\n",
      "1712 step d_loss1: 0.529016375541687, d_loss2: 0.20406362414360046, g_loss: 7.192455768585205\n",
      "1713 step d_loss1: 0.3815809488296509, d_loss2: 0.4587794542312622, g_loss: 9.825304985046387\n",
      "1714 step d_loss1: 0.17354735732078552, d_loss2: 0.31414443254470825, g_loss: 7.653913497924805\n",
      "1715 step d_loss1: 0.37776869535446167, d_loss2: 0.32853901386260986, g_loss: 6.130770683288574\n",
      "1716 step d_loss1: 0.3370947539806366, d_loss2: 0.34886690974235535, g_loss: 6.897833347320557\n",
      "1717 step d_loss1: 0.40404394268989563, d_loss2: 0.29231658577919006, g_loss: 5.9822001457214355\n",
      "1718 step d_loss1: 0.26141706109046936, d_loss2: 0.3769657015800476, g_loss: 6.890392303466797\n",
      "1719 step d_loss1: 0.31828081607818604, d_loss2: 0.35184043645858765, g_loss: 6.38602352142334\n",
      "1720 step d_loss1: 0.2366691380739212, d_loss2: 0.4097844958305359, g_loss: 7.914241313934326\n",
      "1721 step d_loss1: 0.3672392666339874, d_loss2: 0.33159488439559937, g_loss: 5.621086120605469\n",
      "1722 step d_loss1: 0.2605932950973511, d_loss2: 0.35153406858444214, g_loss: 6.3889241218566895\n",
      "1723 step d_loss1: 0.40352070331573486, d_loss2: 0.33443504571914673, g_loss: 5.623838901519775\n",
      "1724 step d_loss1: 0.3086582124233246, d_loss2: 0.4369244873523712, g_loss: 8.421773910522461\n",
      "1725 step d_loss1: 0.16144952178001404, d_loss2: 0.4144446551799774, g_loss: 8.645880699157715\n",
      "1726 step d_loss1: 0.44898223876953125, d_loss2: 0.2464197278022766, g_loss: 8.164772033691406\n",
      "1727 step d_loss1: 0.3967180848121643, d_loss2: 0.2943701148033142, g_loss: 7.007599830627441\n",
      "1728 step d_loss1: 0.2967376112937927, d_loss2: 0.2563697099685669, g_loss: 5.824677467346191\n",
      "1729 step d_loss1: 0.2906075716018677, d_loss2: 0.36715176701545715, g_loss: 7.733039855957031\n",
      "1730 step d_loss1: 0.3830762505531311, d_loss2: 0.3330017328262329, g_loss: 5.061227321624756\n",
      "1731 step d_loss1: 0.11700191348791122, d_loss2: 0.3267810046672821, g_loss: 11.837959289550781\n",
      "1732 step d_loss1: 0.1442001312971115, d_loss2: 0.27117446064949036, g_loss: 8.214967727661133\n",
      "1733 step d_loss1: 0.1877290904521942, d_loss2: 0.25411537289619446, g_loss: 8.090846061706543\n",
      "1734 step d_loss1: 0.5239740610122681, d_loss2: 0.36141330003738403, g_loss: 6.792287349700928\n",
      "1735 step d_loss1: 0.12456682324409485, d_loss2: 0.391959547996521, g_loss: 18.795900344848633\n",
      "1736 step d_loss1: 0.43147388100624084, d_loss2: 0.21191686391830444, g_loss: 7.339346885681152\n",
      "1737 step d_loss1: 0.523524284362793, d_loss2: 0.32339686155319214, g_loss: 8.125822067260742\n",
      "1738 step d_loss1: 0.2977588474750519, d_loss2: 0.3208627700805664, g_loss: 7.750817775726318\n",
      "1739 step d_loss1: 0.12491430342197418, d_loss2: 0.40147680044174194, g_loss: 14.445819854736328\n",
      "1740 step d_loss1: 0.5189197659492493, d_loss2: 0.32510146498680115, g_loss: 6.519803524017334\n",
      "1741 step d_loss1: 0.5658900141716003, d_loss2: 0.28122204542160034, g_loss: 7.3322062492370605\n",
      "1742 step d_loss1: 0.44310951232910156, d_loss2: 0.33850717544555664, g_loss: 6.925629615783691\n",
      "1743 step d_loss1: 0.3199271559715271, d_loss2: 0.3308178186416626, g_loss: 7.1147308349609375\n",
      "1744 step d_loss1: 0.3043939471244812, d_loss2: 0.3929882049560547, g_loss: 7.322361469268799\n",
      "1745 step d_loss1: 0.3634662926197052, d_loss2: 0.31473854184150696, g_loss: 6.8714799880981445\n",
      "1746 step d_loss1: 0.40671035647392273, d_loss2: 0.5176386833190918, g_loss: 4.794560432434082\n",
      "1747 step d_loss1: 0.3847503066062927, d_loss2: 0.3113856911659241, g_loss: 6.30649471282959\n",
      "1748 step d_loss1: 0.17947497963905334, d_loss2: 0.28949499130249023, g_loss: 9.014331817626953\n",
      "1749 step d_loss1: 0.517788827419281, d_loss2: 0.25916779041290283, g_loss: 5.742190361022949\n",
      "1750 step d_loss1: 0.23405615985393524, d_loss2: 0.31250128149986267, g_loss: 7.908501625061035\n",
      "1751 step d_loss1: 0.18984168767929077, d_loss2: 0.2836938798427582, g_loss: 7.012868404388428\n",
      "1752 step d_loss1: 0.12240403145551682, d_loss2: 0.45100638270378113, g_loss: 18.917831420898438\n",
      "1753 step d_loss1: 0.5286475419998169, d_loss2: 0.28647634387016296, g_loss: 5.045050621032715\n",
      "1754 step d_loss1: 0.3673166334629059, d_loss2: 0.2996528148651123, g_loss: 7.182275295257568\n",
      "1755 step d_loss1: 0.1957150101661682, d_loss2: 0.4029276371002197, g_loss: 6.887421607971191\n",
      "1756 step d_loss1: 0.3771313726902008, d_loss2: 0.30924904346466064, g_loss: 6.223077774047852\n",
      "1757 step d_loss1: 0.2562926709651947, d_loss2: 0.29404518008232117, g_loss: 7.353838920593262\n",
      "1758 step d_loss1: 0.3871630132198334, d_loss2: 0.2707073390483856, g_loss: 6.4395551681518555\n",
      "1759 step d_loss1: 0.15426480770111084, d_loss2: 0.2466410994529724, g_loss: 14.564743041992188\n",
      "1760 step d_loss1: 0.38086816668510437, d_loss2: 0.42202040553092957, g_loss: 6.273448467254639\n",
      "1761 step d_loss1: 0.27776187658309937, d_loss2: 0.3621429204940796, g_loss: 7.099671363830566\n",
      "1762 step d_loss1: 0.2787419557571411, d_loss2: 0.3763878345489502, g_loss: 7.2200517654418945\n",
      "1763 step d_loss1: 0.34980374574661255, d_loss2: 0.3127944767475128, g_loss: 6.379698753356934\n",
      "1764 step d_loss1: 0.28004390001296997, d_loss2: 0.26576173305511475, g_loss: 5.863667964935303\n",
      "1765 step d_loss1: 0.4826489984989166, d_loss2: 0.4345589280128479, g_loss: 7.042419910430908\n",
      "1766 step d_loss1: 0.28689777851104736, d_loss2: 0.42974361777305603, g_loss: 6.701407432556152\n",
      "1767 step d_loss1: 0.37335914373397827, d_loss2: 0.3450098931789398, g_loss: 6.255222320556641\n",
      "1768 step d_loss1: 0.45103901624679565, d_loss2: 0.31046608090400696, g_loss: 6.153373718261719\n",
      "1769 step d_loss1: 0.4013218879699707, d_loss2: 0.4152959883213043, g_loss: 4.636608123779297\n",
      "1770 step d_loss1: 0.2644587755203247, d_loss2: 0.3259323239326477, g_loss: 7.736426830291748\n",
      "1771 step d_loss1: 0.3202303349971771, d_loss2: 0.30178385972976685, g_loss: 7.451168060302734\n",
      "1772 step d_loss1: 0.2519143223762512, d_loss2: 0.3713539242744446, g_loss: 7.135598659515381\n",
      "1773 step d_loss1: 0.39926624298095703, d_loss2: 0.37485137581825256, g_loss: 5.09055757522583\n",
      "1774 step d_loss1: 0.15446361899375916, d_loss2: 0.31888705492019653, g_loss: 13.227989196777344\n",
      "1775 step d_loss1: 0.480436772108078, d_loss2: 0.26568296551704407, g_loss: 5.730862617492676\n",
      "1776 step d_loss1: 0.3885113298892975, d_loss2: 0.34251677989959717, g_loss: 6.0862836837768555\n",
      "1777 step d_loss1: 0.2528460621833801, d_loss2: 0.4075133204460144, g_loss: 7.932785987854004\n",
      "1778 step d_loss1: 0.49634116888046265, d_loss2: 0.3123950958251953, g_loss: 6.948337078094482\n",
      "1779 step d_loss1: 0.22745132446289062, d_loss2: 0.28497591614723206, g_loss: 8.34320068359375\n",
      "1780 step d_loss1: 0.35612592101097107, d_loss2: 0.3408430814743042, g_loss: 6.675141334533691\n",
      "1781 step d_loss1: 0.4479844570159912, d_loss2: 0.29878997802734375, g_loss: 5.474686145782471\n",
      "1782 step d_loss1: 0.2605859041213989, d_loss2: 0.3587609529495239, g_loss: 5.680732727050781\n",
      "1783 step d_loss1: 0.3838954269886017, d_loss2: 0.31910479068756104, g_loss: 5.8162102699279785\n",
      "1784 step d_loss1: 0.2607555389404297, d_loss2: 0.48118191957473755, g_loss: 4.259676933288574\n",
      "1785 step d_loss1: 0.24958837032318115, d_loss2: 0.4091944098472595, g_loss: 6.783828258514404\n",
      "1786 step d_loss1: 0.41336485743522644, d_loss2: 0.3014514446258545, g_loss: 5.392251014709473\n",
      "1787 step d_loss1: 0.31428977847099304, d_loss2: 0.4438726305961609, g_loss: 6.426774024963379\n",
      "1788 step d_loss1: 0.31053322553634644, d_loss2: 0.41389477252960205, g_loss: 5.900567531585693\n",
      "1789 step d_loss1: 0.3845111131668091, d_loss2: 0.2649615406990051, g_loss: 7.225190162658691\n",
      "1790 step d_loss1: 0.2682431638240814, d_loss2: 0.3682599365711212, g_loss: 6.136153697967529\n",
      "1791 step d_loss1: 0.1251383274793625, d_loss2: 0.5554336905479431, g_loss: 7.256382465362549\n",
      "1792 step d_loss1: 0.5056856274604797, d_loss2: 0.2733387351036072, g_loss: 7.802761554718018\n",
      "1793 step d_loss1: 0.3501799702644348, d_loss2: 0.38350793719291687, g_loss: 11.968727111816406\n",
      "1794 step d_loss1: 0.4362407326698303, d_loss2: 0.31865230202674866, g_loss: 7.235195636749268\n",
      "1795 step d_loss1: 0.37982603907585144, d_loss2: 0.30910566449165344, g_loss: 6.12884521484375\n",
      "1796 step d_loss1: 0.39959266781806946, d_loss2: 0.33643317222595215, g_loss: 5.697720527648926\n",
      "1797 step d_loss1: 0.473679780960083, d_loss2: 0.2876930832862854, g_loss: 6.050204277038574\n",
      "1798 step d_loss1: 0.3225727677345276, d_loss2: 0.332864373922348, g_loss: 7.769411563873291\n",
      "1799 step d_loss1: 0.41979295015335083, d_loss2: 0.2868540287017822, g_loss: 5.738062381744385\n",
      "1800 step d_loss1: 0.27417683601379395, d_loss2: 0.3428153395652771, g_loss: 7.377694129943848\n",
      "1801 step d_loss1: 0.32205793261528015, d_loss2: 0.33765456080436707, g_loss: 5.632009506225586\n",
      "1802 step d_loss1: 0.25726383924484253, d_loss2: 0.35412824153900146, g_loss: 7.4137864112854\n",
      "1803 step d_loss1: 0.10204184055328369, d_loss2: 0.6526640057563782, g_loss: 11.760428428649902\n",
      "1804 step d_loss1: 0.21731141209602356, d_loss2: 0.2911810278892517, g_loss: 7.66363525390625\n",
      "1805 step d_loss1: 0.3832612633705139, d_loss2: 0.24187979102134705, g_loss: 7.173797607421875\n",
      "1806 step d_loss1: 0.38065242767333984, d_loss2: 0.22992506623268127, g_loss: 6.047124862670898\n",
      "1807 step d_loss1: 0.10677331686019897, d_loss2: 0.5382215976715088, g_loss: 26.457427978515625\n",
      "1808 step d_loss1: 0.24431777000427246, d_loss2: 0.3006513714790344, g_loss: 8.30078411102295\n",
      "1809 step d_loss1: 0.2769326865673065, d_loss2: 0.3726254403591156, g_loss: 7.326035022735596\n",
      "1810 step d_loss1: 0.4940374493598938, d_loss2: 0.30173537135124207, g_loss: 6.164418697357178\n",
      "1811 step d_loss1: 0.3121393322944641, d_loss2: 0.36956852674484253, g_loss: 8.426888465881348\n",
      "1812 step d_loss1: 0.2811504006385803, d_loss2: 0.45868155360221863, g_loss: 6.608423233032227\n",
      "1813 step d_loss1: 0.18984436988830566, d_loss2: 0.2593196630477905, g_loss: 7.482485294342041\n",
      "1814 step d_loss1: 0.41103270649909973, d_loss2: 0.5464824438095093, g_loss: 5.960260391235352\n",
      "1815 step d_loss1: 0.4422502815723419, d_loss2: 0.29868704080581665, g_loss: 5.37338399887085\n",
      "1816 step d_loss1: 0.2921346426010132, d_loss2: 0.3014175295829773, g_loss: 13.965648651123047\n",
      "1817 step d_loss1: 0.38260093331336975, d_loss2: 0.3126421570777893, g_loss: 7.157620906829834\n",
      "1818 step d_loss1: 0.3057807981967926, d_loss2: 0.4098576009273529, g_loss: 7.087118148803711\n",
      "1819 step d_loss1: 0.27473199367523193, d_loss2: 0.2365204095840454, g_loss: 8.368271827697754\n",
      "1820 step d_loss1: 0.3406643569469452, d_loss2: 0.2874395549297333, g_loss: 7.583102226257324\n",
      "1821 step d_loss1: 0.18681678175926208, d_loss2: 0.3364500403404236, g_loss: 8.91431999206543\n",
      "1822 step d_loss1: 0.5336353778839111, d_loss2: 0.34646326303482056, g_loss: 6.702597618103027\n",
      "1823 step d_loss1: 0.32497942447662354, d_loss2: 0.3413925766944885, g_loss: 8.498856544494629\n",
      "1824 step d_loss1: 0.22584788501262665, d_loss2: 0.2684977948665619, g_loss: 7.670975685119629\n",
      "1825 step d_loss1: 0.2766585350036621, d_loss2: 0.320843905210495, g_loss: 6.638317108154297\n",
      "1826 step d_loss1: 0.5124814510345459, d_loss2: 0.2845720946788788, g_loss: 6.040362358093262\n",
      "1827 step d_loss1: 0.41978588700294495, d_loss2: 0.34414172172546387, g_loss: 5.493879318237305\n",
      "1828 step d_loss1: 0.4277583360671997, d_loss2: 0.327384889125824, g_loss: 4.804996013641357\n",
      "1829 step d_loss1: 0.4044378399848938, d_loss2: 0.2856377065181732, g_loss: 5.491372108459473\n",
      "1830 step d_loss1: 0.4243795871734619, d_loss2: 0.31643611192703247, g_loss: 6.0497870445251465\n",
      "1831 step d_loss1: 0.44627809524536133, d_loss2: 0.38985851407051086, g_loss: 6.071647644042969\n",
      "1832 step d_loss1: 0.3556717038154602, d_loss2: 0.3789586126804352, g_loss: 4.954631805419922\n",
      "1833 step d_loss1: 0.3852274417877197, d_loss2: 0.2940436601638794, g_loss: 8.201436042785645\n",
      "1834 step d_loss1: 0.26523643732070923, d_loss2: 0.3701101541519165, g_loss: 7.779831886291504\n",
      "1835 step d_loss1: 0.42933014035224915, d_loss2: 0.30293262004852295, g_loss: 5.396886348724365\n",
      "1836 step d_loss1: 0.1674690991640091, d_loss2: 0.24807648360729218, g_loss: 6.579685211181641\n",
      "1837 step d_loss1: 0.08025919646024704, d_loss2: 0.40260547399520874, g_loss: 9.907530784606934\n",
      "1838 step d_loss1: 0.424380898475647, d_loss2: 0.262225478887558, g_loss: 6.521300315856934\n",
      "1839 step d_loss1: 0.22330288589000702, d_loss2: 0.3834007680416107, g_loss: 9.126815795898438\n",
      "1840 step d_loss1: 0.3255932927131653, d_loss2: 0.2928004562854767, g_loss: 5.47625732421875\n",
      "1841 step d_loss1: 0.35045185685157776, d_loss2: 0.3765735626220703, g_loss: 6.301520347595215\n",
      "1842 step d_loss1: 0.27843478322029114, d_loss2: 0.2678871750831604, g_loss: 6.921084880828857\n",
      "1843 step d_loss1: 0.24097679555416107, d_loss2: 0.36840611696243286, g_loss: 5.131664276123047\n",
      "1844 step d_loss1: 0.3168976306915283, d_loss2: 0.3493356704711914, g_loss: 5.590272903442383\n",
      "1845 step d_loss1: 0.18617402017116547, d_loss2: 0.275566965341568, g_loss: 6.185243606567383\n",
      "1846 step d_loss1: 0.28846198320388794, d_loss2: 0.4260726869106293, g_loss: 5.878720760345459\n",
      "1847 step d_loss1: 0.37834566831588745, d_loss2: 0.514129638671875, g_loss: 4.362773418426514\n",
      "1848 step d_loss1: 0.35195374488830566, d_loss2: 0.25847741961479187, g_loss: 5.5733137130737305\n",
      "1849 step d_loss1: 0.33301061391830444, d_loss2: 0.3449820876121521, g_loss: 7.940620422363281\n",
      "1850 step d_loss1: 0.4336770176887512, d_loss2: 0.2996160686016083, g_loss: 5.266401290893555\n",
      "1851 step d_loss1: 0.3601242005825043, d_loss2: 0.39647233486175537, g_loss: 5.732355117797852\n",
      "1852 step d_loss1: 0.26470062136650085, d_loss2: 0.2848265469074249, g_loss: 6.945159912109375\n",
      "1853 step d_loss1: 0.13089115917682648, d_loss2: 0.3047437071800232, g_loss: 8.326131820678711\n",
      "1854 step d_loss1: 0.13165290653705597, d_loss2: 0.2866165339946747, g_loss: 8.156685829162598\n",
      "1855 step d_loss1: 0.17421862483024597, d_loss2: 0.29256418347358704, g_loss: 7.715843677520752\n",
      "1856 step d_loss1: 0.37852686643600464, d_loss2: 0.3428201675415039, g_loss: 5.323715686798096\n",
      "1857 step d_loss1: 0.43527913093566895, d_loss2: 0.3760041296482086, g_loss: 6.2245588302612305\n",
      "1858 step d_loss1: 0.4504406452178955, d_loss2: 0.35564300417900085, g_loss: 6.791039943695068\n",
      "1859 step d_loss1: 0.5113610625267029, d_loss2: 0.3384988605976105, g_loss: 7.242060661315918\n",
      "1860 step d_loss1: 0.16366738080978394, d_loss2: 0.3802327513694763, g_loss: 17.25721549987793\n",
      "1861 step d_loss1: 0.4552151560783386, d_loss2: 0.3232851028442383, g_loss: 5.9160356521606445\n",
      "1862 step d_loss1: 0.25550541281700134, d_loss2: 0.2676964998245239, g_loss: 7.390205383300781\n",
      "1863 step d_loss1: 0.23269927501678467, d_loss2: 0.3324609100818634, g_loss: 6.127480983734131\n",
      "1864 step d_loss1: 0.19309359788894653, d_loss2: 0.285054087638855, g_loss: 6.339696884155273\n",
      "1865 step d_loss1: 0.10369380563497543, d_loss2: 0.3725622594356537, g_loss: 16.865812301635742\n",
      "1866 step d_loss1: 0.5515806674957275, d_loss2: 0.2698376178741455, g_loss: 6.592023849487305\n",
      "1867 step d_loss1: 0.45823246240615845, d_loss2: 0.2872926890850067, g_loss: 6.845870018005371\n",
      "1868 step d_loss1: 0.37747490406036377, d_loss2: 0.4270136058330536, g_loss: 4.015861988067627\n",
      "1869 step d_loss1: 0.3761747479438782, d_loss2: 0.44632211327552795, g_loss: 9.942896842956543\n",
      "1870 step d_loss1: 0.28102919459342957, d_loss2: 0.31178587675094604, g_loss: 7.1585564613342285\n",
      "1871 step d_loss1: 0.3817022144794464, d_loss2: 0.3649551272392273, g_loss: 6.596522331237793\n",
      "1872 step d_loss1: 0.20986832678318024, d_loss2: 0.36787381768226624, g_loss: 7.867665767669678\n",
      "1873 step d_loss1: 0.5248563289642334, d_loss2: 0.23529133200645447, g_loss: 6.322955131530762\n",
      "1874 step d_loss1: 0.194331094622612, d_loss2: 0.3652609586715698, g_loss: 7.325715065002441\n",
      "1875 step d_loss1: 0.46155625581741333, d_loss2: 0.28103113174438477, g_loss: 6.047441482543945\n",
      "1876 step d_loss1: 0.16549912095069885, d_loss2: 0.39897722005844116, g_loss: 7.093912601470947\n",
      "1877 step d_loss1: 0.4088916778564453, d_loss2: 0.2745426297187805, g_loss: 5.962843418121338\n",
      "1878 step d_loss1: 0.12663882970809937, d_loss2: 0.36048340797424316, g_loss: 8.854620933532715\n",
      "1879 step d_loss1: 0.18120890855789185, d_loss2: 0.25423017144203186, g_loss: 10.671947479248047\n",
      "1880 step d_loss1: 0.4856419861316681, d_loss2: 0.32080748677253723, g_loss: 8.813729286193848\n",
      "1881 step d_loss1: 0.4503355622291565, d_loss2: 0.27300164103507996, g_loss: 6.015283107757568\n",
      "1882 step d_loss1: 0.35247546434402466, d_loss2: 0.2785375118255615, g_loss: 8.61904525756836\n",
      "1883 step d_loss1: 0.33876195549964905, d_loss2: 0.29104727506637573, g_loss: 7.712854862213135\n",
      "1884 step d_loss1: 0.3429887890815735, d_loss2: 0.3512711822986603, g_loss: 6.858020305633545\n",
      "1885 step d_loss1: 0.26259273290634155, d_loss2: 0.4071882963180542, g_loss: 5.604897499084473\n",
      "1886 step d_loss1: 0.3423778712749481, d_loss2: 0.3799397051334381, g_loss: 6.534374713897705\n",
      "1887 step d_loss1: 0.43015679717063904, d_loss2: 0.30188629031181335, g_loss: 5.261474132537842\n",
      "1888 step d_loss1: 0.34045374393463135, d_loss2: 0.3206610083580017, g_loss: 5.2277984619140625\n",
      "1889 step d_loss1: 0.2955735921859741, d_loss2: 0.33456215262413025, g_loss: 6.480719089508057\n",
      "1890 step d_loss1: 0.20989550650119781, d_loss2: 0.4034794569015503, g_loss: 6.366771697998047\n",
      "1891 step d_loss1: 0.3608153164386749, d_loss2: 0.36142483353614807, g_loss: 6.110702037811279\n",
      "1892 step d_loss1: 0.32573091983795166, d_loss2: 0.3960467576980591, g_loss: 5.327822208404541\n",
      "1893 step d_loss1: 0.359820157289505, d_loss2: 0.37586304545402527, g_loss: 5.362008094787598\n",
      "1894 step d_loss1: 0.35542115569114685, d_loss2: 0.2911582291126251, g_loss: 5.868671894073486\n",
      "1895 step d_loss1: 0.14106716215610504, d_loss2: 0.3530970811843872, g_loss: 8.557621002197266\n",
      "1896 step d_loss1: 0.24500715732574463, d_loss2: 0.26473116874694824, g_loss: 7.349031448364258\n",
      "1897 step d_loss1: 0.46548107266426086, d_loss2: 0.3039412498474121, g_loss: 5.774094104766846\n",
      "1898 step d_loss1: 0.3438167870044708, d_loss2: 0.34404292702674866, g_loss: 5.164493083953857\n",
      "1899 step d_loss1: 0.25242453813552856, d_loss2: 0.4177838861942291, g_loss: 6.834351062774658\n",
      "1900 step d_loss1: 0.3140990734100342, d_loss2: 0.37628281116485596, g_loss: 4.9543046951293945\n",
      "1901 step d_loss1: 0.33508822321891785, d_loss2: 0.3104880452156067, g_loss: 6.284233570098877\n",
      "1902 step d_loss1: 0.13843397796154022, d_loss2: 0.2901972532272339, g_loss: 7.75580358505249\n",
      "1903 step d_loss1: 0.4311487674713135, d_loss2: 0.2723405361175537, g_loss: 7.154284954071045\n"
     ]
    }
   ],
   "source": [
    "# define the composite model\n",
    "gan_model = define_gan(g_model, d_model, image_shape)\n",
    "# train model\n",
    "train(d_model, g_model, gan_model, dataset, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils.vis_utils import plot_model\n",
    "# plot_model(g_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
